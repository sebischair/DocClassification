Yes, We should deprecate ConnectionManager in before removing it in Deprecate NIO ConnectionManager
Yes, What StringIndexerInverse does is not strictly associated with StringIndexer and the name is not super clear Rename StringIndexerInverse to IndexToString
Yes, This requires some discussion I m not sure whether runs is a useful parameter It certainly complicates the implementation We might want to optimize the k means implementation with block matrix operations In this case having runs may not be worth the trade offs Remove runs from KMeans under the pipeline API
Yes, There exists a chance that the prefixes keep growing to the maximum pattern length Then the final local processing step becomes unnecessary Skip local processing in PrefixSpan if there are no small prefixes
Yes, parquet mr fixed several issues that affect Spark For example PARQUET SPARK Upgrade parquet mr to
Yes, The utilities such as Substring substringBinarySQL and BinaryPrefixComparator computePrefix for binary data are put together in ByteArray for easy to read Move utilities for binary data into ByteArray
Yes, PlatformDependent UNSAFE is way too verbose Rename PlatformDependent UNSAFE Platform
Yes, I took a look at the commit messages in git log it looks like the individual commit messages are not that useful to include but do make the commit messages more verbose They are usually just a bunch of extremely concise descriptions of bug fixes merges etc See mailing list discussions http apache spark developers list n nabble com discuss Removing individual commit messages from the squash commit message td html Remove individual commit messages from the squash commit message
Yes, Add Python API user guide and example for ml regression IsotonicRegression Add Python API for ml regression IsotonicRegression
Yes, Add Python API for MultilayerPerceptronClassifier Add Python API for MultilayerPerceptronClassifier
Yes, Add Python API user guide and example for ml feature CountVectorizerModel Add Python API for ml feature CountVectorizer
Yes, We introduced the Netty network module for shuffle in Spark and has turned it on by default for releases The old ConnectionManager is difficult to maintain It s time to remove it Remove ConnectionManager
No, Check and add miss docs for PySpark ML this issue only check miss docs for o a s ml not o a s mllib check and add missing docs for PySpark ML
Yes, see discussion here https github com apache spark pull issuecomment Improve performance of Decimal times and casting from integral
Yes, TypeCheck no longer applies in the new Tungsten world Remove TypeCheck in debug package
Yes, In https github com apache spark pull we added FromUnsafe to convert nexted unsafe data like array map struct to safe versions It s a quick solution and we already have GenerateSafe to do the conversion which is codegened So we should remove FromUnsafe and implement its codegen version in GenerateSafe remove FromUnsafe and add its codegen version to GenerateSafe
Yes, JoinedRow anyNull currently loops through every field to check for null which is inefficient if the underlying rows are UnsafeRows It should just delegate to the underlying implementation JoinedRow anyNull should delegate to the underlying rows
No, All data sources show up as PhysicalRDD in physical plan explain It d be better if we can show the name of the data source Existing Improve explain message for data source scan node
No, To identify potential API issues list public API changes which affect binary and source incompatibility by using command Report result attached List Binary and Source Compatibility Issues with japi compliance checker
Yes, This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages Issue brought up by eronwright Descriptions below copied from http apache spark developers list n nabble com Make ML Developer APIs public post td html We plan to make these APIs public in Spark However they will be marked DeveloperApi and are very likely to be broken in the future VectorUDT To define a relation with a vector field VectorUDT must be instantiated Identifiable trait The trait generates a unique identifier for the associated pipeline component Nice to have a consistent format by reusing the trait ProbabilisticClassifier Third party components should leverage the complex logic around computing only selected columns We will not yet make these public SchemaUtils Third party pipeline components have a need for checking column types and appending columns This will probably be moved into Spark SQL Users can copy the methods into their own as needed Make some ML APIs public VectorUDT Identifiable ProbabilisticClassifier
Yes, Consider SortMergeJoin which requires a sorted clustered distribution of its input rows Say that both of SMJ s children produce unsorted output but are both single partition In this case we will need to inject sort operators but should not need to inject exchanges Unfortunately it looks like the Exchange unnecessarily repartitions using a hash partitioning We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied I d like to fix this for Spark since it makes certain types of unit tests easier to write EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied
Yes, Previously we use MB as the default page size which was way too big for a lot of Spark applications especially for single node This patch changes it so that the default page size if unset by the user is determined by the number of cores available and the total execution memory available Pick default page size more intelligently
Yes, Add feature interaction as a transformer which takes a list of vector double columns and generate a single vector column that contains the interactions multiplication among them with proper handling of feature names Add feature interaction as a transformer
Yes, Eg currently we can do up to sorts within a task During the aggregation During a sort on the same key During the shuffle In environments with tight memory restrictions the first operator may acquire so much memory such that the subsequent ones in the same task are starved A simple fix is to reserve at least a page in advance in each of these places The reserved page size need not be the same as the normal page size This is a sister problem to SPARK in Spark Core Reserve a page in all unsafe operators to avoid starving an operator
Yes, A small performance optimization we don t need to generate a Tuple and then immediately discard the key We also don t need an extra wrapper Remove SqlNewHadoopRDD s generated Tuple and InterruptibleIterator
Yes, It is subsumed by the new aggregate implementation Remove GeneratedAggregate
Yes, GenerateUnsafeProjection can be used directly as a code generated serializer We no longer need SparkSqlSerializer Remove SparkSqlSerializer in favor of Unsafe exchange
Yes, Currently we don t support using DecimalType with precision in new unsafe aggregation it s good to support it Support update DecimalType with precision in UnsafeRow
Yes, In many modeling application data points are not necessarily sampled with equal probabilities Linear regression should support weighting which account the over or under sampling LinearRegression should supported weighted data
Yes, update InternalRow toSeq to make it accept data type info
Yes, Spark s style checker should ban the use of Scala s JavaConversions which provides implicit conversions between Java and Scala collections types Instead we should be performing these conversions explicitly using JavaConverters or forgoing the conversions altogether if they re occurring inside of performance critical code Ban use of JavaConversions and migrate all existing uses to JavaConverters
Yes, Add StreamingContext getActiveOrCreate to python API
No, This is an epic for Spark release QA plans for tracking various components Spark Testing Plan
Yes, See http apache spark developers list n nabble com Re Should spark ec get its own repo td html for more details Move spark ec from mesos to amplab
Yes, remove the createCode and createStructCode and replace the usage of them by createStructCode
Yes, While reviewing yhuai s patch for SPARK I noticed that Exchange s compatible check may be incorrectly returning false in many cases As far as I know this is not actually a problem because the compatible meetsRequirements and needsAnySort checks are serving only as short circuit performance optimizations that are not necessary for correctness In order to reduce code complexity I think that we should remove these checks and unconditionally rewrite the operator s children This should be safe because we rewrite the tree in a single bottom up pass Remove compatibleWith meetsRequirements and needsAnySort checks from Exchange
Yes, They were added to improve performance so JIT can inline the JoinedRow calls However we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators Remove all extra JoinedRows
Yes, Remove UnsafeRowConverter in favor of UnsafeProjection
Yes, We should consolidate LocalScheduler and ClusterScheduler given most of the functionalities are duplicated in both This can be done by removing the LocalScheduler and create a LocalSchedulerBackend that connects directly to an Executor Consolidate local scheduler and cluster scheduler
Yes, It is a big change but it lets us use the type information to prevent accidentally passing internal types to external types Remove InternalRow s inheritance from Row
Yes, See SPARK We added varargs again Though it is technically correct it often requires that developers do clean assembly rather than not clean assembly which is a nuisance during development This JIRA will remove it for now pending a fix to the Scala compiler Params setDefault should not keep varargs annotation
Yes, Spark has an option called spark localExecution enabled according to the docs quote Enables Spark to run certain jobs such as first or take on the driver without sending tasks to the cluster This can make certain jobs execute very quickly but may require shipping a whole partition of data to the driver quote This feature ends up adding quite a bit of complexity to DAGScheduler especially in the runLocallyWithinThread method but as far as I know nobody uses this feature I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method As a step towards scheduler complexity reduction I propose that we remove this feature and all code related to it for Spark Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled
Yes, A small change based on code review and offline discussion with dragos Removing unnecessary self types in Catalyst
Yes, As the new Parquet external data source matures we should remove the old Parquet support now Removes old Parquet support code
Yes, They are not very useful and cause problems with toString due to the order they are mixed in Remove LeafNode UnaryNode BinaryNode from TreeNode
Yes, The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark Also the feature in SPARK is strictly better than a correct implementation of that feature We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work Remove references to preferredNodeLocalityData in javadoc and print warning when used
Yes, It is unnecessary and makes the type hierarchy slightly more complicated than needed Remove ExtractValueWithOrdinal abstract class
Yes, Right now InternalRow is megamorphic because it has many different implementations We should work towards having only one or at most two InternalRow implementations Remove EmptyRow class
Yes, The type alias was there because initially when I moved Row around I didn t want to do massive changes to the expression code But now it should be pretty easy to just remove it One less concept to worry about Remove InternalRow type alias in expressions package
Yes, Based on discussion offline with marmbrus we should remove GenerateProjection Remove GenerateProjection
Yes, This will make it convenient for R users to use SparkR from their browsers Install and configure RStudio server on Spark EC
Yes, We should remove the existing ExpressionOptimizationSuite and update checkEvaluation to also run the optimizer version ExpressionEvalHelper checkEvaluation should also run the optimizer version
Yes, From my perspective as a code reviewer I find them more confusing than using String directly Remove Term Code type aliases in code generation
Yes, It s not a very useful type to use We can just remove it to simplify expressions slightly Remove EvaluatedType from SQL Expression
Yes, ExternalSorter contains a bunch of to move this functionality out of ExternalSorter and into a separate class which shares a common interface insertAll writePartitionedFile This is a stepping stone towards eventually removing this bypass path see SPARK Move hash style shuffle code out of ExternalSorter and into own file
Yes, Learnt a lesson from SPARK Spark should avoid to use scala concurrent ExecutionContext Implicits global because the user may submit blocking actions to scala concurrent ExecutionContext Implicits global and exhaust all threads in it This could crash Spark So Spark should always use its own thread pools for safety Remove import scala concurrent ExecutionContext Implicits global
Yes, Removed calling size length in while condition to avoid extra JVM call
Yes, We can just rewrite distinct using groupby i e aggregate operator Remove physical Distinct operator in favor of Aggregate
Yes, Removed diffSum which is theoretical zero in LinearRegression and coding formating
Yes, We want to change and improve the spark ml API for trees and ensembles but we cannot change the old API in spark mllib To support the changes we want to make we should move the implementation from spark mllib to spark ml We will generalize and modify it but will also ensure that we do not change the behavior of the old API There are several steps to this Copy the implementation over to spark ml and change the spark ml classes to use that implementation rather than calling the spark mllib implementation The current spark ml tests will ensure that the implementations learn exactly the same models Note This should include performance testing to make sure the updated code does not have any regressions UPDATE I have run tests using spark perf and there were no regressions Remove the spark mllib implementation and make the spark mllib APIs wrappers around the spark ml implementation The spark ml tests will again ensure that we do not change any behavior Move the unit tests to spark ml and change the spark mllib unit tests to verify model equivalence This JIRA is now for step only Steps and will be in separate JIRAs After these updates we can more safely generalize and improve the spark ml implementation Move tree forest implementation from spark mllib to spark ml
Yes, This maven repository is blocked in China We should get rid of that dependency so people in China can compile Spark Remove dependency on Twitter J repository
Yes, This is not always possible but whenever possible we should remove or reduce the differences between Pandas and Spark DataFrames in Python Improve DataFrame API compatibility with Pandas
Yes, This depends on some internal interface of Spark SQL should be done after merging into Spark DataFrame UDFs in R
Yes, Would be great to create APIs for external block stores rather than doing a bunch of if statements everywhere Create external block store API
Yes, Upgrade Tachyon dependency to
Yes, Deprecated configs are currently all strewn across the code base It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere Centralize deprecated configs in SparkConf
Yes, Continue the discussion from the LDA PR CheckpoingDir is a global Spark configuration which should not be altered by an ML algorithm We could check whether checkpointDir is set if checkpointInterval is positive Remove setCheckpointDir from LDA and tree Strategy
Yes, This method survived the code review and it has been there since v It exposes jblas types Let s remove it from the public API I expect that no one calls it directly Hide ALS solveLeastSquares
Yes, toLocalIterator is available in Java and Scala If we add this functionality to Python then we can also be able to use PySpark to iterate over a dataset partition by partition Add toLocalIterator to pyspark rdd
Yes, Fix a todo in spark sql remove Command and use RunnableCommand instead Refactory command in spark sql
Yes, Remove unneeded staging repositories from build
Yes, mqtt client was removed from the Eclipse Paho repository and hence is breaking Spark build Upgrade MQTT dependency to use mqtt client
Yes, In this refactoring the performance is slightly increased by removing the overhead from breeze vector The bottleneck is still in breeze norm which is implemented by activeIterator This inefficiency of breeze norm will be addressed in next PR At least this PR makes the base Refactorize Normalizer to make code cleaner
Yes, In RDDSampler it try use numpy to gain better performance for possion but the number of call of random is only faction N in the pure python implementation of possion so there is no much performance gain from numpy numpy is not a dependent of pyspark so it maybe introduce some problem such as there is no numpy installed in slaves but only installed master as reported in xxxx It also complicate the code a lot so we may should remove numpy from RDDSampler remove numpy from RDDSampler of PySpark
Yes, For example YarnRMClient and YarnRMClientImpl can be merged YarnAllocator and YarnAllocationHandler can be merged Remove layers of abstraction in YARN code no longer needed after dropping yarn alpha
Yes, Due to vertex attribute caching EdgeRDD previously took two type parameters ED and VD However this is an implementation detail that should not be exposed in the interface so this PR drops the VD type parameter This requires removing the filter method from the EdgeRDD interface because it depends on vertex attribute caching Drop VD type parameter from EdgeRDD
No, With some of our more complicated modules I m not sure whether Intellij correctly understands all source locations Also we might require specifying some profiles for the build to work directly We should document clearly how to start with vanilla Spark master and get the entire thing building in Intellij Create instructions on fully building Spark in Intellij
Yes, We should upgrade snappy java to across all of our maintenance branches This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream this operation is always an error but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid corrupted streams see https github com xerial snappy java issues for more context This should be a major help in the Snappy debugging work that I ve been doing Upgrade snappy java to
Yes, After Pyrolite release a new version with PR https github com irmen Pyrolite pull we should remove the workaround introduced in PR https github com apache spark pull remove workaround to pickle array of float for Pyrolite
Yes, This will depend a bit on both user demand and the commitment level of maintainers but I d like to propose the following timeline for yarn alpha support Spark Deprecate YARN alpha Spark Remove YARN alpha i e require YARN stable Since YARN alpha is clearly identified as an alpha API it seems reasonable to drop support for it in a minor release However it does depend a bit whether anyone uses this outside of Yahoo and that I m not sure of In the past this API has been used and maintained by Yahoo but they ll be migrating soon to the stable API s Deprecate and later remove YARN alpha support
Yes,HiveLocalContext is nearly completely redundant with HiveContext We should consider deprecating it and removing all uses Get rid of LocalHiveContext
Yes,According to http doc akka io docs akka intro getting started html Akka is now published to Maven Central so our documentation and POM files don t need to use the old Akka repo It will be one less step for users to worry about Remove use of special Maven repo for Akka
No,SQLParser fails to resolve nested CASE WHEN statement like this select case when case when then else end then else end from tb Exception Exception in thread main org apache spark sql catalyst parser ParseException mismatched input then expecting OR AND IN NOT BETWEEN LIKE RLIKE IS WHEN EQ GTE DIV line pos SQL select case when case when then else end then else end from tb But remove parentheses will be fine select case when case when then else end then else end from tb SQL SQLParser fails to resolve nested CASE WHEN statement with parentheses
No,The names method fails to check for validity of the assignment values This can be fixed by calling colnames within names See example below Fix bug in the name assignment method in SparkR
No,Range operator currently does not output any input metrics and as a result in the SQL UI the number of rows shown is always Fix input metrics for range operator
Yes,The removed codes are not reachable because InConversion already resolve the type coercion issues Remove IN type coercion from PromoteStrings
No,The current executorId toInt will cause NumberformatException This unit test can pass currently because of askWithRetry when catching exception RPC will call again thus it will go if branch and return true ExecutorId in HearbeatReceiverSuite is incorrect
Yes,There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown As we upgrade to Parquet which includes the fix for the pushdown of optional columns we don t need this metadata now Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown
Yes,Apache Parquet is released officially last week on Jan This issue aims to bump Parquet version to since it includes many fixes https lists apache org thread html af c f a d ec b bbeecaea aa ef f c Cdev parquet apache org E Upgrade Parquet to
No, Function to json ignores the user provided options
No,There is a number of names which are exported with all but don t exist in the module DataFrame DataFrameNaFunctions DataFrameStatFunctions https github com apache spark blob a ca db fefaf a c d eeda a python pyspark sql column py L This results in unexpected import erros pyspark sql column exports non existent names
No,After resolving the JIRA https issues apache org jira browse SPARK the doc needs an update DOC Options are case insensitive since Spark
No,we need updates to programming guide example and vignettes Bisecting k means in SparkR documentation
No,Although udf and udf are different UDF but the name in the plans are the same It looks confusing Always Identical Name for UDF in the EXPLAIN output
Yes,Create a Python wrapper for spark ml classification LinearSVC LinearSVC Python API
No,Similar to SPARK codegen is in danger of arbitrary Fix the code injection vulnerability related to Generator functions
No,Whenever there are stdout outputs from Spark in JVM typically when calling println they are dropped by SparkR For example explain for Column https github com apache spark blob master sql core src main scala org apache spark sql Column scala L JVM stdout output is dropped in SparkR
No,When Kmeans using initMode random and some random seed it is possible the actual cluster size doesn t equal to the configured k In this case summary model returns error due to the number of cols of coefficient matrix doesn t equal to k SparkR Kmeans summary returns error when the cluster size doesn t equal to k
No,after patch SPARK was applied Sparkconf object is ignored when launching SparkContext programmatically via python from spark submit https github com apache spark blob master python pyspark context py L in case when we are running python SparkContext conf xxx from spark submit conf is set conf jconf is None passed as arg conf object is ignored and used only when we are launching java gateway how to fix python pyspark context py SPARK caused ignorance of conf object passed to SparkContext
Yes,To implement DDL commands we added several analyzer rules in sql hive module to analyze DDL related plans However our Analyzer currently only have one extending interface extendedResolutionRules which defines extra rules that will be run together with other rules in the resolution batch and doesn t fit DDL rules well because DDL rules may do some checking and normalization but we may do it many times as the resolution batch will run rules again and again until fixed point and it s hard to tell if a DDL rule has already done its checking and normalization It s fine because DDL rules are idempotent but it s bad for analysis performance some DDL rules may depend on others and it s pretty hard to write if conditions to guarantee the dependencies It will be good if we have a batch which run rules in one pass so that we can guarantee the dependencies by rules order add a new extending interface in Analyzer for post hoc resolution
No,bq run example sql streaming JavaStructuredKafkaWordCount subscribe topic when i run the spark example raises the following error quote Exception in thread main DEBUG ContextCleaner Got cleaning task CleanBroadcast org apache spark sql streaming StreamingQueryException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost executor driver java lang IllegalStateException Error reading delta file tmp temporary state delta of HDFSStateStoreProvider id op part dir tmp temporary b c bb f a c ca b b d state tmp temporary b c bb f a c ca b b d state delta does not exist at org apache spark sql execution streaming state HDFSBackedStateStoreProvider org apache spark sql execution streaming state HDFSBackedStateStoreProvider updateFromDeltaFile HDFSBackedStateStoreProvider scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider anonfun org apache spark sql execution streaming state HDFSBackedStateStoreProvider loadMap anonfun apply HDFSBackedStateStoreProvider scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider anonfun org apache spark sql execution streaming state HDFSBackedStateStoreProvider loadMap anonfun apply HDFSBackedStateStoreProvider scala at scala Option getOrElse Option scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider anonfun org apache spark sql execution streaming state HDFSBackedStateStoreProvider loadMap apply HDFSBackedStateStoreProvider scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider anonfun org apache spark sql execution streaming state HDFSBackedStateStoreProvider loadMap apply HDFSBackedStateStoreProvider scala at scala Option getOrElse Option scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider org apache spark sql execution streaming state HDFSBackedStateStoreProvider loadMap HDFSBackedStateStoreProvider scala at org apache spark sql execution streaming state HDFSBackedStateStoreProvider getStore HDFSBackedStateStoreProvider scala at org apache spark sql execution streaming state StateStore get StateStore scala at org apache spark sql execution streaming state StateStoreRDD compute StateStoreRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark scheduler ResultTask runTask ResultTask scala at org apache spark scheduler Task run Task scala at org apache spark executor Executor TaskRunner run Executor scala at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java Caused by java io FileNotFoundException File does not exist tmp temporary state delta at org apache hadoop hdfs server namenode INodeFile valueOf INodeFile java at org apache hadoop hdfs server namenode INodeFile valueOf INodeFile java at org apache hadoop hdfs server namenode FSNamesystem getBlockLocationsInt FSNamesystem java at org apache hadoop hdfs server namenode FSNamesystem getBlockLocations FSNamesystem java at org apache hadoop hdfs server namenode FSNamesystem getBlockLocations FSNamesystem java at org apache hadoop hdfs server namenode NameNodeRpcServer getBlockLocations NameNodeRpcServer java at org apache hadoop hdfs protocolPB ClientNamenodeProtocolServerSideTranslatorPB getBlockLocations ClientNamenodeProtocolServerSideTranslatorPB java at org apache hadoop hdfs protocol proto ClientNamenodeProtocolProtos ClientNamenodeProtocol callBlockingMethod ClientNamenodeProtocolProtos java at org apache hadoop ipc ProtobufRpcEngine Server ProtoBufRpcInvoker call ProtobufRpcEngine java at org apache hadoop ipc RPC Server call RPC java at org apache hadoop ipc Server Handler run Server java at org apache hadoop ipc Server Handler run Server java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop ipc Server Handler run Server java at sun reflect NativeConstructorAccessorImpl newInstance Native Method at sun reflect NativeConstructorAccessorImpl newInstance NativeConstructorAccessorImpl java at sun reflect DelegatingConstructorAccessorImpl newInstance DelegatingConstructorAccessorImpl java at java lang reflect Constructor newInstance Constructor java at org apache hadoop ipc RemoteException instantiateException RemoteException java at org apache hadoop ipc RemoteException unwrapRemoteException RemoteException java at org apache hadoop hdfs DFSClient callGetBlockLocations DFSClient java at org apache hadoop hdfs DFSClient getLocatedBlocks DFSClient java at org apache hadoop hdfs DFSClient getLocatedBlocks DFSClient java at org apache hadoop hdfs DFSInputStream fetchLocatedBlocksAndGetLastBlockLength DFSInputStream java at org apache hadoop hdfs DFSInputStream openInfo DFSInputStream java at org apache hadoop hdfs DFSInputStream DFSInputStream java at org apache hadoop hdfs DFSClient open DFSClient java at org apache hadoop hdfs DistributedFileSystem doCall DistributedFileSystem java at org apache hadoop hdfs DistributedFileSystem doCall DistributedFileSystem java at org apache hadoop fs FileSystemLinkResolver resolve FileSystemLinkResolver java at org apache hadoop hdfs DistributedFileSystem open DistributedFileSystem java at org apache hadoop fs FileSystem open FileSystem java at org apache spark sql execution streaming state HDFSBackedStateStoreProvider org apache spark sql execution streaming state HDFSBackedStateStoreProvider updateFromDeltaFile HDFSBackedStateStoreProvider scala more quote I checked my spark configuration file found the value of spark sql adaptive enabled is true to modify it into false the example program can work Is this a bug thanks File does not exist tmp temporary b c bb f a c ca b b d state delta
No, add test for setting location for managed table
Yes,When we use the jdbc in pyspark if we check the lowerBound and upperBound we can give a more friendly suggestion Check the lowerBound and upperBound whether equal None in jdbc API
No,When there is any partial download or download error it is not cleaned up and sparkR session will continue to stuck with no error message SparkR hangs when there is download or untar failure
No,For the datasource other than FileFormat such as spark xml which is based on BaseRelation and uses HadoopRDD NewHadoopRDD InputFileBlockHolder doesn t work with Python UDF The method to reproduce it is running the following codes with bin pyspark packages com databricks spark xml InputFileBlockHolder doesn t work with Python UDF for datasource other than FileFormat
No,The redirect handler that is started in the HTTP port when SSL is enabled only redirects the root of the server Additional handlers do not go through the handler so if you have a deep link to the non https server you won t be redirected to the https port I tested this with the history server but it should be the same for the normal UI the fix should be the same for both too SSL redirect handler only redirects the server s root
Yes,Currently in SQL we implement overwrites by calling fs delete directly on the original data This is not ideal since we the original files end up deleted even if the job aborts We should extend the commit protocol to allow file overwrites to be managed as well Add deleteWithJob hook to internal commit protocol API
No,ml R application fails in spark with yarn cluster mode ml R example fails in yarn cluster mode due to lacks of e package
Yes,In SPARK support for AES encryption was added to the Spark network library But the authentication of different Spark processes is still performed using SASL s DIGEST MD mechanism That means the authentication part is the weakest link since the AES keys are currently encrypted using des strongest cipher supported by SASL Spark can t really claim to provide the full benefits of using AES for encryption We should add a new auth protocol that doesn t need these disclaimers AES based authentication mechanism for Spark
No,glm y family Gamma data dy ERROR RBackendHandler fit on org apache spark ml r GeneralizedLinearRegressionWrapper failed java lang reflect InvocationTargetException at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java at java lang reflect Method invoke Method java at org apache spark api r RBackendHandler handleMethodCall RBackendHandler scala at org apache spark api r RBackendHandler channelRead RBackendHandler scala at org apache spark api r RBackendHandler channelRead RBackendHandler scala at io netty channel SimpleChannelInboundHandler channelRead SimpleChannelInboundHandler java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty handler timeout IdleStateHandler channelRead IdleStateHandler java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty handler codec MessageToMessageDecoder channelRead MessageToMessageDecoder java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty handler codec ByteToMessageDecoder fireChannelRead ByteToMessageDecoder java at io netty handler codec ByteToMessageDecoder channelRead ByteToMessageDecoder java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty channel DefaultChannelPipeline HeadContext channelRead DefaultChannelPipeline java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel DefaultChannelPipeline fireChannelRead DefaultChannelPipeline java at io netty channel nio AbstractNioByteChannel NioByteUnsafe read AbstractNioByteChannel java at io netty channel nio NioEventLoop processSelectedKey NioEventLoop java at io netty channel nio NioEventLoop processSelectedKeysOptimized NioEventLoop java at io netty channel nio NioEventLoop processSelectedKeys NioEventLoop java at io netty channel nio NioEventLoop run NioEventLoop java at io netty util concurrent SingleThreadEventExecutor run SingleThreadEventExecutor java at io netty util concurrent DefaultThreadFactory DefaultRunnableDecorator run DefaultThreadFactory java at java lang Thread run Thread java Caused by java lang IllegalArgumentException glm e cdf parameter family given invalid value Gamma at org apache spark ml param Param validate params scala at org apache spark ml param ParamPair params scala at org apache spark ml param Param minus greater params scala at org apache spark ml param Params class set params scala at org apache spark ml PipelineStage set Pipeline scala at org apache spark ml regression GeneralizedLinearRegression setFamily GeneralizedLinearRegression scala at org apache spark ml r GeneralizedLinearRegressionWrapper fit GeneralizedLinearRegressionWrapper scala at org apache spark ml r GeneralizedLinearRegressionWrapper fit GeneralizedLinearRegressionWrapper scala more Error in handleErrors returnStatus conn java lang IllegalArgumentException glm e cdf parameter family given invalid value Gamma at org apache spark ml param Param validate params scala at org apache spark ml param ParamPair params scala at org apache spark ml param Param minus greater params scala at org apache spark ml param Params class set params scala at org apache spark ml PipelineStage set Pipeline scala at org apache spark ml regression GeneralizedLinearRegression setFamily GeneralizedLinearRegression scala at org apache spark ml r GeneralizedLinearRegressionWrapper fit GeneralizedLinearRegressionWrapper scala at org apache spark ml r GeneralizedLinearRegressionWrapper fit GeneralizedLinearRegressionWrapper scala at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java SparkR glm Gamma family results in error
No,Maybe the partition match have something wrong when the partition value is set to empty string alter table table name drop partition with a empty string will drop the whole table
No,The returned result is empty after table loading We should refresh the metadata cache after loading the data to the table Returned an Empty Result after Loading a Hive Table
No,There are some tests failed on Windows via AppVeyor as below due to this problem The problem is in this line https github com apache spark blob c a a e d a d b c d sql hive src main scala org apache spark sql hive execution ScriptTransformation scala L We always assume bash is located in bin bash In some cases such as installing Cygwin and use bash in cmd or using bash in Windows they are not located there script transformation does not work on Windows due to fixed bash executable location
No,While adding DistributedLDAModel training summary for SparkR I found that the logPrior for original and loaded model is different For example in the test read write DistributedLDAModel I add the test val logPrior model asInstanceOf DistributedLDAModel logPrior val logPrior model asInstanceOf DistributedLDAModel logPrior assert logPrior logPrior The test fails did not equal DistributedLDAModel returns different logPrior for original and loaded model
No,See reproduction at https databricks prod cloudfront cloud databricks com public ec e c eaaa f bcfc latest html Consider the following Cached tables are not used in SubqueryExpression
No,We have a config spark sql files ignoreCorruptFiles which can be used to ignore corrupt files when reading files in SQL Currently the ignoreCorruptFiles config has two issues and can t work for Parquet We only ignore corrupt files in FileScanRDD Actually we begin to read those files as early as inferring data schema from the files For corrupt files we can t read the schema and fail the program A related issue reported at http apache spark developers list n nabble com Skip Corrupted Parquet blocks footer tc html In FileScanRDD we assume that we only begin to read the files when starting to consume the iterator However it is possibly the files are read before that In this case ignoreCorruptFiles config doesn t work too The config ignoreCorruptFiles doesn t work for Parquet
No,I have met a problem like https issues apache org jira browse SPARK but not with this parameter Map my evaluate function return a Map public Map evaluate Text url when run spark sql with this udf getting the following exception scala MatchError interface java util Map of class java lang Class at org apache spark sql hive HiveInspectors class javaClassToDataType HiveInspectors scala at org apache spark sql hive HiveSimpleUdf javaClassToDataType hiveUdfs scala at org apache spark sql hive HiveSimpleUdf dataType lzycompute hiveUdfs scala at org apache spark sql hive HiveSimpleUdf dataType hiveUdfs scala at org apache spark sql catalyst expressions Alias toAttribute namedExpressions scala at org apache spark sql catalyst plans logical Project anonfun output apply basicOperators scala at org apache spark sql catalyst plans logical Project anonfun output apply basicOperators scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection mutable ResizableArray class foreach ResizableArray scala at scala collection mutable ArrayBuffer foreach ArrayBuffer scala at scala collection TraversableLike class map TraversableLike scala at scala collection AbstractTraversable map Traversable scala at org apache spark sql catalyst plans logical Project output basicOperators scala at org apache spark sql catalyst plans logical InsertIntoTable resolved lzycompute basicOperators scala spark sql use HIVE UDF throw exception when return a Map value
No, Update Structured Streaming Programming guide for Update Mode
No,This bug was caused by the fix for SPARK https github com apache spark pull This can be reproduced by adding the following test to PredicateSuite scala which will consistently fail val value NonFoldableLiteral Double PositiveInfinity DoubleType checkEvaluation In value List value true This bug is causing org apache spark sql catalyst expressions PredicateSuite IN to fail approximately of the time it fails anytime the value is Infinity or Infinity and the correct answer is True e g https amplab cs berkeley edu jenkins job SparkPullRequestBuilder testReport org apache spark sql catalyst expressions PredicateSuite IN https amplab cs berkeley edu jenkins job SparkPullRequestBuilder console Catalyst s IN always returns false for infinity
No,spark lda pass the optimizer em or online to the backend However LDAWrapper doesn t set optimizer based on the value from R Therefore for optimizer em the isDistributed field is FALSE which should be TRUE In addition the summary method should bring back the results related to DistributedLDAModel SparkR LDA doesn t set optimizer correctly
No,Right now if you use dropDuplicates in a stream you get an exception because attribute replacement is broken Here is an example dropDuplicates uses the same expression id for Alias and Attribute and breaks attribute replacement
Yes, remove the supportsPartial flag in AggregateFunction
No,It looks like there is some bug introduced in Spark preventing to read data from a parquet table hive support is enabled whose name starts with underscore CREATE and INSERT statements on the same table instead seems to work as expected The problem can be reproduced from spark shell through the following steps Create a table with some values scala spark sql CREATE TABLE a i INT USING parquet show scala spark sql INSERT INTO a VALUES show Select data from the just created and filled table no results scala spark sql SELECT FROM a show i rename the table so that the prefixing underscore disappears scala spark sql ALTER TABLE a RENAME TO a show select data from the just renamed table results are shown scala spark sql SELECT FROM a show i Unable to retrieve data from a parquet table whose name starts with underscore
No, Fix EventTimeWatermarkSuite delay in months and years handled correctly
No,The data in the managed table should be deleted after table is dropped However if the partition location is not under the location of the partitioned table it is not deleted as expected Users can specify any location for the partition when they adding a partition Managed Partitioned Table in InMemoryCatalog the user specified partition location is not deleted after table dropping
Yes,Remove useless databaseName from SimpleCatalogRelation Remove databaseName from SimpleCatalogRelation
No,Currently PySpark does not work with Python Running bin pyspark simply throws the error as below It works fine It seems now we should properly set these into the hijected one PySpark does not work with Python
No,When putting more than one column in the NOT IN the query may not return correctly if there is a null data We can demonstrate the problem with the following data set and query NOT IN subquery with more than one column may return incorrect results
No,Using a viewName where the the fist char is a numerical value on dataframe createOrReplaceTempView viewName String causes CreateOrReplaceTempView throws org apache spark sql catalyst parser ParseException when viewName first char is numerical
No, Add doc for Streaming Rest API
No,After https github com apache spark pull is merged I am unable to build it in my IntelliJ Got the following compilation error Unable to build compile Spark in IntelliJ due to missing Scala deps in spark tags
Yes,Since spark sql hive thriftServer singleSession is a configuration of SQL component this conf can be moved from SparkConf to StaticSQLConf When we introduced spark sql hive thriftServer singleSession all the SQL configuration can be modified in different sessions Later static SQL configuration is added It is a perfect fit for spark sql hive thriftServer singleSession Previously we did the same move for spark sql warehouse dir from SparkConf to StaticSQLConf Move spark sql hive thriftServer singleSession to SQLConf
Yes,Right now ContextCleaner referenceBuffer is ConcurrentLinkedQueue and the time complexity of the remove action is O n It can be changed to use ConcurrentHashMap whose remove is O Change ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster
Yes,SortPartitions and RedistributeData logical operators are not actually used and can be removed Note that we do have a Sort operator with global flag false that subsumed SortPartitions Remove SortPartitions and RedistributeData
No,https spark tests appspot com test details suite name org apache spark streaming BasicOperationsSuite test name rdd cleanup map and window Fix flaky test o a s streaming BasicOperationsSuite rdd cleanup map and window
Yes,I recently hit a bug of com thoughtworks paranamer paranamer which causes jackson fail to handle byte array defined in a case class Then I find https github com FasterXML jackson module scala issues which suggests that it is caused by a bug in paranamer Let s upgrade paranamer Since we are using jackson and jackson module paranamer use com thoughtworks paranamer paranamer I suggests that we upgrade paranamer to Upgrade com thoughtworks paranamer paranamer to
Yes,Currently we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog MSCK REPAIR TABLE or ALTER TABLE table RECOVER PARTITIONS Actually very hard for me to remember MSCK and have no clue what it means After the new Scalable Partition Handling the table repair becomes much more important for making visible the data in the created data source partitioned table It is desriable to add it into the Catalog interface so that users can repair the table by Add recoverPartitions API to Catalog
No,If the logical plan fails to create e g some Source options are invalid the user cannot use the code to detect the failure The only place receiving this error is Thread s UncaughtExceptionHandler This bug is because logicalPlan is lazy and when we try to create StreamingQueryException to wrap the exception thrown by creating logicalPlan it calls logicalPlan agains It s hard for the user to see the failure if StreamExecution fails to create the logical plan
No,the current implementation of Spark streaming considers a batch is completed no matter the results of the jobs https github com apache spark blob db bc d e feb ba b d c c streaming src main scala org apache spark streaming scheduler JobScheduler scala L Let s consider the following case A micro batch contains jobs and they read from two different kafka topics respectively One of these jobs is failed due to some problem in the user defined logic after the other one is finished successfully The main thread in the Spark streaming application will execute the line mentioned above and another thread checkpoint writer will make a checkpoint file immediately after this line is executed Then due to the current error handling mechanism in Spark Streaming StreamingContext will be closed https github com apache spark blob db bc d e feb ba b d c c streaming src main scala org apache spark streaming scheduler JobScheduler scala L the user recovers from the checkpoint file and because the JobSet containing the failed job has been removed taken as completed before the checkpoint is constructed the data being processed by the failed job would never be reprocessed I might have missed something in the checkpoint thread or this handleJobCompletion or it is a potential bug Potential Issue of Semantics of BatchCompleted
No,Currently SparkR tests R run tests sh succeeds only once because test sparkSQL R does not clean up the test table people As a result the test data is accumulated at every run and the test cases fail The following is the failure result for the second run Fix SparkR SQL Test to drop test table
No,Internally we use CalendarInterval to parse the delay Non determinstic intervals like month and year are handled such a way that the generated delay in milliseconds is delayThreshold is in months or years Event time watermark delay threshold specified in months or years gives incorrect results
No,When reading below mentioned csv data even though the maximum decimal precision is following exception is thrown java lang IllegalArgumentException requirement failed Decimal precision exceeds max precision Decimal Unable to read given csv data Excepion java lang IllegalArgumentException requirement failed Decimal precision exceeds max precision
No,Unsupported operations checking dont check whether AggregationExpression have isDistinct true So streamingDf groupBy agg countDistinct key gives incorrect results Distinct aggregates give incorrect answers on streaming dataframes
No,smilegator has found that the following query does not raise a syntax error note the GROUP BY clause is commented out There could be multiple values of c cv for each value of avg c cv Output non aggregate expressions without GROUP BY in a subquery does not yield an error
Yes,SparkR mllib R is getting bigger as we add more ML wrappers I d like to split it into multiple files to make us easy to maintain mllibClassification R mllibRegression R mllibClustering R mllibFeature R or mllib classification R mllib regression R mllib clustering R mllib features R For R convention it s more prefer the first way And I m not sure whether R supports the second organized way will check later Please let me know your preference I think the start of a new release cycle is a good opportunity to do this since it will involves less conflicts If this proposal was approved I can work on it cc felixcheung josephkb mengxr Split SparkR mllib R into multiple files
No,We are trying to run a sql query on our spark cluster and extracting around million records through SparkSQL ThriftServer interface This query works fine for Spark version however for spark thrift server hangs after fetching data from a few partitions we are using incremental collect mode with partitions As per documentation max memory taken up by thrift server should be what is required by the biggest data partition But we observed that Thrift server is not releasing the old partitions memory whenever the GC occurs even though it has moved to next partition data fetches which is not the case with version On further investigation we found that SparkExecuteStatementOperation scala was modified for SPARK SQL fix spark sql thrift server FetchResults bug and result set iterator was duplicated to keep a reference to the first set val itra itrb iter duplicate iterHeader itra iter itrb We suspect that this is resulting in the memory not being cleared on GC To confirm this we created an iterator in our test class and fetched the data once without duplicating and second time with creating a duplicate we could see that in first instance it ran fine and fetched the entire data set while in second instance driver hanged after fetching data from a few partitions SparkSQL ThriftServer hangs while extracting huge data volumes in incremental collect mode
No,This is a bug introduced by subquery handling generateTreeString numbers trees including innerChildren used to print subqueries but getNodeNumbered ignores that As a result getNodeNumbered is not always correct Repro Output looks like Note that should be the Project node but getNodeNumbered ignores innerChild and as a result returns the wrong one getNodeNumbered and generateTreeString are not consistent
No,We currently define statistics in UnaryNode This has a few issues This can aggressively underestimate the size for Project We assume each array map has elements which is an overestimate If the user projects a single field out of a deeply nested field this would lead to huge underestimation A safer sane default is probably It is not a property of UnaryNode to propagate statistics this way It should be a property of Project Project UnaryNode is way too aggressive in estimating statistics
Yes,Make StreamExecution and progress classes serializable because it is too easy for it to get captured with normal usage Make StreamExecution and progress classes serializable
Yes, Expose event time time stats through StreamingQueryProgress
No,How to reproduce it NoSuchElementException will throw since SPARK if a broadcast cannot cache in memory The reason is that that change cannot cover unrolled hasNext in next function Cann t read broadcast if broadcast blocks are stored on disk
Yes,When starting a stream with a lot of backfill and maxFilesPerTrigger the user could often want to start with most recent files first This would let you keep low latency for recent data and slowly backfill historical data It s better to add an option to control this behavior Make FileStream be able to start with most recent files
Yes,Implement a wrapper in SparkR to support bisecting k means Bisecting k means wrapper in SparkR
No,How to reproduce with standalone mode Launch a spark master Launch a spark shell At this point there is no executor associated with this application Launch a slave Now there is an executor assigned to the spark shell However there is no link to stdout stderr on the executor page please see https issues apache org jira secure attachment screenshot png executor page fails to show log links if executors are added after an app is launched
No,It seems the CheckAnalysis rule introduced by SPARK is incorrect rejecting this TPCDS query which ran fine in Spark There doesn t seem to be any obvious error in the query or the check rule though in the plan below the scalar subquery s condition field is scalar subquery cs item sk i item sk which should reference cs item sk Nonetheless CheckAnalysis complains that cs item sk is not referenced by the scalar subquery predicates analysis error CheckAnalysis rejects TPCDS query
No,We publish source archives of the SparkR package now in RCs and in nightly snapshot builds One of the problems that still remains is that install spark does not work for these as it looks for the final Spark version to be present in the apache download mirrors SparkR install spark does not work for RCs snapshots
Yes,spark logit is added in We need to update spark vignettes to reflect the changes This is part of SparkR QA work Update spark logit in sparkr vignettes
Yes,Currently when users use Python UDF in Filter BatchEvalPython is always generated below FilterExec However not all the predicates need to be evaluated after Python UDF execution Thus we can push down the predicates through BatchEvalPython Push Down Filter Through BatchEvalPython
No,We currently rely on FileFormat implementations to override toString in order to get a proper explain output It d be better to just depend on shortName for those Before Provide consistent format output for all file formats
Yes,An informal poll of a bunch of users found this name to be more clear Rename recentProgresses to recentProgress
No,Found an inconsistent behavior when using parquet Inconsistent behavior after writing to parquet files
No,When SparkContext stop is called in Utils tryOrStopSparkContext the following three places it will cause deadlock because the stop method needs to wait for the thread running stop to exit ContextCleaner keepCleaning LiveListenerBus listenerThread run TaskSchedulerImpl start Deadlock when SparkContext stop is called in Utils tryOrStopSparkContext
No,When running Sql queries on large datasets Job fails with stack overflow warning and it shows it is requesting lots of executors Looks like there is no limit to number of executors or not even having an upperbound based on yarn available resources If you notice in the error above YARN is trying to request executor containers whereas the available cores are The Driver is requesting for executor s which too high This exception should be fixed spark should be able to control the number of executor and should not throw stack overslow
No,Running query with decreased executor memory using GB executors instead of GB on TB parquet database using the Spark master dated gave IndexOutOfBoundsException The query is as follows Likely an integer overflow issue java lang IndexOutOfBoundsException running query Spark SQL on TB
Yes,Otherwise other threads cannot query the content in MemorySink when DataFrame collect takes long time to finish MemorySink should not call DataFrame collect when holding a lock
No, Datasets crash compile exception when mapping to immutable scala map
No,The AIC calculation in Binomial GLM seems to be wrong when there are weights The result is different from that in R The current implementation is Fix wrong AIC calculation in Binomial GLM
Yes,Many Spark developers often want to test the runtime of some function in interactive debugging and testing It d be really useful to have a simple spark time method that can test the runtime SparkSession time a simple timer function
No,this is a bug in the branch but i don t think it was in rc generation for the aggregator result NPE in generated SpecificMutableProjection for Aggregator
No,When converting an RDD with a float type field to a spark dataframe with an IntegerType LongType schema field spark and silently convert the field values to nulls instead of throwing an error like LongType can not accept object in type However this seems to be fixed in Spark The following example should make the problem clear For the purposes of my computation I m doing a mapPartitions on a spark data frame and for each partition converting it into a pandas data frame doing a few computations on this pandas dataframe and the return value will be a list of lists which is converted to an RDD while being returned from mapPartitions for all partitions This RDD is then converted into a spark dataframe similar to the example above using sqlContext createDataFrame rdd schema The rdd has a column that should be converted to a LongType in the spark data frame but since it has missing values it is a float type When spark tries to create the data frame it converts all the values in that column to nulls instead of throwing an error that there is a type mismatch Automatic null conversion bug instead of throwing error when creating a Spark Datarame with incompatible types for fields
No,Below are the files directories generated for three inserts againsts a Hive table Ideally we should drop the created staging files and temporary data files after each insert CTAS The temporary files directories could accumulate a lot when we issue many inserts since each insert generats at least six files This could eat a lot of spaces and slow down the JVM termination Insertion CTAS against Hive Tables Staging Directories and Data Files Not Dropped Until Normal Termination of JVM
Yes,We currently have function input file name to get the path of the input file but don t have functions to get the block start offset and length This patch introduces two functions input file block start returns the file block start offset or if not available input file block length returns the file block length or if not available input file block start and input file block length function
No,Poisson GLM fails for many standard data sets The issue is incorrect initialization leading to almost zero probability and weights The following simple example reproduces the error Will create a PR Poisson GLM fails due to wrong initialization
No, Bump master branch version to SNAPSHOT
No,With a local spark instance built with hive support Pyarn Phadoop Dhadoop version Phive Phive thriftserver The following script sequence works in Pyspark without any error in x but fails in x The error produced is The error goes away if sqlContext is replaced with sqlContext in the last error line Since the SQLContext class is preserved for backward compatibility the changes in x break scripts notebooks that follow the above pattern of calls and used to run fine with x Backward compatibility creating a Dataframe on a new SQLContext object fails with a Derby error
No,The current json path parser fails to parse expressions like key which are used for named expressions with spaces Json path implementation fails to parse key
No, improve the error message of using join
No,To be able to restart StreamingQueries across Spark version we have already made the logs offset log file source log file sink log use json We should added tests with actual json files in the Spark such that any incompatible changes in reading the logs is immediately caught Add tests to ensure stability of that all Structured Streaming log formats
No, Update Apache docs regard watermarking in Structured Streaming
No,input file name does not return the file name but empty string instead when it is used as input for UDF in PySpark as below with the data as below This seems PySpark specific issue input file name function does not work with UDF
No,SPARK changes the event log format of Structured Streaming We should make sure our changes not break the history server Ignore Structured Streaming logs in history server
No,start thriftserver sh can reproduce this spark daemon sh arguments error lead to throws Unrecognized option
No,Install SparkR from source package ie R CMD INSTALL SparkR tar gz Start SparkR not from sparkR shell library SparkR sparkR session Notice SparkR hangs when it couldn t find spark submit to launch the JVM backend If SparkR is running as a package and it has previously downloaded Spark Jar it should be able to run as before without having to set SPARK HOME Basically with this bug the auto install Spark will only work in the first session This seems to be a regression on the earlier behavior SparkR hangs at session start when installed as a package without SPARK HOME set
No,There are some weird issues with exploding Python UDFs in SparkSQL There are cases where based on the DataType of the exploded column the result can be flat out wrong or corrupt Seems like something bad is happening when telling Tungsten the schema of the rows during or after applying the UDF Please check the code below for reproduction Notebook https databricks prod cloudfront cloud databricks com public ec e c eaaa f bcfc latest html Corruption and Correctness issues with exploding Python UDFs
Yes,AggregateFunction currently implements ImplicitCastInputTypes which enables implicit input type casting This can lead to unexpected results and should only be enabled when it is suitable for the function at hand AggregateFunction should not ImplicitCastInputTypes
No,Key Points to re produce issue or more union clauses One column is sum aggregate in one union clause and is Integer type in other union clause Another column has different date types in union clauses The reason of issue Step Apply TypeCoercion WidenSetOperationTypes add project with cast since the union clauses has different datatypes for one column With union clauses the inner union clause also be projected with cast Step Apply TypeCoercion FunctionArgumentConversion the return type of sum int will be extended to BigInt meaning one column in union clauses changed datatype Step Apply TypeCoercion WidenSetOperationTypes again another cast project added in inner union clause since sum int datatype changed at this point the reference of project ON inner union will be missed since the project IN inner union is newly added see the Analyzed Logical Plan Solutions to fix Since set operation type coercion should be applied after inner clause be stabled apply WidenSetOperationTypes at last will fix the issue To avoiding multi level projects on set operation clause handle the existing cast project carefully in WidenSetOperationTypes should be also work Appreciate for any comments Missing Reference in Multi Union Clauses Cause by TypeCoercion
No,The recently added CollapseWindow optimizer rule changes the column order of attributes This actually modifies the schema of the logical plan which optimization should not do and breaks collect in a subtle way we bind the row encoder to the output of the logical plan and not the optimized plan For example the following code Collapse Window optimizer rule changes column order
Yes,Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly See discussion at https github com apache spark pull discussion r Move DT RF GBT Param setter methods to subclasses
Yes,We should include in Spark distribution the built source package for SparkR This will enable help and vignettes when the package is used Also this source package is what we would release to CRAN R Include package vignettes and help pages build source package in Spark distribution
No,Smells like another optimizer bug that s similar to SPARK and SPARK I m seeing this on and on master at commit fb bbe aabe fd a fb fa I don t have a minimal repro for this yet but the error I m seeing is Note the error at the end when Spark tries to print the physical plan I ve scrubbed some Project fields from the plan to simplify the display but if I ve scrubbed anything you think is important let me know I can get around this problem by adding a persist right before the operation that fails The failing operation is a filter Any clues on how I can boil this down to a minimal repro Any clues about where the problem is persist resolves java lang RuntimeException Invalid PythonUDF requires attributes from more than one child
No,Full outer join with a correlated predicate in the left operand in a subquery may return incorrect results Example Full outer join in correlated subquery returns incorrect results
Yes, remove OverwriteOptions
No,In HyperLogLogPlusPlus if the relative error is so small that p it will cause ArrayIndexOutOfBoundsException in THRESHOLDS p We should check p and when p regress to the original HLL result and use the small range correction they use Fix HLL with small relative error
No,TaskMemoryManager has a memory leak detector that gets called at task completion callback and checks whether any memory has not been released If they are not released by the time the callback is invoked TaskMemoryManager releases them The current error message says something like the following In practice there are multiple reasons why these can be triggered in the normal code path e g limit or task failures and the fact that these messages are log means the leak is fixed by TaskMemoryManager To not confuse users we should downgrade the message from warning to debug level and avoid using the word leak since it is not actually a leak Downgrade the memory leak warning message
No,Manly the issue is clarified in the following example Given a Dataset scala data show a b theoretically when we call na fill nothing should change while the current result is scala data na fill show a b na fill miss up original values in long integers
No,Due to a bug in TaskSchedulerImpl the complete sudden loss of an executor may cause a TaskSetManager to be leaked causing ShuffleDependencies and other data structures to be kept alive indefinitely leading to various types of resource leaks including shuffle file leaks In a nutshell the problem is that TaskSchedulerImpl did not maintain its own mapping from executorId to running task ids leaving it unable to clean up taskId to taskSetManager maps when an executor is totally lost Executor loss may cause TaskSetManager to be leaked
No,The merging algorithm in UnsafeShuffleWriter does not consider encryption and when it tries to merge encrypted files the result data cannot be read since data encrypted with different initial vectors is interleaved in the same partition data This leads to exceptions when trying to read the files during shuffle This is our internal branch so don t worry if lines don t necessarily match UnsafeShuffleWriter corrupts encrypted shuffle files when merging
No, Cannot filter by nonexisting column in parquet file
No,The above two DataFrameReader JDBC APIs ignore the user specified parameters of parallelism degree Concurrent Fetching DataFrameReader JDBC APIs Do Not Work
Yes,trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming it let us no choice but to implement one for ourself this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only here is how you can use it endpoint root streaming api v Endpoint Meaning statistics Statistics information of stream receivers A list of all receiver streams receivers stream id Details of the given receiver stream batches A list of all retained batches batches batch id Details of the given batch batches batch id operations A list of all output operations of the given batch batches batch id operations operation id Details of the given operation given batch Add a REST api to spark streaming
No,A Spark user may have to provide a sensitive information for a Spark configuration property or a source out an environment variable in the executor or driver environment that contains sensitive information A good example of this would be when reading writing data from to S using Spark The S secret and S access key can be placed in a hadoop credential provider https hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html However one still needs to provide the password for the credential provider to Spark which is typically supplied as an environment variable to the driver and executor environments This environment variable shows up in logs and may also show up in the UI For logs it shows up in a few places A Event logs under SparkListenerEnvironmentUpdate event B YARN logs when printing the executor launch context For UI it would show up in the Environment tab but it is redacted if it contains the words password or secret in it And these magic words are hardcoded https github com apache spark blob a d cd daa d bf bde c e e a core src main scala org apache spark ui env EnvironmentPage scala L and hence not customizable This JIRA is to track the work to make sure sensitive information is redacted from all logs and UIs in Spark while still being passed on to all relevant places it needs to get passed on to Redact sensitive information from Spark logs and UI
No,A timeout should inherit from RuntimeException as its not a fatal error Timeouts shouldn t be AssertionErrors
No,Using limit on a DataFrame prior to groupBy will lead to a crash Repartitioning will avoid the crash will crash df limit groupBy user id count show will work df limit coalesce groupBy user id count show will work df limit repartition user id groupBy user id count show Here is a reproducible example along with the error message quote df spark createDataFrame user id genre id df show user id genre id df groupBy user id count show user id count df limit groupBy user id count show Stage WARN TaskSetManager Lost task in stage TID lvsp hdn stubprod com java lang NullPointerException at org apache spark sql catalyst expressions GeneratedClass GeneratedIterator agg doAggregateWithKeys Unknown Source at org apache spark sql catalyst expressions GeneratedClass GeneratedIterator processNext Unknown Source at org apache spark sql execution BufferedRowIterator hasNext BufferedRowIterator java at org apache spark sql execution WholeStageCodegenExec anonfun anon hasNext WholeStageCodegenExec scala at org apache spark sql execution SparkPlan anonfun apply SparkPlan scala at org apache spark sql execution SparkPlan anonfun apply SparkPlan scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark scheduler ResultTask runTask ResultTask scala at org apache spark scheduler Task run Task scala at org apache spark executor Executor TaskRunner run Executor scala at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java quote limit groupBy leads to java lang NullPointerException
No,When you run some memory heavy spark job Spark driver may consume more memory resources than host available to provide In this case OOM killer comes on scene and successfully kills a spark submit process The pyspark SparkContext is not able to handle such state of things and becomes completely broken You cannot stop it as on stop it tries to call stop method of bounded java context jsc and fails with Py JError because such process no longer exists as like as the connection to it You cannot start new SparkContext because you have your broken one as active one and pyspark still is not able to not have SparkContext as sort of singleton The only thing you can do is shutdown your IPython Notebook and start it over Or dive into SparkContext internal attributes and reset them manually to initial None state The OOM killer case is just one of the many any reason of spark submit crash in the middle of something leaves SparkContext in a broken state Example on error log on sc stop in broken state OOM killer may leave SparkContext in broken state causing Connection Refused errors
No,We should record the watermark into the persistent log and recover it to ensure determinism Record and recover watermark
No,The current documentation for DateDiff does not make it clear which one is the start date and which is the end date The example is also wrong about the direction Fix documentation for DateDiff
No, Scalar subquery with extra group by columns returning incorrect result
No,Spark GeneralizedLinearRegression can handle collinear data since the underlying WeightedLeastSquares can be solved by local l bfgs rather than normal But the SparkR wrapper spark glm throw errors when fitting on collinear data After depth study of this error I found it was caused the standard error of coefficients t value and p value are not available when the underlying WeightedLeastSquares was solved by local l bfgs So the coefficients matrix was generated failed SparkR spark glm error on collinear data
No,I have a pretty standard stream I call writeStream foreach start and get What do ForeachSink fails with assertion failed No plan for EventTimeWatermark
Yes,These two methods were added to Scala Datasets but are not available in Python yet Add withWatermark and checkpoint to python dataframe
No,I have stumbled onto a corner case where an INNER join appears to return incorrect results I believe the join should behave as the identity but instead some values are shuffled around and some are just plain wrong This can be reproduced as follows joining Correctness issue in INNER join result with window functions
Yes,Refactor StaticInvoke Invoke and NewInstance as Introduce InvokeLike to extract common logic from StaticInvoke Invoke and NewInstance to prepare arguments Remove unneeded null checking and fix nullability of NewInstance Modify to short circuit if arguments have null when propageteNull true Refactor StaticInvoke Invoke and NewInstance
No,I have a old table that was created without providing a schema Seems branch fail to load it and says that the schema is corrupt With spark sql debug enabled I get the metadata by using describe formatted Spark SQL fails to load tables created without providing a schema
No,The following test fails with a ClassCastException due to oddities in how Jackson object mapping works breaking the SQL tab in the history server SparkListenerDriverAccumUpdates event does not deserialize properly in history server
No,Running a query on TB parquet database using the Spark master dated dump cores on Spark executors The query is TPCDS query though this query is not the only one can produce this core dump just the easiest one to re create the error Spark output that showed the exception This is not easily reproducible on smaller data volumes e g TB or TB but easily reproducible on TB so look into data types that may not be big enough to handle hundreds of billion core dumped running Spark SQL on large data volume TB
No,When using a JDBC data source the isin function generates invalid SQL syntax when called with an empty array which causes the JDBC driver to throw an exception If the array is not empty it works fine In the below example you can assume that SOURCE CONNECTION SQL DRIVER and TABLE are all correctly defined isin causing SQL syntax error with JDBC
No,When the exception is an invocation exception during function lookup we return a useless confusing error message For example Returned Message Null when Hitting an Invocation Exception of Function Lookup
No,Currently when CTE is used in RunnableCommand the Analyzer does not replace the logical node With The child plan of RunnableCommand is not resolved However the output of the With plan node looks very confusing For example Weird Plan Output when CTE used in RunnableCommand
No,spark randomForest classification throws exception when training on libsvm data It can be reproduced as following This error is caused by the label column of the R formula already exists we can not force to index label However it must index the label for classification algorithms so we need to rename the RFormula labelCol to a new value and then we can index the original label This issue also appears at other algorithms spark naiveBayes spark glm only for binomial family and spark gbt only for classification SparkR spark randomForest classification throws exception when training on libsvm data
No,This assertion https github com apache spark blob eaad daed b e a b aa d e sql core src main scala org apache spark sql execution streaming StreamExecution scala L fails when you run a stream against json data that is stored in partitioned folders if you manually specify the schema and that schema omits the partitioned columns My hunch is that we are inferring those columns even though the schema is being passed in manually and adding them to the end While we are fixing this bug it would be nice to make the assertion better Truncating is not terribly useful as at least in my case it truncated the most interesting part I changed it to this while debugging I also tried specifying the partition columns in the schema and now it appears that they are filled with corrupted data Inferred partition columns cause assertion error
No,This test suite fails occasionally on Jenkins due to OOM errors I ve already reproduced it locally but haven t figured out the root cause We should probably disable it temporarily before getting it fixed so that it doesn t break the PR build too often ObjectHashAggregateSuite is being flaky occasional OOM errors
No,SPARK fixes regexp replace when it is serialized One of the reviews requested updating the tests so that all expressions that are tested using checkEvaluation are first serialized That caused several new test failures so this issue is to add serialization to the tests pick commit https github com apache spark commit and fix the bugs serialization exposes Test that expressions can be serialized
No,Steps to reproduce Launch spark shell Run the following scala code via Spark Shell scala val hivesampletabledf sqlContext table hivesampletable scala import org apache spark sql DataFrameWriter scala val dfw DataFrameWriter hivesampletabledf write scala sqlContext sql CREATE TABLE IF NOT EXISTS hivesampletablecopypy clientid string querytime string market string deviceplatform string devicemake string devicemodel string state string country string querydwelltime double sessionid bigint sessionpagevieworder bigint scala dfw insertInto hivesampletablecopypy scala val hivesampletablecopypydfdf sqlContext sql SELECT clientid querytime deviceplatform querydwelltime FROM hivesampletablecopypy WHERE state Washington AND devicemake Microsoft AND querydwelltime hivesampletablecopypydfdf show in HDFS in our case WASB we can see the following folders hive warehouse hivesampletablecopypy hive staging hive hive warehouse hivesampletablecopypy hive staging hive ext hive warehouse hivesampletablecopypy hive staging hive the issue is that these don t get cleaned up and get accumulated with the customer we have tried setting SET hive exec stagingdir tmp hive in hive site xml didn t make any difference hive staging folders are created under the folder hive warehouse hivesampletablecopypy we have tried adding this property to hive site xml and restart the components hive exec stagingdir hive exec scratchdir user name staging a new hive staging folder was created in hive warehouse folder moreover please understand that if we run the hive query in pure Hive via Hive CLI on the same Spark cluster we don t see the behavior so it doesn t appear to be a Hive issue behavior in this case this is a spark behavior I checked in Ambari spark yarn preserve staging files false in Spark configuration already The issue happens via Spark submit as well customer used the following command to reproduce this spark submit test hive staging cleanup py Hive staging folders created from Spark hiveContext are not getting cleaned up
No,This query fails with a NullPointerException on line https github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst expressions regexpExpressions scala L The problem is that POSEXPLODE is causing the REGEXP REPLACE to be serialized after it is instantiated The null value is a transient StringBuffer that should hold the result The fix is to make the result value lazy Regular expression replace throws NullPointerException when serialized
Yes,Spark s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing IO performance improvements made in Spark In order to fix this performance problem we should re implement those read paths in terms of TextFileFormat Use TextFileFormat in implementation of CSVFileFormat
Yes,As of today I could not access rdd localCheckpoint in pyspark This is an important issue for machine learning people as we often have to iterate algorithms and perform operations like joins in each iteration If the lineage is not truncated the memory usage the lineage and computation time explode rdd localCheckpoint seems like the most straightforward way of truncating the lineage but the python API does not expose it Expose RDD localCheckpoint in PySpark
No,SPARK broke files and archives options The check should be null instead of if localizedPath null throw new IllegalArgumentException s Attempt to add file multiple times to the distributed cache YARN files archives broke
No,It has been discovered that there is a fair bit of consistency in the documentation of summary functions eg For instance what should be listed for the return value Should it be a name or a phrase or should it be a list of items and should there be a longer description on what they mean or reference link to Scala doc We will need to review this for all model summary implementations in mllib R Update R API documentation on ml model summary
No,Test case initialization order under Maven and SBT are different Maven always creates instances of all test cases and then run them all together This fails ObjectHashAggregateSuite because the randomized test cases there register a temporary Hive function right before creating a test case and can be cleared while initializing other successive test cases In SBT this is fine since the created test case is executed immediately after creating the temporary function To fix this issue we should put initialization destruction code into beforeAll and afterAll ObjectHashAggregateSuite fails under Maven builds
No,While trying to reproduce SPARK in SQL I found the following SQL query fails in Spark via spark beeline with a CassCastException ClassCastException during count distinct
Yes,I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite and the end to end tests FunctionsSuite It d be great to make all test suites consistent with this naming convention Use consistent naming for expression test suites
Yes,hash scala was getting pretty long and it s not obvious that hash expressions belong there Creating a hash scala to put all the hash expressions Move hash expressions from misc scala into hash scala
No,I run the example straight out of the api docs for toLocalIterator and it gives a time out exception toLocalIterator yields time out error on pyspark
No,StandaloneSchedulerBackend dead is called in a RPC thread so it should not call SparkContext stop in the same thread SparkContext stop will block until all RPC threads exit if it s called inside a RPC thread it will be dead lock Potential deadlock in StandaloneSchedulerBackend dead
No,StringIndexerModel won t get collected by GC in Java even when deleted in Python It can be reproduced by this The issue is similar to SPARK and can be probably fixed by calling jvm detach in model s destructor This is implemented in pyspark mlib common JavaModelWrapper but missing in pyspark ml wrapper JavaWrapper Other models in ml package may also be affected by this memory leak Memory leak in PySpark StringIndexer
No,Having a schema with a nullable column thrown an java lang NumberFormatException null when the data delimeter isn t specified in the csv Specifying the schema Reason The csv line is parsed into a Map indexSafeTokens which is short of one value So indexSafeTokens index throws a NullpointerException reading the optional value which isn t in the Map The NullpointerException is then given to the CSVTypeCast castTo datum String as the datum value The subsequent NumberFormatException is thrown due to the fact that a NullpointerException cannot be cast into the Type Possible fix Use the provided schema to parse the line with the correct number of columns Since its nullable implement a try catch on CSVRelation csvParser indexSafeTokens index NumberFormatException when reading csv for a nullable column
No,FileStressSuite doesn t report errors when they occur Improve error reporting for FileStressSuite in streaming
No,Dunno if I m misinterpreting something here but this seems like a bug in how UDFs work or in how they interface with the optimizer Here s a basic reproduction I m using length udf just for illustration it could be any UDF that accesses fields that have been aliased It looks like from the second execution plan that BatchEvalPython somehow gets the unaliased column names whereas the Project right above it gets the aliased names UDFs don t see aliased column names
No,I am running into a runtime exception when a DataSet is holding an Empty object instance for an Option type that is holding non nullable field For instance if we have the following case class case class DataRow id Int value String Then DataSet Option DataRow can only hold Some DataRow objects and cannot hold Empty If it does so the following exception is thrown The bug can be reproduce by using the program https gist github com aniketbhatnagar ed f d defe c afaa e DataSet API RuntimeException Null value appeared in non nullable field when holding Option Case Class
No,hive exec stagingdir have no effect in spark this relevant to https issues apache org jira browse SPARK hive exec stagingdir have no effect in spark
No,Error message is below ClassCastException occurs when using select query on ORC file
Yes,Column expr is private sql but it s an actually really useful field to have for debugging We should open it up similar to how we use QueryExecution Make Column expr public
No,While running a Spark job we see that the job fails because of executor OOM with following stack trace The code is trying to reuse the BytesToBytesMap after spilling by calling the reset function see https github com facebook FB Spark blob fb core src main java org apache spark unsafe map BytesToBytesMap java L The reset function is releasing all memory pages but its not reseting the pointer array If the pointer array size has grown beyond the fair share the BytesToBytes map is not being allocated any memory page further and hence the OOM Executor OOM due to a memory leak in BytesToBytesMap
No,I have wide dataframes that contain nested data structures when I explode one of the dataframes it doesn t include records with an empty nested structure outer explode not supported So I create a similar dataframe with null values and union them together See SPARK for more details as to why I have to do this I was hoping that SPARK was going to address my issue but it does not I was asked by lwlin to open this JIRA I will attach a class org apache spark sql catalyst expressions GeneratedClass SpecificUnsafeProjection grows beyond KB
No,Running GraphX triangle count on large ish file results in the Invalid initial capacity error when running on Spark tested on Spark and You can see the results at http bit ly eQKWDN Running the same runs as well Spark GraphFrames http bit ly fAS W Reference Stackoverflow question Spark GraphX requirement failed Invalid initial capacity http stackoverflow com questions spark graphx requirement failed invalid initial capacity GraphX Invalid initial capacity when running triangleCount
No,just run the following spark says Task not serializable task not serializable with groupByKey mapGroups map
No,Since Spark the following pyspark snippet fails with AnalysisException The second argument of First should be a boolean literal but it s not restricted to Python similar AnalysisException in first last during aggregation
No,The current implementation of Poisson GLM seems to allow only positive values See below This is not correct since the support of Poisson includes the origin override def initialize y Double weight Double Double require y color red color The response variable of Poisson family s should be positive but got y y The fix is easy just change it to require y color red color The response variable of Poisson family GeneralizedLinearRegression Wrong Value Range for Poisson Distribution
No,The following command will fails for spark spark files spark jars should not be passed to driver in yarn mode
No,The following error message points to a random column I m not actually using in my query making it hard to diagnose CC marmbrus Misleading Error Message for Aggregation Without Window GroupBy
No,this is me on purpose trying to break spark sql codegen to uncover potential issues by creating arbitrately complex data structures using primitives strings basic collections map seq option tuples and case classes first example nested case classes gen BaseMutableProjection target MutableRow row info mutableRow row info return this info info info Provide immutable access to the last projected row info public InternalRow currentValue info return InternalRow mutableRow info info info public java lang Object apply java lang Object i info InternalRow i InternalRow i info info info info Object obj Expression references eval null info org apache spark sql expressions Aggregator value org apache spark sql expressions Aggregator obj info info boolean isNull i isNullAt info UTF String value isNull null i getUTF String info info boolean isNull isNull info final java lang String value isNull null java lang String value toString info isNull value null info boolean isNull false isNull info final com tresata spark sql Struct value isNull null com tresata spark sql Struct value finish value info isNull value null info info boolean isNull false info InternalRow value null info if false isNull info info final InternalRow value null info isNull true info value value info else info info boolean isNull false info this values new Object info if isNull info throw new RuntimeException errMsg info info info boolean isNull false info final com tresata spark sql Struct value isNull null com tresata spark sql Struct value a info isNull value null info boolean isNull false info InternalRow value null info if false isNull info info final InternalRow value null info isNull true info value value info else info info boolean isNull false info values new Object apply i info apply i info final InternalRow value new org apache spark sql catalyst expressions GenericInternalRow values info this values null info isNull isNull info value value info info if isNull info values null info else info values value info info if isNull info throw new RuntimeException errMsg info info info boolean isNull false info final com tresata spark sql Struct value isNull null com tresata spark sql Struct value b info isNull value null info boolean isNull false info InternalRow value null info if false isNull info info final InternalRow value null info isNull true info value value info else info info boolean isNull false info values new Object apply i info apply i info final InternalRow value new org apache spark sql catalyst expressions GenericInternalRow values info this values null info isNull isNull info value value info info if isNull info values null info else info values value info info final InternalRow value new org apache spark sql catalyst expressions GenericInternalRow values info this values null info isNull isNull info value value info info this isNull isNull info this value value info info copy all the results into MutableRow info info if this isNull info mutableRow update this value info else info mutableRow setNullAt info info info return mutableRow info info noformat Broken Spark SQL Codegen
No,create table t Name text Id integer insert into t values Mike val df sqlContext read jdbc jdbcUrl t new Properties df filter Id show df filter Id show Error Cause org postgresql util PSQLException ERROR column id does not exist Position at org postgresql core v QueryExecutorImpl receiveErrorResponse QueryExecutorImpl java at org postgresql core v QueryExecutorImpl processResults QueryExecutorImpl java at org postgresql core v QueryExecutorImpl execute QueryExecutorImpl java at org postgresql jdbc PgStatement execute PgStatement java at org postgresql jdbc PgStatement executeWithFlags PgStatement java at org postgresql jdbc PgStatement executeQuery PgStatement java at org apache spark sql execution datasources jdbc JDBCRDD compute JDBCRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala I am working on fix for this issue will submit PR soon jdbc datasource read fails when quoted columns eg mixed case reserved words in source table are used in the filter
Yes,Plan Mark it very explicit in Spark that support for the aforementioned environments are deprecated Remove support it Spark Also see mailing list discussion http apache spark developers list n nabble com Straw poll dropping support for things like Scala tp p html More officially deprecate support for Python Java and Scala
No,when run a sql with distinct on spark github master branch it throw UnresolvedException For example run a test case on spark branch master with sql RewriteDistinctAggregates UnresolvedException when a UDAF has a foldable TypeCheck
No,ERROR src main java org apache spark util collection unsafe sort UnsafeExternalSorter java sizes LineLength Line is longer than characters found WARNING checkstyle check violations detected but failOnViolation set to false spark branch s spark release publish failed because style check failed
No,Code logic looks like this Spark generated code causes CompileException when groupByKey reduceGroups and map are used
Yes,Whenever we aggregate data by event time we want to consider data is late and out of order in terms of its event time Since we keep aggregate keyed by the time as state the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data Since the state is a store in memory we have to prevent building up of this unbounded state Hence we need a watermarking mechanism by which we will mark data that is older beyond a threshold as too late and stop updating the aggregates with them This would allow us to remove old aggregates that are never going to be updated thus bounding the size of the state Here is the design doc https docs google com document d z Pazs v rA azvmYhu I xwqaNQl ZLIS xhkfCQ edit usp sharing Observed delay based event time watermarks
No,Querying on a global temp view throws Table or view not found exception when Hive support is enabled Testcase to reproduce the problem The test needs to run when hive support is enabled Cause HiveSessionCatalog lookupRelation does not check for the global temp views Unable to query global temp views when hive support is enabled
No,As above conf option value is not enclosed by for example if we have a multi value config like quote spark driver extraJavaOptions Dlog j configuration file conf log j server properties Dalluxio user file writetype default THROUGH quote Without the next alluxio config will be treated as spark submit options and cause error MesosClusterScheduler generate bad command options
No,Executor sends AskPermissionToCommitOutput to driver failed and retry another sending Driver receives AskPermissionToCommitOutput messages and handles them But executor ignores the first response true and receives the second response false The TaskAttemptNumber for this partition in authorizedCommittersByStage is locked forever Driver enters into infinite loop h Driver Log Sending AskPermissionToCommitOutput failed driver enter into task deadloop
No,When multiple records have the minimum value the answer of ApproximatePercentile is wrong Suppose we have a table with records and partitions values of column col in these partitions are If we query percentile approx col array the current answer is which is far from the correct answer The test case is as below Wrong ApproximatePercentile answer when multiple records have the minimum value
No,I find insert overwrite statement running in spark sql or spark shell spends much more time than it does in hive client i start it in apache hive bin bin hive where spark costs about ten minutes but hive client just costs less than seconds These are the steps I took Test sql is insert overwrite table login game partition pt mix en dt select distinct account name role id server as recdate mix as platform mix as pid mix as dev from tbllog login where pt mix en and dt there are lines of data in tbllog login with partition pt mix en dt ps I m sure it must be insert overwrite costing a lot of time in spark may be when doing overwrite it need to spend a lot of time in io or in something else I also compare the executing time between insert overwrite statement and insert into statement insert overwrite statement and insert into statement in spark insert overwrite statement costs about minutes insert into statement costs about seconds insert into statement in spark and insert into statement in hive client spark costs about seconds hive client costs about seconds the difference is little that we can ignore Insert overwrite statement runs much slower in spark sql than it does in hive client
No,Recently for the changes to SPARK Handle jar conflict issue when uploading to distributed cache If by default yarn client will upload all the files and archives in assembly to HDFS staging folder It should throw if file appears in both files and archives exception to know whether uncompress or leave the file compressed Spark distributed cache should throw exception if same file is specified to dropped in files archives
No,The behavior of variables in the SQL shell has changed from to Specifically hivevar name value and SET hivevar name value no longer work Queries that worked correctly in will either fail or produce unexpected results in so I think this is a regression that should be addressed Hive and Spark work like this Command line args hiveconf and hivevar can be used to set session properties hiveconf properties are added to the Hadoop Configuration SET adds a Hive Configuration property SET hivevar adds a Hive var Hive vars can be substituted into queries by name and Configuration properties can be substituted using hiveconf name In hiveconf sparkconf and conf variable prefixes are all removed then the value in SQLConf for the rest of the key is returned SET adds properties to the session config and according to a comment https github com apache spark blob master sql core src main scala org apache spark sql RuntimeConfig scala L the Hadoop configuration during I O Regression Hive variables no longer work in Spark
No,Many parts of the behavior would be unless default locale was changed because it will currently default to en This affects SQL date time functions At the moment the only SQL function that lets the user specify language country is sentences which is consistent with Hive It affects dates passed in the JSON API It affects some strings rendered in the UI potentially Although this isn t a correctness issue there may be an argument for not letting that vary It affects a bunch of instances where dates are formatted into strings for things like IDs or file names which is far less likely to cause a problem but worth making consistent The other occurrences are in tests The downside to this change is also its upside the behavior doesn t depend on default JVM locale but also can t be affected by the default JVM locale For example if you wanted to parse some dates in a way that depended on an non US locale not just the format string then it would no longer be possible There s no means of specifying this for example in SQL functions for parsing dates However controlling this by globally changing the locale isn t exactly great either The purpose of this change is to make the current default behavior deterministic and fixed PR coming CC hyukjin kwon Fix default Locale used in DateFormat NumberFormat to Locale US
No,The following Spark shell snippet reproduces this issue The reason is that we treat two StructType incompatible even if their only differ from each other in field nullability AnalysisException may be thrown when union two DFs whose struct fields have different nullability
No,for example the following it throws exception Exception in thread dag scheduler event loop java lang StackOverflowError at java lang Exception Exception java at org apache spark serializer SerializationDebugger SerializationDebugger visitSerializableWithWriteObjectMethod SerializationDebugger scala at org apache spark serializer SerializationDebugger SerializationDebugger visitSerializable SerializationDebugger scala at org apache spark serializer SerializationDebugger SerializationDebugger visit SerializationDebugger scala at org apache spark serializer SerializationDebugger SerializationDebugger visitSerializableWithWriteObjectMethod SerializationDebugger scala at org apache spark serializer SerializationDebugger SerializationDebugger visitSerializable SerializationDebugger scala Custom PartitionCoalescer cause serialization exception
No,in spark I enable hive support and when init the sqlContext throw a AlreadyExistsException message Database default already exists same as https www mail archive com dev spark apache org msg html my spark enable hive throw AlreadyExistsException message Database default already exists
Yes,We should upgrade to the latest release of MiMa in order to include my fix for a bug which led to flakiness in the MiMa checks https github com typesafehub migration manager issues Upgrade to MiMa
Yes,There are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge Currently the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing refer the method mergeApplicationListing fileStatus FileStatus The process of replay involves each line in the event log being read as a string parsing the string to a Json structure converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive Tests show that majority of time spent in replay is in doing this work When the replay is performed for building the application listing the only two events that the code really cares for are SparkListenerApplicationStart and SparkListenerApplicationEnd since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener This means that when processing an event log file with a huge number hundreds of thousands can be more of events the work done to deserialize all of these event and then replay them is not needed Only two events are what we re interested in and this can be used to ensure that when replay is performed for the purpose of building the application list we only make the effort to replay these two events and not others My tests show that this drastically improves application list load time For a MB event log from a user with over events the load time local on my mac comes down from about secs to under second using this approach For customers that typically execute applications with large event logs and thus have multiple large event logs present this can speed up how soon the history server UI lists the apps considerably I will be updating a pull request with take at fixing this Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page
No,After deploy spark thrift server on YARN then I tried to execute from the beeline following command show databases I ve got this error message quote beeline connect jdbc hive localhost a a Connecting to jdbc hive localhost INFO Utils Supplied authorities localhost INFO Utils Resolved authority localhost INFO HiveConnection Will try to open client transport with JDBC Uri jdbc hive localhost Connected to Spark SQL version Driver Hive JDBC version spark Transaction isolation TRANSACTION REPEATABLE READ jdbc hive localhost show databases java lang IllegalStateException Can t overwrite cause with java lang ClassCastException org apache spark sql catalyst expressions GenericInternalRow cannot be cast to org apache spark sql catalyst expressions UnsafeRow at java lang Throwable initCause Throwable java at org apache hive service cli HiveSQLException toStackTrace HiveSQLException java at org apache hive service cli HiveSQLException toStackTrace HiveSQLException java at org apache hive service cli HiveSQLException toCause HiveSQLException java at org apache hive service cli HiveSQLException HiveSQLException java at org apache hive jdbc Utils verifySuccess Utils java at org apache hive jdbc Utils verifySuccessWithInfo Utils java at org apache hive jdbc HiveQueryResultSet next HiveQueryResultSet java at org apache hive beeline BufferedRows BufferedRows java at org apache hive beeline BeeLine print BeeLine java at org apache hive beeline Commands execute Commands java at org apache hive beeline Commands sql Commands java at org apache hive beeline BeeLine dispatch BeeLine java at org apache hive beeline BeeLine execute BeeLine java at org apache hive beeline BeeLine begin BeeLine java at org apache hive beeline BeeLine mainWithInputRedirection BeeLine java at org apache hive beeline BeeLine main BeeLine java Caused by org apache spark SparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID edw java lang ClassCastException org apache spark sql catalyst expressions GenericInternalRow cannot be cast to org apache spark sql catalyst expressions UnsafeRow at org apache spark sql execution SparkPlan anonfun apply SparkPlan scala at org apache spark sql execution SparkPlan anonfun apply SparkPlan scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark scheduler ResultTask runTask ResultTask scala at org apache spark scheduler Task run Task scala at org apache spark executor Executor TaskRunner run Executor scala at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java Driver stacktrace at sun reflect NativeConstructorAccessorImpl newInstance Native Method at sun reflect NativeConstructorAccessorImpl newInstance NativeConstructorAccessorImpl java at sun reflect DelegatingConstructorAccessorImpl newInstance DelegatingConstructorAccessorImpl java at java lang reflect Constructor newInstance Constructor java at org apache hive service cli HiveSQLException newInstance HiveSQLException java at org apache hive service cli HiveSQLException toStackTrace HiveSQLException java more Error Error retrieving next row state code quote add jar command also same error occurred quote add jar udf jar java lang IllegalStateException Can t overwrite cause with java lang ClassCastException org apache spark sql catalyst expressions GenericInternalRow cannot be cast to org apache spark sql catalyst expressions UnsafeRow at java lang Throwable initCause Throwable java quote Spark SQL Thrift Error
No,RDD zipWithIndex generate wrong result when one partition contains more than Int MaxValue records when RDD contains a partition with more than records error occurs for example if partition has more than records the index became when we do some operation such as repartition or coalesce it is possible to generate big partition so this bug should be fixed RDD zipWithIndex generate wrong result when one partition contains more than records
No,It looks like https github com apache spark pull broke parquet log output redirection After that patch when querying parquet files written by Parquet mr Spark prints a torrent of harmless warning messages from the Parquet reader This only happens during execution not planning and it doesn t matter what log level the SparkContext is set to This is a regression I noted as something we needed to fix as a follow up to PR I feel responsible so I m going to expedite a fix for it I suspect that PR broke Spark s Parquet log output redirection That s the premise I m going by Spark prints an avalanche of warning messages from Parquet when reading parquet files written by older versions of Parquet mr
No,The following Spark shell snippet creates a series of query plans that grow exponentially The i th plan is created using cached copies of the i th plan and significantly affects usability This issue can be fixed by introducing a checkpoint method for Dataset that truncates both the query plan and the lineage of the underlying RDD Query planning slows down dramatically for large query plans even when sub trees are cached
No,I reported a similar bug two months ago and it s fixed in Spark https issues apache org jira browse SPARK But I find a new bug when I insert a na fill call between outer join and inner join in the same workflow in SPARK I get wrong result Calling outer join and na fill and then inner join will miss rows
Yes,The new Tungsten execution engine has very robust memory management and speed for simple data types It does however suffer from the following For user defined aggregates Hive UDAFs Dataset typed operators it is fairly expensive to fit into the Tungsten internal format For aggregate functions that require complex intermediate data structures Unsafe on raw bytes is not a good programming abstraction due to the lack of structs The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases This operator however should limit its memory usage to avoid putting too much pressure on GC e g falling back to sort based aggregate as soon the number of objects exceeds a very low threshold Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark Introduce a JVM object based aggregate operator
No,Greetings I m currently in the process of migrating a project I m working on from Spark to The project uses Spark Streaming to convert Thrift structs coming from Kafka into Parquet files stored in S This conversion process works fine in but I think there may be a bug in I ll paste the stack trace below org codehaus janino JaninoRuntimeException Code of method Lorg apache spark sql catalyst expressions GeneratedClass Ljava lang Object V of class org apache spark sql catalyst expressions GeneratedClass SpecificUnsafeProjection grows beyond KB at org codehaus janino CodeContext makeSpace CodeContext java at org codehaus janino CodeContext write CodeContext java at org codehaus janino UnitCompiler writeShort UnitCompiler java at org codehaus janino UnitCompiler writeLdc UnitCompiler java Also later on ERROR o a s u SparkUncaughtExceptionHandler Uncaught exception in thread Thread Executor task launch worker run main group java lang OutOfMemoryError Java heap space I ve seen similar issues posted but those were always on the query side I have a hunch that this is happening at write time as the error occurs after batchDuration Here s the write snippet stream flatMap case Success row thriftParseSuccess Some row case Failure ex thriftParseErrors logger error Error during deserialization ex None foreachRDD rdd val sqlContext SQLContext getOrCreate rdd context transformer sqlContext createDataFrame rdd converter schema coalesce coalesceSize write mode Append partitionBy partitioning parquet parquetPath Please let me know if you can be of assistance and if there s anything I can do to help Best Justin CodeGenerator failed to compile org codehaus janino JaninoRuntimeException Code of method Error
No,SQL show table extended like table name doesn t work in spark that works in spark Error org apache spark sql catalyst parser ParseException missing FUNCTIONS at extended line pos SQL show table extended like test state code Failed to run SQL show table extended like table name in Spark
Yes,In the existing code there are three layers of serialization involved in sending a task from the scheduler to an executor A Task object is serialized The Task object is copied to a byte buffer that also contains serialized information about any additional JARs files and Properties needed for the task to execute This byte buffer is stored as the member variable serializedTask in the TaskDescription class The TaskDescription is serialized in addition to the serialized task JARs the TaskDescription class contains the task ID and other metadata and sent in a LaunchTask message While it is necessary to have two layers of serialization so that the JAR file and Property info can be deserialized prior to deserializing the Task object the third layer of deserialization is unnecessary this is as a result of SPARK We should eliminate a layer of serialization by moving the JARs files and Properties into the TaskDescription class taskScheduler has some unneeded serialization
No,Reproducer Expected behavior in this case is probably to choose one side and cast the other compare string to string or long to long instead of using a data type with less precision Filter join expressions can return incorrect results when comparing strings to longs
Yes,Code generation to get data from ColumnVector and ColumnarBatch is becoming pervasive The generation part as a trait for ease of reuse Refactor code generation to get data for ColumnVector ColumnarBatch
No,When a logical plan is built containing the following somewhat nonsensical filter Filter NOT isnotnull f During optimization the filter is converted into a condition that will always fail Filter isnotnull f NOT isnotnull f This appears to be caused by the following check for NullIntolerant https github com apache spark commit df beb de bb d b a a b dbc bf diff ac cebe a c d c f R Which recurses through the expression and extracts nested IsNotNull calls converting them to IsNotNull calls on the attribute at the root level https github com apache spark commit df beb de bb d b a a b dbc bf diff ac cebe a c d c f R This results in the nonsensical condition above not isnotnull is converted to the always false condition isnotnull not isnotnull
No,Hi all I hope that this is not a known issue I haven t had any luck finding anything similar in Jira or the mailing lists but I could be searching with the wrong terms I just started to experiment with Spark SQL and am seeing what appears to be a bug When using Spark SQL to join two tables with a three column inner join the first column join is ignored The example and data Please let me know if I can provide any other details Best regards Eli SQL based three column join loses first column
No,The method to count ChiSqureTestResult in mllib feature ChiSqSelector scala line is wrong For feature selection method ChiSquareSelector it is based on the ChiSquareTestResult statistic ChiSqure value to select the features It select the features with the largest ChiSqure value But the Degree of Freedom df of ChiSqure value is different in Statistics chiSqTest RDD and for different df you cannot base on ChiSqure value to select features Because of the wrong method to count ChiSquare value the feature selection results are strange Take the test suite in ml feature ChiSqSelectorSuite scala as an example If use selectKBest to select the feature will be selected If use selectFpr to select feature and will be selected This is strange I use scikit learn to test the same data with the same parameters When use selectKBest to select feature will be selected When use selectFpr to select feature and will be selected This result is make sense because the df of each feature in scikit learn is the same I plan to submit a PR for this problem ML MLLIB ChiSquareSelector based on Statistics chiSqTest RDD is wrong
Yes,We generate bitmasks for grouping sets during the parsing process and use these during analysis These bitmasks are difficult to work with in practice and have lead to numerous bugs I suggest that we remove these and use actual sets instead however we would need to generate these offsets for the grouping id Do not use bitmasks during parsing and analysis of CUBE ROLLUP GROUPING SETS
No, SELECT distinct does not work if there is a order by clause
No,spark submit support jar url with http protocol If the url contains any query strings worker DriverRunner downloadUserJar method will throw Did not see expected jar exception This is because this method checks the existance of a downloaded jar whose name contains query strings This is a problem when your jar is located on some web service which requires some additional information to retrieve the file For example to download a jar from s bucket via http the url contains signature datetime etc as query string Worker will look for a jar named spark job jar X Amz Algorithm AWS HMAC SHA X Amz Credential us east s aws request X Amz Date T Z X Amz Expires X Amz SignedHeaders host X Amz Signature instead of spark job jar Hence all the query string should be removed before checking jar existance I created a pr to fix this if anyone can review it Spark worker throw Exception when uber jar s http url contains query string
No,During migration from Spark to I observed OffsetOutOfRangeException reported by Kafka client In our scenario we create single DStream as a union of multiple DStreams One DStream for one Kafka cluster multi dc solution Both Kafka clusters have the same topics and number of partitions After quick investigation I found that class DirectKafkaInputDStream keeps offset state for topic and partitions but it is not aware of different Kafka clusters For every topic single DStream is created as a union from all configured Kafka clusters At the end offsets from one Kafka cluster overwrite offsets from second one Fortunately OffsetOutOfRangeException was thrown because offsets in both Kafka clusters are significantly different Kafka OffsetOutOfRangeException on DStreams union from separate Kafka clusters with identical topic names
No,The Then HadoopRDD doesn t fail the job when files are corrupted e g corrupted gzip files Note NewHadoopRDD doesn t have this issue This is reported by Bilal Aslam HadoopRDD should not swallow EOFException
No,spark sql grouping sets throws NullPointerException This problem can be recreated using the following lines of code case class point a String b String c String val data Seq point point point sc parallelize data toDF registerTempTable table spark sql select a b count c from table group by a b GROUPING SETS show grouping set throws NPE
Yes,Classifier getNumClasses can not support Non Double types and classification algos relying on it do not support non double labelCol like NavieBayes As suggested by sethah it is not a reasonable way to do datatype cast everywhere And we can make cast only happen in Predictor yanboliang josephkb srowen Move LabelCol datatype cast into Predictor fit
Yes,ANSI SQL uses the following to specify the frame boundaries for window functions Improve window function frame boundary API in DataFrame
Yes,When I was creating the example code for SPARK I realized it was pretty convoluted to define the frame boundaries for window functions when there is no partition column or ordering column The reason is that we don t provide a way to create a WindowSpec directly with the frame boundaries We can trivially improve this by adding rowsBetween and rangeBetween to Window object DataFrame API should simplify defining frame boundaries without partitioning ordering
No,Current implementation is just iterating not polling and removing Kafka commitQueue needs to be drained
No,JVMObjectTracker objMap is used to track JVM objects for SparkR However we observed that JVM objects that are not used anymore are still trapped in this map which prevents those object get GCed Seems it makes sense to use weak reference like persistentRdds in SparkContext JVMObjectTracker objMap may leak JVM objects
No,Filing this based on a email thread with Reynold Xin From the docs http spark apache org docs latest sql programming guide html running the thrift jdbcodbc server the JDBC connection URL to the thriftserver looks like Specified database in JDBC URL is ignored when connecting to thriftserver
No,Calling repartition on a PySpark RDD to increase the number of partitions results in highly skewed partition sizes with most having rows The repartition method should evenly spread out the rows across the partitions and this behavior is correctly seen on the Scala side Please reference the following The issue here is that highly skewed partitions can result in severe memory pressure in subsequent steps of a processing pipeline resulting in OOM errors PySpark RDD Repartitioning Results in Highly Skewed Partition Sizes
No,This is the stack trace See ConcurrentModificationException Json serialzation of accumulators are failing with ConcurrentModificationException
No,To reproduce SparkR cannot parallelize data frame with NA or NULL in Date columns
No,I have a Dataframe with a struct and I need to rename some fields to lower case before saving it to cassandra It turns out that it s not possible to cast a boolean field of a struct to another boolean field in the renamed struct quote case class ClassWithBoolean flag Boolean case class Parent cwb ClassWithBoolean val structCwb DataType StructType Seq StructField flag BooleanType true Seq Parent ClassWithBoolean true toDF withColumn cwb cwb cast structCwb collect scala MatchError BooleanType of class org apache spark sql types BooleanType quote A workaround is to temporarily cast the field to an Integer and back quote val structCwbTmp DataType StructType Seq StructField flag IntegerType true Seq Parent ClassWithBoolean true toDF withColumn cwb cwb cast structCwbTmp withColumn cwb cwb cast structCwb collect quote scala MatchError BooleanType when casting a struct
No,Attempting to create a DataFrame using a BinaryType field fails under Python because the underlying Pyrolite library is out of date Spark appears to be using Pyrolite this issue was fixed in Pyrolite See original bug report https github com irmen Pyrolite issues and patch https github com irmen Pyrolite commit eec d b d c eaeb e ae e Test case output attached I m just a Python guy not really sure how to build Spark do classpath magic to test if this works correctly with updated Pyrolite BinaryType fails in Python due to outdated Pyrolite
No,without saving to parquet it works fine If you change type of c column to IntegerType it also works fine Incorrect result when work with data from parquet
Yes, Remove redundant Experimental annotations in sql streaming package
No,The issue in subject happens on attempt to transform DataFrame in Parquet format into ORC while DF contains SparseVector DenseVector data In sources https github com apache spark blob v mllib src main scala org apache spark mllib linalg Vectors scala L it looks like that there shouldn t be any serialization issues but they happens org apache spark mllib linalg VectorUDT cannot be cast to org apache spark sql types StructType
Yes,The current InternalRow hierarchy makes a difference between immutable and mutable rows In practice we cannot guarantee that an immutable internal row is immutable you can always pass a mutable object as an one of its elements Lets make all internal rows mutable and reduce the complexity Simplify InternalRow hierarchy
No,I somehow saw a failed test org apache spark DistributedSuite caching in memory serialized replicated Its log shows that Spark master asked the worker to launch an executor before the worker actually got the response of registration So the master knew that the worker had been registered But the worker did not know if it self had been registered Then seems the worker did not launch any executor Master may ask a worker to launch an executor before the worker actually got the response of registration
No,Run the following r SPARK HOME sessionInfo R version Patched r Platform x apple darwin bit Running under macOS Sierra Spark returns incorrect result when collect ing a cached Dataset with many columns
Yes,Dataset always does eager analysis now Thus spark sql eagerAnalysis is not used any more Thus we need to remove it Remove spark sql eagerAnalysis
No,Given table create table dates ts timestamp the following view creation SQL failes with Failed to analyze the canonicalized SQL It is possible there is a bug in Spark create view test dates as select ts interval day from dates Cannot create view which includes interval arithmetic
No,The following complicated example becomes stuck in the InferFiltersFromConstraints rule and never runs However it doesn t fail with a stack overflow and doesn t hit the limit on optimization passes so I think there s some sort of non obvious infinite loop within the rule itself I attached YourKit to my Spark process and recorded some stack traces See the attached screenshots showing the distribution of time for the hung query InferFiltersFromConstraints rule never terminates for query
No,There is a bug in how a transposed SparseMatrix isTransposed true does multiplication with a SparseVector The bug is present for v in both org apache spark mllib linalg BLAS mllib and org apache spark ml linalg BLAS mllib local in the private gemv method with signature bq gemv alpha Double A SparseMatrix x SparseVector beta Double y DenseVector This bug can be verified by running the following snippet in a Spark shell here using v Erroneous computation in multiplication of transposed SparseMatrix with SparseVector
No,The master Maven build is currently broken because ReplSuite consistently fails with ClassCircularityErrors See https spark tests appspot com jobs spark master test maven hadoop for a timeline of the failure Here s the first build where this failed https amplab cs berkeley edu jenkins job spark master test maven hadoop This appears to correspond to https github com apache spark commit a c d b eb e ed b dd cd d bba d The same tests pass in the SBT build ReplSuite fails with ClassCircularityError in master Maven builds
No,Here are re production steps create any scala spark application which will work long enough to open the application details in Web UI run spark submit command for standalone cluster like master spark localhost open running application details in Web UI like localhost spark submit will never finish you will have to kill the process Cause The application creates a thread with infinite loop for web UI communication and never stops it The application is waiting for the thread to be finished instead even if you close the web page Web UI prevents spark submit application to be finished
No,Code generation including too many mutable states exceeds JVM size limit to extract values from references into fields in the constructor We should split the generated extractions in the constructor into smaller functions Code generation including too many mutable states exceeds JVM size limit
No, Fixed Insert Failure To Data Source Tables when the Schema has the Comment Field
No,Spark Scala repl doesn t honor spark replClassServer port configuration so user cannot set a fixed port number through spark replClassServer port There s no issue in Spark since this class is removed Spark Scala repl doesn t honor spark replClassServer port
No,https datastax oss atlassian net browse SPARKC Was brought to my attention where the following Which leads me to believe there is something wrong with the reused exchange Reused Exchange Aggregations Produce Incorrect Results
No,When there are K application history in the history server back end it can take a very long time to even get a single application history page After some investigation I found the root cause was the following piece of transforms the map to an iterator and then uses the find api which is O n instead of O from a map get operation Spark history server web Ui takes too long for a single application
No,I m experiensing problems with s a and working with parquet with dataset api the symptom of problem tasks failing with so when program runs you can jps for pid and do lsof p grep https and you ll see constant grow of CLOSE WAITs Our way to bypass problem is to use count In addition we ve seen that df dropDuplicates a b rdd isEmpty doesn t produce problem too take or isEmpty on dataset leaks s a connections
Yes,There are listLeafFiles related functions in Spark ListingFileCatalog listLeafFiles which calls HadoopFsRelation listLeafFilesInParallel if the number of paths passed in is greater than a threshold if it is lower then it has its own serial version implemented HadoopFsRelation listLeafFiles called only by HadoopFsRelation listLeafFilesInParallel HadoopFsRelation listLeafFilesInParallel called only by ListingFileCatalog listLeafFiles It is actually very confusing and error prone because there are effectively two distinct implementations for the serial version of listing leaf files This into ListingFileCatalog since it is the only class that needs this Keep only one function for listing files in serial Consolidate various listLeafFiles implementations
Yes,Query Only one distinct should be necessary This makes a bunch of unions slower than a bunch of union alls followed by a distinct Optimizer should remove unnecessary distincts in multiple unions
No,Using a malformed URL in sc addJar or sc addFile bricks the executors forever The executors try to update their dependencies but because the URL is malformed they always throw a malformedURL exception Then your cluster is unusable until you restart it Adding a malformed URL to sc addJar and or sc addFile bricks Executors
No,there is a race condition when FetchFailed and resubmit failed stage job job run in different threads if job failed times due to fetchfailed and aborted then job can not post ResubmitFailedStages becase the failedStages in DAGScheduler is not empty now The failed stage never resubmitted due to abort stage in another thread
No,We were getting incorrect results from the DataFrame except method all rows were being returned instead of the ones that intersected Calling subtract on the underlying RDD returned the correct result We tracked it down to the use of coalesce the following is the simplest example case we created that reproduces the issue We should get the same result from both uses of except but the one using coalesce returns instead of Dataframe except returns incorrect results when combined with coalesce
No,h Problem Remainder expression returns incorrect result when using expression eval to calculate the result expression eval is called in case like constant folding Remainder expression eval returns incorrect result
No,The FileStreamSource used by StructuredStreaming first resolves globs and then creates a ListingFileCatalog which listFiles with the resolved glob patterns If a folder is deleted after glob resolution but before the ListingFileCatalog can list the files we can run into a FileNotFoundException This should not be a fatal exception for a streaming job However we should include a warn message Folder deletion after globbing may fail StructuredStreaming jobs
No,AssertOnQuery has two apply constructor one that accepts a closure that returns boolean and another that accepts a closure that returns Unit This is actually very confusing because developers could mistakenly think that AssertOnQuery always require a boolean return type and verifies the return result when indeed the value of the last statement is ignored in one of the constructors AssertOnQuery condition should be consistent in requiring Boolean return type
No,When parsing a CSV with a date time column that contains a variant ISO that doesn t include a colon in the offset casting to Timestamp fails Here s a simple example CSV content quote time T T T quote Here s the stack trace that results from processing this data quote ERROR Utils Aborting task java lang IllegalArgumentException T at org apache xerces jaxp datatype XMLGregorianCalendarImpl Parser skip Unknown Source at org apache xerces jaxp datatype XMLGregorianCalendarImpl Parser parse Unknown Source at org apache xerces jaxp datatype XMLGregorianCalendarImpl Unknown Source at org apache xerces jaxp datatype DatatypeFactoryImpl newXMLGregorianCalendar Unknown Source at javax xml bind DatatypeConverterImpl parseDateTime DatatypeConverterImpl java at javax xml bind DatatypeConverterImpl parseDateTime DatatypeConverterImpl java at javax xml bind DatatypeConverter parseDateTime DatatypeConverter java at org apache spark sql catalyst util DateTimeUtils stringToTime DateTimeUtils scala at org apache spark sql execution datasources csv CSVTypeCast castTo CSVInferSchema scala quote Somewhat related I believe Python standard libraries can produce this form of zone offset The system I got the data from is written in Python https docs python org library datetime html strftime strptime behavior Spark SQL Catalyst doesn t handle ISO date without colon in offset
No, fix some DDL bugs about table management when same name temp view exists
No,In PySpark df take ends up running a single stage job which computes only one partition of df while df limit collect ends up computing all partitions of df and runs a two stage job This difference in performance is confusing so I think that we should generalize the fix from SPARK so that Dataset collect can be implemented efficiently in Python df take and df limit collect perform differently in Python
No,When I run a python application and specify a remote path for the extra files to be included in the PYTHON PATH using the py files or spark submit pyFiles configuration option in YARN Cluster mode I get the following error Exception in thread main java lang IllegalArgumentException Launching Python applications through spark submit is currently only supported for local files s xxxx app py at org apache spark deploy PythonRunner formatPath PythonRunner scala at org apache spark deploy PythonRunner anonfun formatPaths apply PythonRunner scala at org apache spark deploy PythonRunner anonfun formatPaths apply PythonRunner scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection IndexedSeqOptimized class foreach IndexedSeqOptimized scala at scala collection mutable ArrayOps ofRef foreach ArrayOps scala at scala collection TraversableLike class map TraversableLike scala at scala collection mutable ArrayOps ofRef map ArrayOps scala at org apache spark deploy PythonRunner formatPaths PythonRunner scala at org apache spark deploy SparkSubmit anonfun prepareSubmitEnvironment apply SparkSubmit scala at org apache spark deploy SparkSubmit anonfun prepareSubmitEnvironment apply SparkSubmit scala at scala Option foreach Option scala at org apache spark deploy SparkSubmit prepareSubmitEnvironment SparkSubmit scala at org apache spark deploy SparkSubmit submit SparkSubmit scala at org apache spark deploy SparkSubmit main SparkSubmit scala at org apache spark deploy SparkSubmit main SparkSubmit scala Here are sample commands which would throw this error in Spark sparkApp py requires app py spark submit deploy mode cluster py files s xxxx app py s xxxx sparkApp py works fine in spark submit deploy mode cluster conf spark submit pyFiles s xxxx app py s xxxx sparkApp py not working in This would work fine if app py is downloaded locally and specified This was working correctly using py files option in earlier version of Spark but not using the spark submit pyFiles configuration option But now it does not work through either of the ways The following diff shows the comment which states that it should work with non local paths for the YARN cluster mode and we are specifically doing separate validation to fail if YARN client mode is used with remote paths https github com apache spark blob master core src main scala org apache spark deploy SparkSubmit scala L And then this was newly added in which now stores it and hence this validation gets triggered even if we specify files through py files option https github com apache spark blob master core src main scala org apache spark deploy SparkSubmit scala L Also we changed the logic in YARN client to read values directly from spark submit pyFiles configuration instead of from py files earlier https github com apache spark commit ba b f fee c e ea bd f a a e diff b df f b d e b R So now its broken whether we use py files or spark submit pyFiles as the validation gets triggered in both cases irrespective of whether we use Client or Cluster mode with YARN Specifying remote files for Python based Spark jobs in Yarn cluster mode not working
No,While trying to reach launch multiple containers in pool if running executors count reaches or goes beyond the target running executors the container is released and marked failed This can cause many jobs to be marked failed causing overall job failure I will have a patch up soon after completing testing panel title Typical Exception found in Driver marking the container to Failed panel Dynamic allocation race condition Containers getting marked failed while releasing
Yes,We use multiple DStreams coming from different Kafka topics in a Streaming application Some settings like maxrate and backpressure enabled disabled would be better passed as config to KafkaUtils createStream and KafkaUtils createDirectStream instead of setting them in SparkConf Being able to set a different maxrate for different streams is an important requirement for us we currently work around the problem by using one receiver based stream and one direct stream We would like to be able to turn on backpressure for only one of the streams as well Set Streaming MaxRate Independently For Multiple Streams
Yes,Profiling a job we saw that patten matching in wrap function of HiveInspector is consuming around of the time which can be avoided A similar change in the unwrap function was made in SPARK When wrapping catalyst datatype to Hive data type avoid pattern matching
No,h Problem description The following query triggers out of memory error will not collect the GB unrolled until all GB input data has been processed Memory leak in Memory store when unable to cache the whole RDD in memory
No,When the permanent tables views do not exist but the temporary view exists the expected error should be NoSuchTableException for partition related ALTER TABLE commands However it always reports a confusing error message For example Multiple Bugs in DDL Statements on Temporary Views
No,several default params in sparkR spark mlp is wrong layers should be null tol should be e stepSize should be seed should be make the default params in sparkR spark mlp consistent with MultilayerPerceptronClassifier
No,If you create tables as follows create table a as select A as str cast as decimal as num create table b as select A as str Then select floor num from a returns but select floor num from a join b on a str b str returns Floor ceil of decimal returns wrong result if it s in compact format
No,For data sources without extending SchemaRelationProvider we expect users to not specify schemas when they creating tables If the schema is input from users an exception is issued Since Spark for any data source to avoid infer the schema every time we store the schema in the metastore catalog Thus when reading a cataloged data source table the schema could be read from metastore catalog In this case we also got an exception For example Reading Cataloged Data Sources without Extending SchemaRelationProvider
No,MemoryStore putIteratorAsBytes may silently lose values when used with KryoSerializer because it does not properly close the serialization stream before attempting to deserialize the already serialized values which may cause values buffered in Kryo s internal buffers to not be read This is the root cause behind a user reported wrong answer bug in PySpark caching reported by Ben Leslie on the Spark user mailing list in a thread titled pyspark persist MEMORY ONLY vs MEMORY AND DISK MemoryStore putIteratorAsBytes may silently lose values when KryoSerializer is used
Yes,In logical plan SerializeFromObject for an array always use GenericArrayData as a destination UnsafeArrayData could be used for an primitive array This is a simple approach to solve issues that are addressed by SPARK Here is a motivating example Optimize SerializeFromObject for primitive array
Yes,The TaskMetricsUIData updatedBlockStatuses field is assigned to but never read increasing the memory consumption of the web UI We should remove this field Remove unused TaskMetricsUIData updatedBlockStatuses field
No,Because of this bug Python UDF will not work with ORDER BY and LIMIT Python UDF does not work between Sort and Limit
No,After updating Spark from to I found that it seems to have a memory leak on my Spark streaming application Here is the head of the heap histogram of my application which has been running about hours It shows that scala collection mutable DefaultEntry and java lang Long have unexpected big numbers of instances In fact the numbers started growing at streaming process began and keep growing proportional to total number of tasks After some further investigation I found that the problem is caused by some inappropriate memory management in releaseUnrollMemoryForThisTask and unrollSafely method of class org apache spark storage MemoryStore https github com apache spark blob branch core src main scala org apache spark storage MemoryStore scala In Spark x a releaseUnrollMemoryForThisTask operation will be processed only with the parameter memoryToRelease https github com apache spark blob branch core src main scala org apache spark storage MemoryStore scala L L But in fact if a task successfully unrolled all its blocks in memory by unrollSafely method the memory saved in unrollMemoryMap would be set to zero https github com apache spark blob branch core src main scala org apache spark storage MemoryStore scala L So the result is the memory saved in unrollMemoryMap will be released but the key of that part of memory will never be removed from the hash map The hash table will keep increasing while new tasks keep incoming Although the speed of increase is comparatively slow about dozens of bytes per task it is possible that result into OOM after weeks or months Inappropriate memory management in org apache spark storage MemoryStore may lead to memory leak
No,Check out the following ConcurrentModificationException Even though accumulators aren t thread safe they can be concurrently read while serializing executor heartbeats and modified while tasks are running leading to ConcurrentModificationException errors thereby leading to missing heartbeats or leading to inconsistent data since individual fields of a multi field object might be serialized at different points in time leading to inconsistencies in accumulators like LongAccum This seems like a pretty serious issue but I m not sure what s the best way to fix this An obvious fix would be to properly synchronize all accesses to the fields of our accumulators and to synchronize the writeObject and writeKryo methods but this may have an adverse performance impact Serialization of accumulators in heartbeats is not thread safe
No,Dataset joinWith is performing a BroadcastJoin on a table that is gigabytes in size due to the dataset logicalPlan statistics sizeInBytes i Seq i Seq i This is really not that long of a string toDS You might have to remove private sql from Dataset logicalPlan to get this to work val stats ds logicalPlan statistics yields stats org apache spark sql catalyst plans logical Statistics Statistics false This causes joinWith to performWith to perform a broadcast join even tho my data is gigabytes in size which of course causes the executors to run out of memory Setting spark sql autoBroadcastJoinThreshold does not help because the logicalPlan statistics sizeInBytes is a large negative number and thus it is less than the join threshold of I ve been able to work around this issue by setting autoBroadcastJoinThreshold to a very large negative number Dataset joinWith broadcasts gigabyte sized table causes OOM Exception
No,When using pivot and multiple aggregations we need to alias to avoid special characters but alias does not help because df groupBy C pivot A agg avg D as COLD max B as COLB show C bar avg D AS COLD bar max B AS COLB foo avg D AS COLD foo max B AS COLB small two two large two one Expected Output C bar COLD bar COLB foo COLD foo COLB small two two large two one One approach you can fix this issue is to change the class sql catalyst src main scala org apache spark sql catalyst analysis Analyzer scala and change the outputName method in Alias specified for aggregates in a pivot are not honored
No,The Pool Adjacent Violators Algorithm PAVA implementation that s currently in MLlib can take O N time for certain inputs when it should have worst case complexity of O N To reproduce this I pulled the private method poolAdjacentViolators out of mllib regression IsotonicRegression and into a benchmarking harness Given this input I vary the length of the input to get these timings Input Length Time us tests were performed using https github com sirthias scala benchmarking template I can also confirm that I run into this issue on a real dataset I m working on when trying to calibrate random forest probability output Some partitions take hours to run This isn t a skew issue since the largest partitions finish in minutes I can only assume that some partitions cause something approaching this worst case complexity I m working on a patch that borrows the implementation that is used in scikit learn and the R iso package both of which handle this particular input in linear time and are quadratic in the worst case IsotonicRegression takes non polynomial time for some inputs
No,write df passes everything in its arguments to underlying data source in x but it is not passing header true in Spark For example the following Additional arguments in write df are not passed to data source
No,ALTER TABLE RENAME PARTITION is unable to handle data source tables just like the other ALTER PARTITION commands We should issue an exception instead Issue Exceptions when ALTER TABLE RENAME PARTITION tries to alter a data source table
No,clockfly found the following corner case that returns the wrong quantile off by The value of the LEFT column represents the output when using QuantileSummaries in Window function the value on the RIGHT column represents the expected result The different between LEFT and RIGHT column is that the LEFT column does intermediate compression on the storage of QuantileSummaries QuantilesSummaries returns the wrong result after compression
No,The core info of an application in Master UI doesn t consider ApplicationInfo executorLimit It s pretty confusing that UI says Unlimited when executorLimit is set Master UI should show the correct core limit when ApplicationInfo executorLimit is set
No,select length select length these sql will return errors but hive is ok Error in query cannot resolve length due to data type mismatch argument requires string or binary type however is of int type line pos Error in query cannot resolve length due to data type mismatch argument requires string or binary type however is of double type line pos spark sql length return error
Yes,Many users have requirements to use third party R packages in executors workers but SparkR can not satisfy this requirements elegantly For example you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios Users can install R packages from CRAN or custom CRAN like repository for each executors Users can load their local R packages and install them on each executors To achieve this goal the first thing is to make SparkR executors support virtualenv like Python conda I have investigated and found packrat http rstudio github io packrat is one of the candidates to support virtualenv for R Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space Then SparkR users can install third party packages in the application scope destroy after the application exit and don t need to bother IT administrators to install these packages manually I would like to know whether it make sense SparkR executors workers support virtualenv
No,In SPARK we fix the OOM issue when Metadata is super big There are other cases that may also trigger OOM Current implementation of TreeNode toJSON will recursively search and print all fields of current TreeNode even if the field s type is of type Seq or type Map This is not safe because the Seq or Map can be very big Converting them to JSON make take huge memory which may trigger out of memory error Some user space input may also be propagated to the Plan The user space input can be of arbitrary type and may also be self referencing Trying to print user space input to JSON is very risky The following example triggers a StackOverflowError when calling toJSON on a plan with user defined UDF Current TreeNode toJSON may trigger OOM under some corner cases
Yes,For building SparkR vignettes on Jenkins machines we need the rmarkdown R The package is available at https cran r project org web packages rmarkdown index html I think running something like Rscript e install packages rmarkdown repos http cran stat ucla edu should work Install rmarkdown R package on Jenkins machines
No,Spark currently assumes of partitions to be less than and uses d padding If we exceed this no the sort logic in ReliableCheckpointRDD gets messed up and fails This is because of part files are sorted and compared as strings This leads filename order to be part part instead of part part part and while reconstructing the checkpointed RDD the job fails Possible solutions Bump the padding to allow more partitions or Sort the part files extracting a sub portion as string and then verify the RDD Fix sorting of part files while reconstructing RDD partition from checkpointed files
No,https amplab cs berkeley edu jenkins view Spark QA Test job spark branch test sbt hadoop console Seems showDF in test sparkSQL R is broken BRANCH Broken test showDF in test sparkSQL R
No,In the latest branch we have two test case failure due to backport test ALTER VIEW AS should keep the previous table properties comment create time etc test SPARK The EXPLAIN output of CTAS only shows the analyzed plan Fix Two Test Failures After Backport
No,Demo case We silently replace corrupted line with null without any error message Improves the error message when fails to parse some json file lines in DataFrameReader
No,Using Scala value classes as the inner type for Datasets breaks in Spark and X This simple Spark application demonstrates that the for Spark Scala value classes create encoder problems and break at runtime
Yes,To regulate pending and running executors we determine the executors which are eligible to kill and kill them iteratively rather than a loop This does an RPC call and is synchronized leading to lock contention for SparkListenerBus Side effect listener bus is blocked while we iteratively remove executors Kill multiple executors together to reduce lock contention
No,I can do it with spark Can not query hive table starting with number
No,When using MLLib when calling toJSON on a plan with many level of sub queries it may cause out of memory exception with stack trace like this The query plan stack trace and jmap distribution is attached A large Metadata filed in Alias can cause OOM when calling TreeNode toJSON
Yes,It would be useful if more of JDBCRDD s JDBC Spark SQL functionality was usable from outside of JDBCRDD this would make it easier to write test harnesses comparing Spark output against other JDBC databases Refactor JDBCRDD to expose JDBC SparkSQL conversion functionality
No,SPARK updated the version of vis js to As of some class was renamed like timeline to vis timeline but that ticket didn t care and now style is broken Style of event timeline is broken
No,A number of SparkR tests are current failing when run on Windows as discussed in https github com apache spark pull The list of tests that fail right now is at https gist github com shivaram df bd dc e e d ce c A full log from a build and test on AppVeyor is at https ci appveyor com project HyukjinKwon spark build test Fix SparkR tests on Windows
No,While investigating SPARK I found an incorrect results case from a NOT IN subquery I thought originally it is an edge case Further investigation found this is a more general problem Incomplete algorithm for name resolution in Catalyst paser may lead to incorrect result
Yes,This is another step to get rid of HiveClient from HiveSessionState All the metastore interactions should be through ExternalCatalog interface However the existing implementation of InsertIntoHiveTable still requires Hive clients Thus we can remove HiveClient by moving the metastore interactions into ExternalCatalog Remove Direct Usage of HiveClient in InsertIntoHiveTable
Yes,In publishing SparkR to CRAN it would be nice to have a vignette as a user guide that describes the big picture introduces the use of various methods This is important for new users because they may not even know which method to look up Add package vignette to SparkR
Yes,Kolmogorov Smirnov Test is a popular nonparametric test of equality of distributions There is implementation in MLlib It will be nice if we can expose that in SparkR Add Kolmogorov Smirnov Test to SparkR
No,In spark shell Apparently this example is minimal removing the CROSS or one of the JOIN causes no issue Spark SQL cross join two joins BUG
No, better error message for exceptions during ScalaUDF execution
No,https amplab cs berkeley edu jenkins job spark master test sbt hadoop testReport junit org apache spark util collection unsafe sort PrefixComparatorsSuite String prefix comparator I could not reproduce it locally But let me add this case in the regressionTests to explicitly test it PrefixComparatorsSuite s String prefix comparator failed when both input strings are empty strings
No,After stopping SparkSession if we recreate it and use HiveContext in it it will throw error Steps to reproduce spark SparkSession builder enableHiveSupport getOrCreate spark sql show databases spark stop spark SparkSession builder enableHiveSupport getOrCreate spark sql show databases Java lang illegalStateException Cannot call methods on a stopped sparkContext Above error occurs only in case of Pyspark not in SparkShell Using HiveContext after re creating SparkContext in Spark throws Java lang illegalStateException Cannot call methods on a stopped sparkContext
Yes, move CreateTables to HiveStrategies
No,The following example fails with a ClassCastException It s surprising to me that this error is occurring during query parsing My hunch is that we re performing expression evaluation too early and need to run more analysis and type promotion rules prior to trying to evaluate the expressions here Performing arithmetic in VALUES can lead to ClassCastException MatchErrors during query parsing
No,The following test case produces a ClassCastException in the analyzer This bug was discovered while trying to run SQLite bug reports through Spark SQL see https www sqlite org src tktview name e c ClassCastException OuterReference cannot be cast to NamedExpression for correlated subquery on the RHS of an IN operator
No,This issue has been fixed in Spark Seems ClientWrapper conf is trying to access the ThreadLocal SessionState which has been set NPE thrown by ClientWrapper conf
No,The summary page of Spark history server web UI keep displaying Loading history summary all the time and crashes the browser when there are more than K application history event logs on HDFS I did some investigation historypage js file sends a REST request to api v applications endpoint of history server REST endpoint and gets back json response When there are more than K applications inside the event log directory it takes forever to parse them and render the page When there are only hundreds or thousands of application history it is running fine Spark history server summary page gets stuck at loading history summary with K application history
Yes,Spark has configurable L regularization parameter for generalized linear regression It is very important to have them in SparkR so that users can run ridge regression SparkR spark glm should have configurable regularization parameter
No,I am trying to run a pivot transformation which I ran on a spark cluster namely sc parallelize Seq toDF a b c res org apache spark sql DataFrame a int b int c int scala res groupBy a pivot b agg count c avg c na fill res org apache spark sql DataFrame a int count c bigint avg c double count c bigint avg c double scala res groupBy a pivot b agg count c avg c na fill show a count c avg c count c avg c after upgrade the environment to spark got an error while executing na fill method scala sc parallelize Seq toDF a b c res org apache spark sql DataFrame a int b int more field scala res groupBy a pivot b agg count c avg c na fill org apache spark sql AnalysisException syntax error in attribute name count c at org apache spark sql catalyst analysis UnresolvedAttribute e unresolved scala at org apache spark sql catalyst analysis UnresolvedAttribute parseAttributeName unresolved scala at org apache spark sql catalyst plans logical LogicalPlan resolveQuoted LogicalPlan scala at org apache spark sql Dataset resolve Dataset scala at org apache spark sql Dataset col Dataset scala at org apache spark sql DataFrameNaFunctions org apache spark sql DataFrameNaFunctions fillCol DataFrameNaFunctions scala at org apache spark sql DataFrameNaFunctions anonfun apply DataFrameNaFunctions scala at org apache spark sql DataFrameNaFunctions anonfun apply DataFrameNaFunctions scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection TraversableLike anonfun map apply TraversableLike scala at scala collection IndexedSeqOptimized class foreach IndexedSeqOptimized scala at scala collection mutable ArrayOps ofRef foreach ArrayOps scala at scala collection TraversableLike class map TraversableLike scala at scala collection mutable ArrayOps ofRef map ArrayOps scala at org apache spark sql DataFrameNaFunctions fill DataFrameNaFunctions scala at org apache spark sql DataFrameNaFunctions fill DataFrameNaFunctions scala at org apache spark sql DataFrameNaFunctions fill DataFrameNaFunctions scala DataFrame fill after pivot causing org apache spark sql AnalysisException
No,Hive Index tables are not supported by Spark SQL Thus we issue an exception when users try to access Hive Index tables When the internal function tableExists tries to access Hive Index tables it always gets the same error message Hive index table is not supported This message could be confusing to users since their SQL operations could be completely unrelated to Hive Index tables For example when users try to alter a table to a new name and there exists an index table with the same name the expected exception should be a TableAlreadyExistsException Table Existence Checking when Index Table with the Same Name Exists
No,If you look at event timeline for a stage in chrome you get total time that s bigger than sum of all individual elements Actually proportions are calculated correctly but due to css issue they re not expanded to the size of the full bar Even timeline for a stage doesn t core of the bar timeline bar in chrome
Yes,Method SQLContext parseDataType dataTypeString String could be removed we should use SparkSession parseDataType dataTypeString String instead This require updating PySpark Method SQLContext parseDataType dataTypeString String could be removed
No,Broadcast join produces incorrect columns in join result see below for an example The same join but without using broadcast gives the correct columns Running PySpark on YARN on Amazon EMR Broadcast join produces incorrect results when compressed Oops differs between driver executor
No,Here s the And this is the exception in executor log sparkr zip is not distributed to executors when run sparkr in RStudio
No,The result of compare two vectors using UnitTests org apache spark mllib util TestingUtils is not right sometime For example val a Vectors dense Arrary val b Vectors zeros a b absTol e the result is true Comparing Vector in relative tolerance or absolute tolerance in UnitTests error
Yes,Since HiveClient is used to interact with the Hive metastore it should be hidden in HiveExternalCatalog After moving HiveClient into HiveExternalCatalog HiveSharedState becomes a wrapper of HiveExternalCatalog Thus removal of HiveSharedState becomes straightforward After removal of HiveSharedState the reflection logic is directly applied on the choice of ExternalCatalog types based on the configuration of CATALOG IMPLEMENTATION HiveClient is also used invoked by the other entities besides HiveExternalCatalog we defines the following two APIs Removal of HiveSharedState
Yes, remove catalog table type INDEX
Yes, Refactor R mllib for easier ml implementations
No,Currently Analyze Table is only for Hive serde tables We should issue exceptions in all the other cases When the tables are data source tables we issued an exception However when tables are In Memory Cataloged tables we do not issue any exception Issue Exceptions when Analyze Table on In Memory Cataloged Tables
No,The following end to end test uncovered a bug in GetExternalRowField We need to update GetExternalRowField to escape field names and also need to audit other expressions to make sure that we re not making the same mistake there GetExternalRowField does not properly escape field names causing generated code not to compile
Yes,Inline tables currently do not support SQL generation and as a result a view that depends on inline tables would fail Support SQL generation for inline tables
Yes,CreateHiveTableAsSelectLogicalPlan is a dead code after refactoring Removal of useless CreateHiveTableAsSelectLogicalPlan
No,The following example runs successfully on Spark but fails in the current master as of b bb d f d c b e bd fbb if not earlier Note that this error occurs during query execution not during analysis or physical planning Complex query triggers binding error in HashAggregateExec
Yes, Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler
No,See discussion on the mailing list http apache spark developers list n nabble com Re Aggregations with scala pairs td html The above implementation is not order preserving and does not allow duplicate names RelationalGroupedDataset agg should be order preserving and allow duplicate column names
No,Found below case was broken in Spark but it can run successfully in Spark BTW the case can also run successfully in Hive Please see below reproduces for details Spark SQL CLI usr lib spark bin spark sql spark sql use test db spark sql DROP DATABASE IF EXISTS test db CASCADE INFO execution SparkSqlParser Parsing command DROP DATABASE IF EXISTS test db CASCADE Error in query Can not drop current database test db Hive CLI usr bin hive hive use test db OK hive DROP DATABASE IF EXISTS test db CASCADE OK Time taken seconds Failed to drop database when use the database in Spark
No,Consider the following query In the innermost query the LEFT JOIN s condition is false but nevertheless the number of rows produced should equal the number of rows in table which is non empty Since no values are null the outer where should retain all rows so the overall result of this query should contain a single row with the value Instead the current Spark master as of a e cbd fa da e cd d bf f at least returns no rows Looking at explain it appears that the logical plan is optimizing to LocalRelation so Spark doesn t even run the query My suspicion is that there s a bug in constraint propagation or filter pushdown This issue doesn t seem to affect Spark so I think it s a regression in master Analyzer incorrectly optimizes plan to empty LocalRelation
No,Running SELECT NULL fails with but SELECT NULL returns NULL as expected SELECT NULL throws AnalysisException while SELECT NULL works
No,We have been seeing many job failure due to executor OOM with following stack trace https github com sitalkedia spark blob master core src main java org apache spark util collection unsafe sort UnsafeExternalSorter java L when the UnsafeExternalSorter is checking if memory page is being used by upstream the base object in case of off heap memory is always null so the UnsafeExternalSorter does not spill the memory pages Job failure due to Executor OOM in offheap mode
No,In Pyspark any task that accesses cached data non locally throws a StreamCorruptedException like the stacktrace below The simplest way I have found to reproduce this is by running the following Pyspark with locality ANY throw java io StreamCorruptedException
No,Currently LogicalRelation newInstance simply creates another LogicalRelation object with the same parameters However the newInstance method inherited from MultiInstanceRelation should return a copy of unique expression ids Current LogicalRelation newInstance causes failure when doing self join LogicalRelation newInstance should follow the semantics of MultiInstanceRelation
No,In pyspark when filtering on a udf derived column after some join types the optimized logical plan results is a java lang UnsupportedOperationException I could not replicate this in scala It fails when the join is how outer on column expression how left outer on string or column expression how right outer on string or column expression It passes when the join is how inner on string or column expression how outer on string I made some tests to demonstrate each of these Run with bin spark submit test bug py pyspark filter on a udf column after join gives java lang UnsupportedOperationException
No,Random query generation uncovered the following query which returns incorrect results when run on Spark SQL This wasn t the original query uncovered by the generator since I performed a bit of minimization to try to make it more understandable With the following tables Based on this the output after adding the HAVING should contain four rows not zero I m not sure how to further shrink this in a straightforward way so I m opening this bug to get help in triaging further Incorrect result when HAVING clause is added to group by query
No,Running works fine my hunch is that this is uncovering a bug in the ordering of our optimizer rules or a bug in the constant folding rule itself This particular example is probably unimportant by itself but may be an indicator of other problems SELECT COUNT NULL OVER throws UnsupportedOperationException during analysis
No,The following failing test demonstrates a bug where Spark mis encodes array of struct fields if whole stage codegen is disabled path forgot to copy somewhere Roundtrip encoding of array fields is wrong when whole stage codegen is disabled
No,I discovered this bug when working with a build from the master branch which I believe is This used to work fine when running spark I have a dataframe with an intData column that has values like I don t think that there should be duplicate splits generated should there be QuantileDiscretizer throws InvalidArgumentException parameter splits given invalid value on valid data
Yes,After acquiring allocations from YARN and launching containers Spark currently waits for seconds for executors to connect to the driver On Spark standalone nothing like this happens I m wondering whether we can just remove this sleep entirely Is there a reason I m missing why YARN is different than standalone in this regard At the least we could do something smarter like wait until all executors have registered Remove second sleep before starting app on YARN
No,When encountering an incompatible DataSourceRegister it s better to add instructions to remove or upgrade it Improve the error message when encountering an incompatible DataSourceRegister
Yes,Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher The only option right now is to created a property file Add conf to mesos dispatcher process
No,We have hit a consistent bug where we have a dataset with more than columns I am raising as a blocker because spark is returning the WRONG results rather than erroring leading to data integrity issues I have put together the following test case which will show the issue it will run in spark shell In this example i am joining a dataset with lots of fields onto another dataset The join works fine and if you show the dataset you will get the expected result However if you run a map step over the dataset you end up with a strange error where the sequence that is in the right dataset now only contains the last value Whilst this test may seem a rather contrived example what we are doing here is a very standard analtical pattern My original test case case class Name name String case class SmallCaseClass joinkey Integer names Seq Name case class BigCaseClass field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer field Integer val bigCC Seq BigCaseClass val smallCC Seq SmallCaseClass Seq Name Jamie Name Ian Name Dave Name Will val bigCCDS spark createDataset spark sparkContext parallelize bigCC val smallCCDS spark createDataset spark sparkContext parallelize smallCC val joined test bigCCDS as A joinWith smallCCDS as B A field B joinkey LEFT This next step is fine it shows all names WrappedArray Jamie Ian Dave Will joined test show false This one ends up repeating will I did the most simple map step possible here WrappedArray Will Will Will Will joined test map identity show false This one works because we have less than fields Jamie Ian Dave Will joined test map show false Incorrect results returned following a join of two datasets and a map step where total number of columns
No,Suggest t t t At t someone called YarnSchedulerBackend doRequestTotalExecutors from one of three functions CoarseGrainedSchedulerBackend killExecutors CoarseGrainedSchedulerBackend requestTotalExecutors or CoarseGrainedSchedulerBackend requestExecutors in all of which will hold the lock CoarseGrainedSchedulerBackend Then YarnSchedulerBackend doRequestTotalExecutors will send a RequestExecutors message to yarnSchedulerEndpoint and wait for reply At t someone send a RemoveExecutor to yarnSchedulerEndpoint and the message is received by the endpoint At t the RequestExexutor message sent at t is received by the endpoint Then the endpoint would first handle RemoveExecutor then the RequestExecutor message When handling RemoveExecutor it would send the same message to driverEndpoint and wait for reply In driverEndpoint it will request lock CoarseGrainedSchedulerBackend to handle that message while the lock has been occupied in t So it would cause a deadlock We have found the issue in our deployment it would block the driver to make it handle no messages until the two message all went timeout Potential deadlock in driver handling message
No, group by order by ordinal should throw AnalysisException instead of UnresolvedException
No,As found in https github com apache spark pull files r Spark parses negative numeric literals as the unary minus of positive literals This introduces problems for the edge cases such as being parsed as decimal instead of bigint negative numeric literal parsing
No,The same issue as SPARK But for branch we are missing the profile for scala build release build sh is missing hive thriftserver for scala
No,Using a Dataset containing rows each containing null and an array of Ints I observe the following performance in local mode select column explode column is extremely slow
No,A TreeNodeException is thrown when executing the following minimal example in Spark Crucial is that the column q is generated with lit expr The exception is org apache spark sql catalyst errors package TreeNodeException Binding attribute tree x A possible workaround is to write the dataframe to disk before grouping and mapping TreeNodeException when flat mapping RelationalGroupedDataset created from DataFrame containing a column created with lit expr
No,I found strange behaviour using fullouter join in combination with inner join It seems that inner join can t match values correctly after full outer join Here is a reproducible example in spark Full outer join followed by inner join produces wrong results
No,Spark seems to have some problems reading a parquet dataset generated by I have originally posted it to user mailing list but with the last discoveries this clearly seems like a bug Spark unable to infer schema for parquet data written by Spark
Yes,CC mgummelt tnachen skonto I think this is fairly easy and would be beneficial as more work goes into Mesos It should separate into a module like YARN does just on principle really but because it also means anyone that doesn t need Mesos support can build without it I m entirely willing to take a shot at this Collect Mesos support code into a module profile
No,When submitting an application with name bin spark submit name myApplicationTest verbose executor cores num executors master yarn deploy mode client class org apache spark examples SparkKMeans examples target original spark examples SNAPSHOT jar hdfs localhost lr big txt In the history server UI App ID application App Name c dc b b a a ea e b The App Name should not be a randomUUID c dc b b a a ea e b since the spark app name was myApplicationTest The application org apache spark examples SparkKMeans above did not invoke appName App Name is a randomUUID even when spark app name exists
Yes,The execution package is meant to be internal and as a result it does not make sense to mark things as private sql or private spark It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql and private spark from sql execution package
No,The comment in CatalogTable returned from Hive is always empty We store it in the table property when creating a table However when we try to retrieve the table metadata from Hive metastore we do not rebuild it The comment is always empty Table Comment in the CatalogTable returned from HiveMetastore is Always Empty
Yes,There could be same subquery within a single query we could reuse the result without running it multiple times Reuse subqueries within single query
No,The following queries work Using ordinals in ORDER BY causes an analysis error when the query has a GROUP BY clause using ordinals
No,KafkaUtils createDirectStream does not work in python when you set parameter fromOffsets which is starting offsets of the stream on Kafka This is because the long type is removed from python and py j maps numeric variables to java lang Integer or java lang Long depending on number size which causes ClassCastException for small offsets variables This behaviour was noticed before and tests for this functionality are disabled in python https github com apache spark blob e d d f be c fb c fbcd ae d python pyspark streaming tests py L fromOffsets parameter in Kafka s Direct Streams does not work in python
No,When the source table is a data source table the table generated by CREATE TABLE LIKE is non empty The expected table should be empty CREATE TABLE LIKE generates a non empty table when source is a data source table
Yes,Function related HiveExternalCatalog APIs do not have enough verification logics After the PR HiveExternalCatalog and InMemoryCatalog become consistent in the error handling For example below is the exception we got when calling renameFunction Verification of Function related ExternalCatalog APIs
Yes,Update LogisticCostAggregator serialization code to make it consistent with LinearRegression Update LogisticCostAggregator serialization code to make it consistent with LinearRegression
No,If you have a Spark standalone cluster which runs a single application and you have a Spark task which repeatedly fails by causing the executor JVM to exit with a zero exit is non zero whereas I think that we should always call schedule even on a clean executor shutdown since schedule should always be safe to call Spark tasks which cause JVM to exit with a zero exit code may cause app to hang in Standalone mode
No,A query which used to work in Spark fails with executor OOM in Stack trace Query with Broadcast Hash join fails due to executor OOM in Spark
Yes,Remove TestHiveSharedState Otherwise we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION Removal of TestHiveSharedState
No,For DataSet typed selection Analysis error for DataSet typed selection
No,Greatest least function does not have the most friendly error message for data types Error in SQL statement AnalysisException cannot resolve greatest CAST AS DECIMAL due to data type mismatch The expressions should all have the same type got GREATEST ArrayBuffer DecimalType StringType line pos We should report the human readable data type instead rather than having ArrayBuffer and StringType Improve error message for greatest least
No,I have a wide table columns when I try fitting the traindata on all columns the fatal error occurs more Caused by org codehaus janino JaninoRuntimeException Code of method Lorg apache spark sql catalyst InternalRow Lorg apache spark sql catalyst InternalRow I of class org apache spark sql catalyst expressions GeneratedClass SpecificOrdering grows beyond KB at org codehaus janino CodeContext makeSpace CodeContext java at org codehaus janino CodeContext write CodeContext java org apache spark sql catalyst expressions GeneratedClass SpecificOrdering grows beyond KB
No,Right now the constructors for the TimeWindow expression in Catalyst incorrectly uses the windowDuration in place of the slideDuration This will cause incorrect windowing semantics after time window expressions are analyzed by Catalyst Relevant code is here https github com apache spark blob branch sql catalyst src main scala org apache spark sql catalyst expressions TimeWindow scala L L TimeWindow incorrectly drops slideDuration in constructors
No,The avgMetrics are summed up across all folds instead of being averaged This is an easy fix in CrossValidator fit function PySpark CrossValidator reports incorrect avgMetrics
No,bin sparkR Launching java with spark submit command Users mwang spark ws bin spark submit sparkr shell var folders s b sgvj kl kwq stvft pm gn T RtmpQxJGiZ backend porte ed e Using Spark s default log j profile org apache spark log j defaults properties Setting default log level to WARN To adjust logging level use sc setLogLevel newLevel sc setLogLevel INFO Error could not find function sc setLogLevel sc setLogLevel doesn t exist sparkR sc setLogLevel doesn t work
Yes, remove MaxOf and MinOf
No,Hello I m using c xlarge instances on EC with cores and doing lots of parse url url host in Spark SQL Unfortunately it seems that there is an internal thread safe cache in there and the instances end up being idle When I view the thread dump for my executors most of the executor threads are BLOCKED in that state However when I switch from executor with cores to executors with cores throughput is almost x higher and the CPUs are back at use Thanks java util Hashtable limits the throughput of PARSE URL
No,This happens because the file scan operator does not take into account partition pruning in its implementation of sameResult As a result executions may be incorrect on self joins over the same base file relation Here s a minimal test case to reproduce Exchange reuse incorrectly reuses scans over different sets of partitions
Yes,The catalyst package is meant to be internal and as a result it does not make sense to mark things as private sql or private spark It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql and private spark from catalyst package
Yes,SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added Open up SparkILoop getAddedJars
No,The root of the history server is rendered dynamically with javascript and this doesn t honor APPLICATION WEB PROXY BASE https github com apache spark blob master core src main resources org apache spark ui static historypage template html L Other links in the history server do honor it https github com apache spark blob master core src main scala org apache spark ui UIUtils scala L This means the links on the history server root page are broken when deployed behind a proxy History Server main page does not honor APPLICATION WEB PROXY BASE
No,It is useful to log the timezone when query result does not match especially on build machines that have different timezone from AMPLab Jenkins Log timezone when query result does not match
No,Correlated subqueries with LIMIT could return incorrect results The rule ResolveSubquery in the Analysis phase moves correlated predicates to a join predicates and neglect the semantic of the LIMIT Example The correct result contains both rows from T Correlated subqueries containing non deterministic operators return incorrect results
No,Hello This is a little similar to SPARK https issues apache org jira browse SPARK should I have reopened it I would recommend to give another full review to HashedRelation scala particularly the new LongToUnsafeRowMap joins LongToUnsafeRowMap crashes with ArrayIndexOutOfBoundsException
No,The issue occurs when we run a map over a dataset containing Case Class with a List in it A self contained test case is below case class TestCC key Int letters List String List causes the issue a Seq Array works fine simple test data val ds sc makeRDD Seq List D List S H List F H List D L L map x x length x toDF key letters as TestCC This will fail val test ds map key test show Error Caused by org codehaus commons compiler CompileException File generated java Line Column No applicable constructor method found for actual parameters int scala collection Seq candidates are TestCC int scala collection immutable List It seems to be internally converting the List to a sequence then it cant convert it back If you change the List String to Seq String or Array String the issue doesnt appear Dataset containing a Case Class with a List type causes a CompileException converting sequence to list
No,The behavior of SparkContext addFile changed slightly with the introduction of the Netty RPC based file server which was introduced in Spark where it was disabled by default and became the default only file server in Spark Prior to calling SparkContext addFile twice with the same path would succeed and would cause future tasks to receive an updated copy of the file This behavior was never explicitly documented but Spark has behaved this way since very early x versions some of the relevant lines in Executor updateDependencies have existed since In or with the Netty file server enabled the second addFile call will fail with a requirement error because NettyStreamManager tries to guard against duplicate file registration I believe that this change of behavior was unintentional and propose to remove the require check so that Spark matches x s default behavior This problem also affects addJar in a more subtle way the fileServer addJar call will also fail with an exception but that exception is logged and ignored due to some code which was added in in order to ignore errors caused by missing Spark examples JARs when running on YARN cluster mode AFAIK SparkContext addFile should not fail if called twice with the same file
No,When launching spark on a system with multiple javas installed there are a few options for choosing which JRE to use setting JAVA HOME being the most straightforward However when pyspark s internal py j launches its JavaGateway it always invokes java directly without qualification This means you get whatever java s first on your path which is not necessarily the same one in spark s JAVA HOME This could be seen as a py j issue but from their point of view the fix is easy make sure the java you want is first on your path I can t figure out a way to make that reliably happen through the pyspark executor launch path and it seems like something that would ideally happen automatically If I set JAVA HOME when launching spark I would expect that to be the only java used throughout the stack java launched by PySpark as gateway may not be the same java used in the spark environment
No,How to reproduce In spark sql on Hive This does not happen if I change the name of the CTE I guess Catalyst get caught in an infinite recursion loop because the CTE and the source table have the same name Infinite recursion loop in org apache spark sql catalyst trees TreeNode when table name collides
Yes,This JIRA is to upgrade the derby version from to Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark I now believe it is required based on comments for the pull request and so this is only a dependency upgrade The upgrade is due to an already disclosed vulnerability CVE in derby We used https www versioneye com search and will be checking for any other problems in a variety of libraries too investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this This was raised on the mailing list at http apache spark developers list n nabble com VOTE Release Apache Spark RC tp p html by Stephen Hellberg and replied to by Sean Owen I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version I checked up to the branch so ideally we d backport this for all impacted Spark releases I ve marked this as critical and ticked the important checkbox as it s going to impact every user there isn t a security component should we add one and hence the build tag Upgrade derby to from
No,ML GaussianMixture training failed due to feature column type mistake The feature column type should be ml linalg VectorUDT but got mllib linalg VectorUDT by mistake This bug is easy to reproduce by the following Why the unit tests did not complain this errors Because some estimators transformers missed calling transformSchema dataset schema firstly during fit or transform I will also add this function to all estimators transformers who missed ML GaussianMixture training failed due to feature column type mistake
No, Errors thrown by UDFs cause TreeNodeException when the query has an ORDER BY clause
No,Hello Here is a crash in Spark SQL joins with a minimal reproducible test case Interestingly it only seems to happen when reading Parquet data I added a crash True variable to show it This is an left outer example but it also crashes with a regular inner join joins LongToUnsafeRowMap crashes with NegativeArraySizeException
Yes,Some codes in subexpressionEliminationForWholeStageCodegen are never used actually Remove them using this jira Remove unused codes in subexpressionEliminationForWholeStageCodegen
Yes, use StructType in CatalogTable and remove CatalogColumn
No,In Spark x it is possible to use int string and other functions to perform type cast This functionality is broken in Spark because Spark no longer falls back to Hive for these functions Spark breaks various Hive cast functions
No,Spark currently throws exceptions for invalid casts for all other data types except date type Somehow date type returns null It should be consistent and throws analysis exception as well Spark should throw analysis exception for invalid casts to date type
No,SubexpressionEliminationSuite Semantic equals and hash assumes the default AttributeReference s exprId wont be ExprId However that depends on when this test runs It may happen to use ExprId Fix a potential ExprId conflict for SubexpressionEliminationSuite Semantic equals and hash
No,In Spark we will parse float literals as decimals However it introduces a side effect which is described below Fail to create a decimal arrays with literals having different inferred precessions and scales
No,When a yarn rolling upgrade happens the Spark YarnShuffleService isn t re initializing the tokens soon enough which causes running applications to fail with NullPointerExceptions rather then IOExceptions which causes clients to not retry which in turn causes the application to totally fail when it should have just retried and succeeded shuffle server ERROR server TransportRequestHandler Error while invoking RpcHandler receive on RPC id java lang NullPointerException Password cannot be null if SASL is enabled at org spark project guava base Preconditions checkNotNull Preconditions java at org apache spark network sasl SparkSaslServer encodePassword SparkSaslServer java at org apache spark network sasl SparkSaslServer DigestCallbackHandler handle SparkSaslServer java at com sun security sasl digest DigestMD Server validateClientResponse DigestMD Server java at com sun security sasl digest DigestMD Server evaluateResponse DigestMD Server java at org apache spark network sasl SparkSaslServer response SparkSaslServer java at org apache spark network sasl SaslRpcHandler receive SaslRpcHandler java at org apache spark network server TransportRequestHandler processRpcRequest TransportRequestHandler java at org apache spark network server TransportRequestHandler handle TransportRequestHandler java at org apache spark network server TransportChannelHandler channelRead TransportChannelHandler java at org apache spark network server TransportChannelHandler channelRead TransportChannelHandler java at io netty channel SimpleChannelInboundHandler channelRead SimpleChannelInboundHandler java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty handler timeout IdleStateHandler channelRead IdleStateHandler java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty handler codec MessageToMessageDecoder channelRead MessageToMessageDecoder java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at org apache spark network util TransportFrameDecoder channelRead TransportFrameDecoder java at io netty channel AbstractChannelHandlerContext invokeChannelRead AbstractChannelHandlerContext java at io netty channel AbstractChannelHandlerContext fireChannelRead AbstractChannelHandlerContext java at io netty channel DefaultChannelPipeline fireChannelRead DefaultChannelPipeline java at io netty channel nio AbstractNioByteChannel NioByteUnsafe read AbstractNioByteChannel java at io netty channel nio NioEventLoop processSelectedKey NioEventLoop java at io netty channel nio NioEventLoop processSelectedKeysOptimized NioEventLoop java at io netty channel nio NioEventLoop processSelectedKeys NioEventLoop java at io netty channel nio NioEventLoop run NioEventLoop java at io netty util concurrent SingleThreadEventExecutor run SingleThreadEventExecutor java at java lang Thread run Thread java YarnShuffleService doesn t re init properly on YARN rolling upgrade
No,Hello I found this issue while testing my codebase with rc StructType in Spark accepts the Python type which is very handy rc does not and throws an error I don t know if this was intended but I d advocate for this behaviour to remain the same MapType is probably wasteful when your key names never change and switching to Python tuples would be cumbersome Here is a minimal script to reproduce the issue Thanks StructType doesn t accept Python dicts anymore
Yes, move BucketSpec to catalyst module and use it in CatalogTable
No,Summary to reproduce bug Create a DataFrame DF and sample it with a fixed seed Collect that DataFrame result Call a particular UDF on that DataFrame result You would expect results and to use the same rows from DF but they appear not to Note result and result are both deterministic See the attached notebook for details Cells in the notebook were executed in order Dataset sample with seed result seems to depend on downstream usage
No,Users should not be allowed to issue LOAD DATA against a view Currently when users doing it we got a very strange runtime error For example Strange Error when Issuing Load Table Against A View
Yes,This is similar with https issues apache org jira browse SPARK Currently JdbcUtils savePartition is doing type based dispatch for each row to write appropriate values So appropriate writers can be created first according to the schema and then apply them to each row This approach is similar with CatalystWriteSupport Avoid per record type dispatch in JDBC when writing
No,Calling persist on a data frame with more than columns is removing the data from the data frame This is an issue in Spark Works with out any issues in Spark Following test case demonstrates problem Please let me know if you need any additional information Thanks Spark Persist call on Data frames with more than columns is wiping out the data
No,SPARK will add the ability to blacklist entire executors and nodes to deal w faulty hardware However without displaying it on the UI it can be hard to realize which executor is bad and why tasks aren t getting scheduled on certain executors As a first step we should just show nodes and executors that are blacklisted for the entire application no need to show blacklisting for tasks stages This should also ensure that blacklisting events get into the event logs for the history server UI Should show blacklisted executors nodes
No,The following simple SQL query reproduces this issue Exception thrown This bug is a regression Spark doesn t have this issue LAST VALUE FALSE OVER throws IndexOutOfBoundsException
No,this query fails in but works in constraints propagation may fail the query
No,The case at https github com apache spark blob be ba e f f f bb ee c sql catalyst src main scala org apache spark sql catalyst analysis Analyzer scala L L is shown below This case will be triggered even when the function is an unresolved So when the functions like lead are used we may see errors like Window Frame RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW must match the required frame ROWS BETWEEN FOLLOWING AND FOLLOWING because we wrongly set the the frame specification ResolveWindowFrame should not be triggered on UnresolvedFunctions
Yes,Elt function doesn t support codegen execution It is better to provide the support Add codegen for Elt function
No,this will fail analysis query fails if having condition contains grouping column
No,Please see the attached notebook Seems lag lead somehow fail to recognize that a offset row does not exist and generate wrong results lag lead using constant input values does not return the default value when the offset row does not exist
No,The vectorized parquet reader fails to read certain tables created by Hive When the tables have type tinyint or smallint Catalyst converts those to ByteType and ShortType respectively But when Hive writes those tables in parquet format the parquet schema in the files contains int fields To reproduce run these commands in the hive shell or beeline tries to consolidate both schemas it just chooses whatever is in the parquet file for primitive types see ParquetReadSupport clipParquetType the vectorized reader uses the catalyst schema which comes from the Hive metastore and says it s a byte field so when it tries to read the data the byte data stored in OnHeapColumnVector is null I have tested a small change to ParquetReadSupport clipParquetType that fixes this particular issue but I haven t run any other tests so I ll do that while I wait for others to chime in and maybe tell me that s not the right place to fix this Vectorized parquet reader fails to read certain fields from Hive tables
No,Unfortunately I know very little about databases but I figure this is a bug I have a DataFrame with the following schema I am trying to write it to an Oracle database like this with the right driver against the RDS instance with engine type MySQL works The error message is the same as in https issues apache org jira browse SPARK Could it be that Long is also translated into the wrong data type Thanks Oracle JDBC table creation fails with ORA invalid datatype
No,Currently we don t check the value returned by called method in Invoke When the returned value is null NullPointerException will be thrown Fix NullPointerException when the returned value of the called method in Invoke is null
No,Suppose we have such Spark which is correct output for the app I think it s a bug It s expected to see only positive integers and avoid zeros Environment Spark version is Scala version is RDD pipe returns values for empty partitions
No,For ORC source Spark SQL has a writer option compression which is used to set the codec and its value will be also set to orc compress the orc conf used for codec However if a user only set orc compress in the writer option we should not use the default value of compression snappy as the codec Instead we should respect the value of orc compress When writing ORC files orc compress should not be overridden if users do not set compression in the options
No,Spark cannot select data from a table stored as an orc file which has been created by hive while hive or spark supports Steps Use hive to create a table tbtxt stored as txt and load data into it Use hive to create a table tborc stored as orc and insert the data from table tbtxt Example create table tborc stored as orc as select from tbtxt Use spark to select from tborc error occurs java lang IllegalArgumentException Field nid does not exist Spark cannot select data from a table stored as an orc file which has been created by hive while hive or spark supports
No,Spark error occurs when execute the sql statement which includes nvl function while spark supports Error org apache spark sql AnalysisException cannot resolve nvl b new user due to data type mismatch input to function coalesce should all be the same type but it s string int line pos state code Spark error occurs when execute the sql statement which includes nvl function while spark supports
Yes, Move regexp unit tests to RegexpExpressionsSuite
No,In MultivariateOnlineSummarizer min max method use judgement nnz i weightSum it will cause some numerial problem and make result unstable for example add two vector with weight e with weight e using MultivariateOnlineSummarizer min max we will get minVector maxVector but the right result should be minVector maxVector The bug reason is that e e e Double type because of the floating rounding and different accumulating or merging order may cause different result such as with weight e with weight e lines data with weight e using the input data order listed above we will get the result minVector maxVector but if the input data order is as following with weight e lines data with weight e with weight e than it the result will be minVector maxVector that s because e e e add times e Double type but e e add times e E e Double type Potential numerical problem in MultivariateOnlineSummarizer min max
No,When creating a data source table without explicit specification of schema or SELECT clause we silently ignore the bucket specification CLUSTERED BY SORTED BY For example Instead we should issue an error message Silent Ignore Bucket Specification When Creating Table Using Schema Inference
No,Caching multiple replicas of blocks is currently broken The following examples show replication doesn t happen for various use cases These were run using Spark preview in local cluster mode Again doesn t replicate data and executors show the same ClassNotFoundException These examples worked fine and showed expected results with Spark Caching data with replication doesn t replicate data
Yes,Dataframe drop supported multi columns in spark api and should make python api also support it Dataframe drop supported multi columns in spark api and should make python api also support it
No,When call SparkR sql an error pops up For instance https github com apache spark blob f bcc a d bdd f aa e c f a R pkg R SQLContext R L Cannot use SparkR sql
No,Here is the scenario I launch job into Q and allow it to grow to cluster utilization I wait between mins for this job to complete with of the cluster available takes about hr so job is between complete Note that if I wait less time then the issue sometimes does not occur it appears to be only after the job is at least complete I launch job into Q and preemption occurs on the Q shrinking the job to allow of cluster utilization At this point job basically halts progress while job continues to execute as normal and finishes Job either Fails its attempt and restarts By the time this attempt fails the other job is already complete meaning the second attempt has full cluster availability and finishes The job remains at its current progress and simply does not finish I have waited hrs until finally killing the application Looking into the error log there is this constant error message WARN NettyRpcEndpointRef Error sending message message RemoveExecutor Container container on host ip NUMBERS ec internal was preempted in X attempts My observations have led me to believe that the application master does not know about this container being killed and continuously asks the container to remove the executor until eventually failing the attempt or continue trying to remove the executor I have done much digging online for anyone else experiencing this issue but have come up with nothing Spark application not handling preemption messages
No,In HiveClientImpl there is a method to create database As you can see parameters field is set to null HiveClientImpl throws NPE when reading database from a custom metastore
No,Spark applications running on Mesos throw exception upon exit as follows Applications result is not affected by this error This issue can be simply reproduced by launching a spark shell and exit after running the following commands The root cause is that in SparkContext stop MesosCoarseGrainedSchedulerBackend stop calls CoarseGrainedSchedulerBackend stop The latter sends messages to stop executors and also stop the driver endpoint without waiting for the actual stop of executors MesosCoarseGrainedSchedulerBackend stop still waits for the executors to stop in a timeout During the wait MesosCoarseGrainedSchedulerBackend statusUpdate generally will be called to update executors status and in turn removeExecutor is called But at that time the driver endpoint is not available MESOS Spark application throws exception on exit
Yes,Currently filters for TimestampType and DecimalType are not being pushed down in ORC data source although ORC filters support both Support for pushing down filters for decimal and timestamp types in ORC
No,Run below SQL and get transformation script error for python script like below error message Query SQL SPARK SQL transformation script got failure for python script
No,Right now the YARN shuffle service will swallow errors that happen during startup and just log them This causes two undesirable things to happen because blockHandler will remain null when an error happens every request to the shuffle service will cause an NPE because the NM is running containers may be assigned to that host only to fail to register with the shuffle service Example of the first YARN shuffle service should throw errors when it fails to start
Yes, move hive hack for data source table into HiveExternalCatalog
No,In JobGenerator the always evaluates to true as the checkpoint duration is always set to be equal to the batch duration https s postimg org udy lti j spark streaming job generator png width Image URL https s postimg org udy lti j spark streaming job generator png Some batches might not get marked as fully processed in JobGenerator
No,If we create a table pointing to a parquet json datasets without specifying the schema describe table command does not show the schema at all It only shows Schema of this table is inferred at runtime In describe table does show the schema of such a table If a table s schema is inferred at runtime describe table command does not show the schema
No,This should now be doable with SPARK Bump master version to SNAPSHOT
No,Hello I am using apache spark I am executing bisecting k means algorithm on a specific dataset Dataset details K input vector K k Memory assigned GB per node number of nodes Till K it os working fine but when I set k it fails with java util NoSuchElementException key not found I suspect it is failing because of lack of some resources but somehow exception does not convey anything as why this spark job failed Please can someone point me to root cause of this exception why it is failing This is the exception stack trace Issue is that it is failing but not giving any explicit message as to why it failed BisectingKMeans Algorithm failing with java util NoSuchElementException key not found
Yes,It seems EqualNullSafe filter was missed for batch pruneing partitions in cached tables Supporting this improve the performance roughly it will vary Running the codes below Support partition batch pruning with EqualNullSafe predicate in InMemoryTableScanExec
No,When doing a CTAS with a Partition By clause we got a wrong error message For example Wrong messages when CTAS with a Partition By clause
No,Three broadcast variables created at the beginning of Word Vec fit are never deleted nor unpersisted This seems to cause excessive memory consumption on the driver for a job running hundreds of successive training They are Undeleted broadcast variables in Word Vec causing OoM for long runs
No,StreamingQuery explain shows N A when no data arrives It s pretty confusing Improve StreamingQuery explain when no data arrives
No,Trying to build spark but it fails because the maven is gone https www apache org dyn closer lua action download filename maven maven binaries apache maven bin tar gz Is this something that we control I saw the latest builds were updating to and that exists there Various mirrors and atleast all the ones I ve hit are missing it http mirrors koehn com apache maven maven maven missing from mirror breaks older builds
No,When create below table like below schema Spark SQL error out for struct type SQL Spark SQL Failed to create table due to catalog string error
Yes,ExternalShuffleService is essential for spark In order to better monitor shuffle service we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system Add metrics and source for external shuffle service
No,This is basically the same issue as SPARK https issues apache org jira browse SPARK but for linear regression where coefficients and featuresStd are unnecessarily serialized between stages LeastSquaresAggregator in Linear Regression serializes unnecessary data
Yes,When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider they will hit an error when resolving the relation Data Source APIs Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider
No,These two configs should not be relevant anymore after Spark Remove spark sql nativeView and spark sql nativeView canonical config
No,The catches NoSuchMethodError NoSuchMethodException thrown by Utils waitForProcess
No,Spark web UI The wrong value numCompletedTasks has been assigned to the variable numSkippedTasks See attachment for reference Spark web UI The wrong value numCompletedTasks has been assigned to the variable numSkippedTasks
Yes,Different from the other leaf nodes MetastoreRelation and SimpleCatalogRelation have a pre defined alias which is used to change the qualifier of the node However based on the existing alias handling alias should be put in SubqueryAlias This PR is to separate alias handling from MetastoreRelation and SimpleCatalogRelation to make it consistent with the other nodes For example below is an example query for MetastoreRelation which is converted to LogicalRelation Note the optimized plans are the same For SimpleCatalogRelation the existing code always generates two Subqueries Thus no change is needed Remove Alias from MetastoreRelation and SimpleCatalogRelation
No,The following Java Retag RDD to tallSkinnyQR of RowMatrix
No,I have df where column is struct type and there is M rows sample data from https issues apache org jira browse SPARK gives M in Spark in Spark Is there a change in IS NOT NULL behaviour in Spark IS NOT NULL clause gives false for nested not empty column
No,When creating a view a common user error is the number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW For example given Table t only has columns This error is very confusing Strange Errors When Creating View With Unmatched Column Num
Yes,Currently there are a few reports about Spark query performance regression for large queries This issue speeds up SQL query processing performance by removing redundant consecutive executePlan call in Dataset ofRows function and Dataset instantiation Specifically this issue aims to reduce the overhead of SQL query execution plan generation not real query execution So we can not see the result in the Spark Web UI Please use the following query script Before Speed up SQL query performance by removing redundant executePlan call in Dataset
Yes,LogicalPlan InsertIntoHiveTable is useless Thus we can remove it from the code base Remove InsertIntoHiveTable From Logical Plan
No,When a query containing LIMIT TABLESAMPLE the statistics could be zero Results are correct but it could cause a huge performance regression For example The statistics of both df and df are zero The statistics values should never be zero otherwise sizeInBytes of BinaryNode will also be zero product of children Incorrect Statistics when Queries Containing LIMIT TABLESAMPLE
No, Illegal Inputs In LIMIT or TABLESAMPLE
No,While trying to use a custom classpath for metastore jars spark sql hive metastore jars pointing at some filesystem path I ran into the following issue The issue here is that MRVersion is not packaged anywhere with Spark and the code in IsolatedClientLoader only ever tries the parent class loader when loading hadoop classes in this configuration So even though I had the class in the list of files in spark sql hive metastore jars Spark never tries to load it IsolatedClientLoader ignores needed Hadoop classes not present in Spark s loader
No,This is a weird corner case Users may hit this issue if they have a schema that has an array field whose element type is a struct and the struct has one and only one field and that field is named as element The following Spark shell snippet for Spark reproduces this bug Exception thrown This is because the nested struct field name happens to be element which is used as a dedicated name of the element type container group in the standard level layout and lead to the ambiguity Currently Spark x SNAPSHOT and master chose the nd one We can fix this issue by giving the standard level layout a higher priority when trying to match schema patterns Array of struct with a single field name element can t be decoded from Parquet files written by Spark
Yes,Currently our Optimizer may reorder the predicates to run them more efficient but in non deterministic condition change the order between deterministic parts and non deterministic parts may change the number of input rows For example may call rand for different times and therefore the output rows differ Improve the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition
No,The following works with spark but not anymore with spark spark result I can understand why tables with no columns might not be supported in SQL but in that case I would say that the dfNoCols registerTempTable call should fail with a more descriptive error select from temp table no cols fails
No,UPDATE Please start with this comment https issues apache org jira browse SPARK focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment I assume that problem results from the performance problem with reading parquet files Original Issue description I did some test on parquet file with many nested columns about G in partitions and Spark is x slower Spark min Spark min x slower I used BasicProfiler for this task and cumulative time was Spark sec Spark sec Should I expect such a drop in performance I don t know how to prepare sample data to show the problem Any ideas Or public data with many nested columns Spark Performance regression when reading parquet and using PPD and non vectorized reader
No,When the underlying file changes it can be very confusing to users when they see a FileNotFoundException It would be great to do the following Append a message to the FileNotFoundException that a workaround is to do explicitly metadata refresh Make metadata refresh work on temporary tables views Make metadata refresh work on Datasets DataFrames by introducing a Dataset refresh method Improve metadata refresh
No,If we have a linkage error in the user code Spark executors get killed immediately This is not great for user experience especially in shared environments in which multiple applications are multiplexed through one context LinkageError should not crash Spark executor
No,Running SparkR unit tests randomly has the following error Failed Error pipeRDD on RDDs test rdd R org apache spark SparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost org apache spark SparkException R computation failed with ignoring SIGPIPE signal Calls source lapply lapply FUN writeRaw writeBin Execution halted cannot open the connection Calls source computeFunc FUN system writeLines file In addition Warning message In file con w cannot open file tmp Rtmp Gr aU file de efc b No such file or directory Execution halted at org apache spark api r RRunner compute RRunner scala at org apache spark api r BaseRRDD compute RRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark scheduler ResultTask runTask ResultTask scala at org apache spark scheduler Task run Task scala at org apache spark executor Executor TaskRunner run Executor scala at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java This is related to daemon R worker mode By default SparkR launches an R daemon worker per executor and forks R workers from the daemon when necessary The problem about forking R worker is that all forked R processes share a temporary directory as documented at https stat ethz ch R manual R devel library base html tempfile html When any forked R worker exits either normally or caused by errors the cleanup procedure of R will delete the temporary directory This will affect the still running forked R workers because any temporary files created by them under the temporary directories will be removed together Also all future R workers that will be forked from the daemon will be affected if they use tempdir or tempfile to get tempoaray files because they will fail to create temporary files under the already deleted session temporary directory So in order for the daemon mode to work this problem should be circumvented In current dameon R R workers directly exits skipping the cleanup procedure of R so that the shared temporary directory won t be deleted Capture errors from R workers in daemon R to avoid deletion of R session temporary directory
Yes,Spark SQL currently falls back to Hive for xpath related functions Implement xpath user defined functions
No, SQLContext should import DataStreamReader
Yes,There are some duplicated code for options we should simplify them Cleanup options for DataFrame reader API in Python
Yes,This patch removes the blind fallback into Hive for functions Instead it creates a whitelist and adds only a small number of functions to the whitelist i e the ones we intend to support in the long run in Spark Whitelist the list of Hive fallback functions
No,koertkuipers identified the PR https github com apache spark pull changed the behavior of load API After the change the load API does not add the value of path into the options Thank you We should add the option path back to load API in DataFrameReader if and only if users specify one and only one path in the load API Add Path Option back to Load API in DataFrameReader
No,The above query failed as expected but an empty table t is created Empty Table Remains After CREATE TABLE AS SELECT fails
No,quote select percentile cast id as bigint cast as double from temp bla quote Works quote select percentile cast id as bigint from temp bla quote Throws quote Error in query No handler for Hive UDF org apache hadoop hive ql udf UDAFPercentile org apache hadoop hive ql exec NoMatchingMethodException No matching method for class org apache hadoop hive ql udf UDAFPercentile with bigint decimal Possible choices FUNC bigint array FUNC bigint double line pos quote Percentile needs explicit cast to double
No,Currently CSV data source write DateType and TimestampType as below It would be nicer if it write dates and timestamps as a formatted string just like JSON data sources Also CSV data source currently supports dateFormat option to read dates and timestamps in a custom format It might be better if this option can be applied in writing as well CSV data source does not write date and timestamp correctly
Yes,CollectSet cannot have map typed data because MapTypeData does not implement equals So if we find map type in CollectSet queries fail Improve the type check of CollectSet in CheckAnalysis
No,When we do not turn on the Hive Support the following query generates a confusing error message Unresolved Operator When Creating Table As Select Without Enabling Hive Support
No,Spark streaming documentation recommends application developers create static connection pools To clean up this pool we add a shutdown hook The problem is that in spark the shutdown hook for an executor will be called only for the first submitted job on the second and subsequent job submissions the shutdown hook for the executor will NOT be invoked problem not seen when using java problem not seen when using spark looks like this bug is caused by this modification from to https issues apache org jira browse SPARK steps to reproduce the problem install spark submit this basic spark application import org apache spark SparkContext SparkConf object MyPool def printToFile f java io File op java io PrintWriter Unit val p new java io PrintWriter f try op p finally p close def myfunc a def createEvidence printToFile new java io File var tmp evidence txt p p println the evidence sys addShutdownHook createEvidence object BasicSpark def main args Array String val sparkConf new SparkConf setAppName BasicPi val sc new SparkContext sparkConf sc parallelize to foreach i println f MyPool myfunc sc stop you will see that var tmp evidence txt is created now delete this file submit a second job you will see that var tmp evidence txt is no longer created on the second submission if you use java or spark the evidence file will be created on the second and subsequent submits Utils scala terminateProcess should call Process destroyForcibly if and only if Process destroy fails
Yes,We embed partitioning logic in FileSourceStrategy apply making the function very long This is a small refactoring to move it into its own functions Eventually we would be able to move the partitioning functions into a physical operator rather than doing it in physical planning Move RDD creation logic from FileSourceStrategy apply
No,Currently the column related comment attribute is stored in Metadata For example Compared with the existing SQL interface it is not user friendly to add such a comment attribute when defining a StructField Add New Methods for Comments in StructField and StructType
Yes,In and earlier releases we have package grouping in the generated Java API docs See http spark apache org docs api java index html However this disappeared in http spark apache org docs api java index html Rather than fixing it I d suggest removing grouping Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala No one complained about missing groups since Remove package grouping in genjavadoc
No,Env Scala Java We recently deprecated setLabelCol in ChiSqSelectorModel Switching to multiline is a workaround Reported the bug to genjavadoc at https github com typesafehub genjavadoc issues A single line ScalaDoc and deprecate annotation would break genjavadoc
Yes,We need hashCode and euqals in UnsafeMapData because of the behaivour of UnsafeMapData is different from that of ArrayBasedMapData Remove hashCode and euqals in ArrayBasedMapData
No,Both off heap and on heap variants of ColumnVector reserve can unfortunately overflow while reserving additional capacity during reads Avoid NegativeArraySizeException while reserving additional capacity in VectorizedColumnReader
No,In ListingFileCatalog the implementation of listLeafFiles is shown below When the number of user provided paths is less than the value of sparkSession sessionState conf parallelPartitionDiscoveryThreshold we will not use parallel listing which is different from what does for if the number of children of any inner dir is larger than the threshold we will use the parallel listing ListingFileCatalog does not list in parallel anymore
No,We forgot the getter of dropLast in OneHotEncoder getDropLast is missing in OneHotEncoder
No,When a cluster has PYSPARK PYTHON and PYSPARK DRIVER PYTHON environment variables set needed for using non system Python e g usr bin anaconda bin python then you are unable to override this per submission in YARN cluster mode When using spark submit in this case via LIVY to submit with an override spark submit master yarn deploy mode cluster conf spark yarn appMasterEnv PYSPARK DRIVER PYTHON python conf spark yarn appMasterEnv PYSPARK PYTHON python probe py the environment variable values will override the conf settings A workaround for some can be to unset the env vars but that is not always possible e g submitting batch via LIVY where you can only pass through the parameters to spark submit Expectation is that the conf values above override the environment variables Fix is to change the order of application of conf and env vars in the yarn client Related discussion https issues cloudera org browse LIVY Backporting this to would be great and unblocking for me Can t set Python via spark submit for YARN cluster mode when PYSPARK PYTHON PYSPARK DRIVER PYTHON are set
No,I get a similar error when using complex types in Aggregator Not sure if this is the same issue or something else gen GenerateMutableProjection generate GenerateMutableProjection scala at org apache spark sql execution SparkPlan newMutableProjection SparkPlan scala at org apache spark sql execution aggregate SortAggregateExec anonfun doExecute anonfun anonfun apply SortAggregateExec scala at org apache spark sql execution aggregate SortAggregateExec anonfun doExecute anonfun anonfun apply SortAggregateExec scala at org apache spark sql execution aggregate AggregationIterator generateProcessRow AggregationIterator scala at org apache spark sql execution aggregate AggregationIterator AggregationIterator scala at org apache spark sql execution aggregate SortBasedAggregationIterator SortBasedAggregationIterator scala at org apache spark sql execution aggregate SortAggregateExec anonfun doExecute anonfun apply SortAggregateExec scala at org apache spark sql execution aggregate SortAggregateExec anonfun doExecute anonfun apply SortAggregateExec scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd RDD anonfun mapPartitionsInternal anonfun apply apply RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala at org apache spark rdd RDD computeOrReadCheckpoint RDD scala at org apache spark rdd RDD iterator RDD scala at org apache spark scheduler ShuffleMapTask runTask ShuffleMapTask scala at org apache spark scheduler ShuffleMapTask runTask ShuffleMapTask scala at org apache spark scheduler Task run Task scala at org apache spark executor Executor TaskRunner run Executor scala at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java noformat Aggregator fails with Tungsten error when complex types are used for results and partial sum
No,For an application with YarnApplicationState FINISHED and FinalApplicationStatus FAILED invoking spark submit from command line will got Exception submit with SparkLauncher will got state with FINISHED which means app succeeded Also because the above fact in test YarnClusterSuite assert with false condition will not fail the test Yarn cluster mode should return consistent result for command line and SparkLauncher
No,Since they uses SparkContext Update setJobGroup clearJobGroup cancelJobGroup SparkR API to not require sc
Yes,Both VectorUDT and MatrixUDT are private APIs because UserDefinedType itself is private in Spark However in order to let developers implement their own transformers and estimators we should expose both types in a public API to simply the implementation of transformSchema transform etc Otherwise they need to get the data types using reflection Note that this doesn t mean to expose VectorUDT MatrixUDT classes We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type There are two ways to implement this following DataTypes java in SQL so Java users doesn t need the extra Define DataTypes in Scala Expose VectorUDT MatrixUDT in a public API
No,Several bugs have been found caused by integer overflows in Tungsten This JIRA is for taking a final pass before release to reduce potential bugs and issues We should do at least the following Raise exception early instead of later throwing NegativeArraySize which is slow and might cause silent errors Document clearly the largest array size we support in DataFrames To reproduce one of the issues Not sufficient array size checks to avoid integer overflows in Tungsten
No,Python only UDTs can t work well One example is PySpark SQL python only UDTs don t work well
No, JDBC Source Wrong Results when lowerBound is larger than upperBound in Column Partitioning
Yes,This issue adds read orc write orc to SparkR for API parity Add read orc write orc to SparkR
Yes,There is a ToDo of GenericArrayData class which is to eliminate boxing unboxing for a primitive array described here https github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance Prepare GenericArrayData implementation specialized for a primitive array
Yes,This is a debug only version of SPARK for tutorials and debugging of streaming apps it would be nice to have a text based socket source similar to the one in Spark Streaming It will clearly be marked as debug only so that users don t try to run it in production applications because this type of source cannot provide HA without storing a lot of state in Spark Add debug only socket source in Structured Streaming
Yes,Add a new API method called gapplyCollect for SparkDataFrame It does gapply on a SparkDataFrame and collect the result back to R Compared to gapply collect gapplyCollect offers performance optimization as well as programming convenience as no schema is needed to be provided This is similar to dapplyCollect add gapplyCollect for SparkDataFrame
No,DataFrameRead json string was removed as it was covered by DataFrameRead json string varargs But this breaks compilation where the compiler needs a method with single arg Example This works in and does not compile in DataFrameRead json path compatibility broken with Spark
No,LogisticRegressionAggregator class is used to collect gradient updates in ML logistic regression algorithm The class stores a reference to the coefficients array of length equal to the number of features It also stores a reference to an array of standard deviations which is length numFeatures also When a task is completed it serializes the class which also serializes a copy of the two arrays These arrays don t need to be serialized only the gradient updates are being aggregated This causes issues performance issues when the number of features is large and can trigger excess garbage collection when the executor doesn t have much excess memory This results in serializing numFeatures excess data When multiclass logistic regression is implemented the excess will be numFeatures numClasses numFeatures ML Logistic Regression aggregator serializes unnecessary data
No,This is observed while debugging https issues apache org jira browse SPARK We should fix it or disable it by default SerializationDebugger run into infinite loop
No,This notebook https databricks prod cloudfront cloud databricks com public ec e c eaaa f bcfc latest html demonstrates the bug The obvious issue is that nested UDTs are not supported if the UDT is Python only Looking at the exception thrown this seems to be because the encoder on the Java end tries to encode the UDT as a Java class which doesn t exist for the PythonOnlyUDT PySpark SQL python only UDTs don t support nested types
Yes,Interface method FileFormat prepareRead was added in PR https github com apache spark pull to handle a special case in the LibSVM data source However the semantics of this interface method isn t intuitive it returns a modified version of the data source options map Considering that the LibSVM case can be easily handled using schema metadata inside inferSchema we can remove this interface method to keep the FileFormat interface clean Remove FileFormat prepareRead
No,Issues with current reader behavior text without args returns an empty DF with no columns inconsistent its expected that text will always return a DF with value string field textFile without args fails with exception because of the above reason it expected the DF returned by text to have a value field orc does not have var args inconsistent with others json single arg was removed but that caused source compatibility issues SPARK user specified schema was not respected when text csv were used with no args SPARK The solution I am implementing is to do the following For each format there will be a single argument method and a vararg method For json parquet csv text this means adding json string etc For orc this means adding orc varargs Remove the special handling of text csv etc that returns empty dataframe with no fields Rather pass on the empty sequence of paths to the datasource and let each datasource handle it right For e g text data source should return empty DF with schema value string Harmonize the behavior of DataFrameReader text csv json parquet orc
Yes,It would be good to have an additional implementation which uses dense format for UnsafeArrayData to reduce memory footprint Current UnsafeArrayData implementation uses only a sparse format It is useful for an UnsafeArrayData that is created by a method fromPrimitiveArray which have no null value Introduce additonal implementation with a dense format for UnsafeArrayData
No, document the contract of encoder serializer expressions
Yes,This is for API parity of Scala API Refer to https issues apache org jira browse SPARK Add varargs type dropDuplicates function in SparkR
Yes,Right now Spark does not load hive site xml Based on users feedback it seems make sense to still load this conf file Originally this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances Let s avoid of using this way to load hive site xml Instead since hive site xml is a normal hadoop conf file we can first find its url using the classloader and then use Hadoop Configuration s addResource or add hive site xml as a default resource through Configuration addDefaultResource to load confs Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive Bring back the hive site xml support for Spark
Yes,Sometimes it doesn t make sense to specify partitioning parameters e g when we write data out from Datasets DataFrames into jdbc tables or streaming ForeachWriters We probably should add checks against this in DataFrameWriter Add assertNotPartitioned check in DataFrameWriter
Yes,We should expose codahale metrics for the codegen source text size and how long it takes to compile The size is particularly interesting since the JVM does have hard limits on how large methods can get Metrics for codegen size and perf
Yes,Spark s SBT build currently uses a fork of the sbt pom reader plugin but depends on that fork via a SBT subproject which is cloned from https github com scrapcodes sbt pom reader tree ignore artifact id This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted In order to address these issues I propose to publish a pre built binary of our forked sbt pom reader plugin to Maven Central under the org spark project namespace Publish Spark s forked sbt pom reader to Maven Central
Yes,There s no corresponding python api for KMeansSummary it would be nice to have it Add KMeanSummary in KMeans of PySpark
Yes,DataFrameWriter insertInto includes some Analyzer stuff We should move it to Analyzer Move some Analyzer stuff to Analyzer from DataFrameWriter
Yes,Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines Making them public should be safe even if we change internal formats Make DefaultParamsReadable Writable public APIs
No, better error message for writing bucketing data
Yes,Use the latest Sparksession to replace the existing SQLContext in MLlib Replace SQLContext with SparkSession in MLlib
No,Update spark ml and spark mllib migration guide from to ML QA migration guide update
Yes,See parent task SPARK python converage pyspark ml linalg
Yes,See parent task SPARK python converage ml regression module
Yes,See parent task SPARK python converage ml classification module
Yes,See parent task SPARK python converage ml recommendation module
Yes,See parent task SPARK bryanc did this component python coverage ml feature
Yes,This issue replaces all deprecated SQLContext occurrences with SparkSession in ML MLLib module except the following two classes These two classes use SQLContext as their function arguments ReadWrite scala TreeModels scala Replace SQLContext with SparkSession in ML MLLib
Yes,See containing JIRA for details SPARK ML QA Scala APIs audit for feature
Yes,See containing JIRA for details SPARK ML QA Scala APIs audit for evaluation tuning
No, fix slow tests
Yes,Support for partitioned parquet format in FileStreamSink was added in Spark now let s add support for partitioned csv json text format Add support for writing partitioned csv json text formats in Structured Streaming
Yes,Several classes and methods have been deprecated and are creating lots of build warnings in branch This issue is to identify and fix those items WithSGD classes Change to make class not deprecated object deprecated and public class constructor deprecated Any public use will require a deprecated API We need to keep a non deprecated private API since we cannot eliminate certain uses Python API streaming algs and examples Use in PythonMLlibAPI Change to using private constructors Streaming algs No warnings after we un deprecate the classes Examples Deprecate or change ones which use deprecated APIs MulticlassMetrics fields precision etc Eliminate MLlib build warnings from deprecations
No,We made several changes to ALS in It is necessary to run some tests to avoid performance regression We should test synthetic datasets from million ratings to billion ratings cc mlnick holdenk Do you have time to run some large scale performance tests Links Results spreadsheet https docs google com spreadsheets d iX LisfXcZSTCHp VPoo z eCO A VsZDtZ e ks edit usp sharing Raw results for SPARK https docs google com document d tlWFCv zWJuxv gfAhd TKURVyrYkF v FLl Lpn edit usp sharing Raw results for SPARK https docs google com document d qLLX Dg XJAgoSQzmb bSncjTHhg A JJcQneDiE edit usp sharing Performance test for ALS in Spark
No,Currently the explain of a query with whole stage codegen looks like this The problem is that the plan looks much different than logical plan make us hard to understand the plan especially when the logical plan is not showed together Improve the explain of whole stage codegen
Yes,This is an umbrella ticket to list issues I found with APIs for the release Spark SQL API audit
Yes,We removed some classes in Spark If the user uses an incompatible library he may see ClassNotFoundException It s better to give an instruction to ask people using a correct version Display a better message for not finding classes removed in Spark
Yes,We should open up the APIs for converting between new old linear algebra types in spark mllib linalg Vector asML Vectors fromML same for Sparse Dense and for Matrices I made these private originally but they will be useful for users transitioning workloads Make the mllib ml linalg type conversion APIs public
Yes,We re using asML to convert the mllib vector matrix to ml vector matrix now Using as is more correct given that this conversion actually shares the same underline data structure As a result in this PR toBreeze will be changed to asBreeze This is a private API as a result it will not affect any user s application Change toBreeze to asBreeze in Vector and Matrix
Yes,This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming Here is the design doc for an initial version of the Kafka Source https docs google com document d t rWe x tq e AOfrsM qb m BRuv fel i PqR edit usp sharing Old description Structured streaming doesn t have support for kafka yet I personally feel like time based indexing would make for a much better interface but it s been pushed back to kafka https cwiki apache org confluence display KAFKA KIP Add a time based log index Structured streaming support for consuming from Kafka
Yes,Since ml evaluation has supported save load at Scala side supporting it at Python side is very straightforward and easy PySpark ml evaluation should support save load
Yes,Now picklers for both new and old vectors are implemented under PythonMLlibAPI To separate spark mllib from spark ml we should implement them under spark ml python instead I set the target to since those are private APIs Implement Python picklers for ml Vector and ml Matrix under spark ml python
No,In SPARK we use VectorImplicits and asML in example We should consider update them during QA https github com dbtsai spark blob d ebacfb abf d d f fac d examples src main scala org apache spark examples ml DataFrameExample scala Example code shouldn t use VectorImplicits asML fromML
Yes,SPARK makes KMeansModel store the clusters one per row KMeansModel load method needs to be updated in order to load models saved with Spark Make spark ml KMeansModel load backwards compatible
Yes,Generate currently does not support code generation Lets add support for CG and for it and its most important generators explode and json tuple Implement code generation for Generate
Yes,We should add an interface to the GLR summaries in Python for feature parity Python API for Generalized Linear Regression Summary
Yes,LazyFileRegion was created so we didn t create a file descriptor before having to send the file see https issues apache org jira browse SPARK The change has been pushed back into Netty to support the same things under the DefaultFileRegion https github com netty netty issues https github com netty netty commit a b d c f b f e b a be It looks like that went into Final I believe at the time we created LazyFileRegion we were on Final and we are now using Final so we should be able to use the netty class directly Remove LazyFileRegion
Yes,Our current dataset registerTempTable does not actually materialize data So it should be considered as creating a temp view We can deprecate it and create a new method called dataset createTempView replaceIfExists Boolean The default value of replaceIfExists should be false For registerTempTable it will call dataset createTempView replaceIfExists true Deprecate registerTempTable and add dataset createTempView
Yes,Implement repartitionByColumn on DataFrame This will allow us to run R functions on each partition identified by column groups with dapply method SparkR Implement repartitionByColumn on DataFrame
No,After upgrading testthat package to version new warnings and a new failure were found in SparkR test cases Warnings multiple packages don t produce a warning test client R not is deprecated sparkJars sparkPackages as comma separated strings test context R not is deprecated spark survreg test mllib R not is deprecated date functions on a DataFrame test sparkSQL R Deprecated please use expect gt instead date functions on a DataFrame test sparkSQL R Deprecated please use expect gt instead date functions on a DataFrame test sparkSQL R Deprecated please use expect gt instead Method as data frame as a synonym for collect test sparkSQL R not is deprecated Failure showDF test sparkSQL R s produced no output Changes in releases of testthat can be found at https github com hadley testthat blob master NEWS md Fix warnings and a failure in SparkR test cases with testthat version
Yes,In SparkR spark kmeans take a DataFrame with double columns This is different from other ML methods we implemented which support R model formula We should add support for that as well Support formula in spark kmeans in SparkR
No,See the following repro cannot run aggregate function on explode result
No,This is request adding something in SparkR API like versionadded in PySpark API and since in Scala Java API Add since tag in Roxygen documentation for SparkR API methods
Yes,validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning PySpark TrainValidationSplitModel should support validationMetrics
Yes,JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up including eliminating unnecessary sorting and the use of inefficient collections Improve performance of JSON schema inference s inferField step
Yes,The YarnShuffleService currently just picks a directly in the yarn local dirs to store the leveldb file YARN added an interface in hadoop getRecoverPath to get the location where it should be storing this We should change to use getRecoveryPath This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop YarnShuffleService should use YARN getRecoveryPath for leveldb location
Yes,Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination We should support it Subexpression elimination in wholestage codegen version of TungstenAggregate
Yes,This issues aims to add new FoldablePropagation optimizer that propagates foldable expressions by replacing all attributes with the aliases of original foldable expression Other optimizations will take advantage of the propagated foldable expressions e g EliminateSorts optimizer now can handle the following Case and Case is the previous implementation Literals and foldable expression e g ORDER BY abc Now Foldable ordinals e g SELECT abc Now ORDER BY Foldable aliases e g SELECT x abc y Now z ORDER BY x y z Before Add FoldablePropagation optimizer
Yes,We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work For example there are various options in parquet mr that users might want to set but the data source API does not expose a per job way to set it This patch propagates the user specified options also into Hadoop Configuration Propagate data source options to Hadoop configurations
Yes,Since SPARK breaks behavior of HashingTF we should try to enforce good practice by removing the native hashingAlg option in spark ml and pyspark ml We can leave spark mllib and pyspark mllib alone Remove spark ml HashingTF hashingAlg option
Yes,It looks like the head master branch of Spark uses quite an old version of Jetty v There have been some announcement of security vulnerabilities notably in and there are versions of both and that address those We recently left a web ui port open and had the server compromised within days Albeit this upgrade shouldn t be the only security improvement made the current version is clearly vulnerable as is Upgrade Jetty to latest version of
Yes, Break SQLQuerySuite out into smaller test suites
Yes,In SPARK we switched to use GenericArrayData to store indices and values in vector matrix UDTs However GenericArrayData is not specialized for primitive types This might hurt MLlib performance badly We should consider either specialize GenericArrayData or use a different container cc cloud fan yhuai VectorUDT MatrixUDT should take primitive arrays without boxing
Yes,In current master we have ML methods in SparkR If we make this change we might want to avoid name collisions because they have different signature We can use ml kmeans ml glm etc Sorry for discussing API changes in the last minute But I think it would be better to have consistent signatures in SparkR cc shivaram josephkb yanboliang Make ML APIs in SparkR consistent
Yes,This is an umbrella ticket to reduce the difference between sql core and sql hive Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs Merge functionality in Hive module into SQL core module
Yes,In Spark we shouldn t have two parsers anymore There should be only a single one Merge HiveSqlAstBuilder and SparkSqlAstBuilder
No,There are a few test cases in HiveCompatibilitySuite for subquery we should enable them to have better coverage Enable the tests in HiveCompatibilitySuite for subquery
Yes,The current benchmark framework runs a code block for several iterations and reports statistics However there is no way to exclude per iteration setup time from the overall results Allow custom timing control in microbenchmarks
Yes, Rename upstreams inputRDDs in WholeStageCodegen
Yes,Provide API for SVM algorithm for DataFrames I would recommend using OWL QN rather than wrapping spark mllib s SGD based implementation The API should mimic existing spark ml classification APIs spark ml API for linear SVM
No,The spill size of Aggregate and Sort is broken the data size is actual peak memory Some SQL metrics is broken when whole stage codegen enabled
Yes,This issue aims to implement assert true function It s Hive Generic UDF Function https github com apache hive blob master ql src java org apache hadoop hive ql udf generic GenericUDFAssertTrue java since The following is function description of Hive Please note that Hive s false values are false null and empty string But in Spark we intentionally designed to use implicit typecasting to BooleanType Add assert true function
Yes,This issue aims to expose Scala bround function in Python R API bround function is implemented in SPARK by extending current round function We used the following semantics from Hive https github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java Add bround function in Python R
Yes,The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics They are unnecessarily convoluted and we should be able to simplify them quite a bit This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on At a high level I d would like to create better abstractions for internal implementations as well as creating a simplified accumulator v external interface that doesn t involve a complex type hierarchy Simplify accumulators and task metrics
Yes,This issue aims to add bound function aka Banker s round by extending current round implementation Hive supports bround since Language Manual https cwiki apache org confluence display Hive LanguageManual UDF Add bround function
No,While poking around with I noticed that a few places still referred to spark assembly jar so I updated where it made sense I also updated the usage section of spark submit since you can now use run example argument to run an example from spark submit which was mostly undocumented Minor doc usage changes related to removal of Spark assembly
Yes,When you have an error in your R code using the RDD API you always get as error message Error in if returnStatus argument is of length zero This is not very useful and I think it might be better to catch the R exception and show it instead Improve error messages for RDD API
Yes,Right now filter push down only works with Project Aggregate Generate and Join they can t be pushed through many other plans Improve filter push down
Yes,We currently disable codegen for CaseWhen if the number of branches is greater than in CaseWhen MAX NUM CASES FOR CODEGEN It would be better if this value is a non public config defined in SQLConf spark sql codegen maxCaseBranches config option
Yes,Currently Union only takes intersect of the constraints from it s children all others are dropped we should try to merge them together Improve constraints propagation in Union
Yes,It will be great to have these SQL functions IFNULL NULLIF NVL NVL The meaning of these functions could be found in oracle docs SQL function IFNULL NULLIF NVL and NVL
Yes,In Spark DataFrame is an alias of Dataset Row MLlib API actually works for other types of Dataset so we should accept Dataset instead It maps to Dataset in Java This is a source compatible change Accept Dataset instead of DataFrame in MLlib APIs
Yes, refactor object operator framework to make it easy to eliminate serializations
No,right now the string of DataSourceScan is only HadoopFiles xxx without any information about the table name or path Since we have that in this is kind of regression Show table name or path in string of DataSourceScan
No,According to the Spark Code Style Guide https cwiki apache org confluence display SPARK Spark Code Style Guide SparkCodeStyleGuide Indentation this issue add a new scalastyle rule to prevent the followings Add a new scalastyle NoScalaDoc to prevent ScalaDoc style multiline comments
Yes,In order to upgrade to Kryo we need to shade Kryo in our custom Hive fork Shade Kryo in our custom Hive fork
Yes,We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process Those name and what they are used for is very similar so I think we can merge them Also the original unescapeSQLString method may have a fault When u style character literals are passed to the method it s not unescaped successfully Merge ParserUtils and ParseUtils
Yes,Currently many functions do now show usages like the followings The only exceptions are cube grouping grouping id rollup window All functions should show usages by command DESC FUNCTION
Yes,Right now operations for an existing functions in SessionCatalog do not really check if the function exists We should add this check and avoid of doing the check in command SessionCatalog needs to check function existence
No,Because the concept of partitioning is associated with physical tables we disable all the supports of partitioned views which are defined in the following three commands in Hive DDL Manual https cwiki apache org confluence display Hive LanguageManual DDL LanguageManualDDL Create Drop AlterView An exception is thrown when users issue any of these three DDL commands Throw Exceptions for DDLs of Partitioned Views CREATE VIEW and ALTER VIEW
Yes,There is no unit test for KMeansSummary in spark ml Other items which could be fixed here Add Since version to KMeansSummary class Modify clusterSizes method to match GMM method to be robust to empty clusters in case we support that sometime See PR for SPARK Unit test for spark ml KMeansSummary
Yes,The time windowing function window was added to Datasets This JIRA is to track the status for the R Python and SQL API Dateset Time Windowing API for Python R and SQL
Yes, Decouple deserializer expression resolution from ObjectOperator
Yes,The toLocalIterator of RDD is super slow we should have a optimized implementation for Dataset DataFrame Add toLocalIterator for Dataset
Yes,We use a single object SparkRWrappers https github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala to wrap method calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakr ml r instead of spark ml api r Refactor GLMs code in SparkRWrappers
Yes,We use a single object SparkRWrappers https github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala to wrap method calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakr ml r instead of spark ml api r Refactor k means code in SparkRWrappers
No,Spark has some tests for compilation of Java sources using lambdas guarded behind a java tests maven profile but we currently do not build or run those tests As a result the tests no longer compile We should fix these tests and set up automated CI so that they don t break again Fix the java tests profile and run those tests in Jenkins
Yes,While running a Spark job which is spilling a lot of data in reduce phase we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method Please see the stack trace below Stack trace org xerial snappy SnappyNative YJP arrayCopy Native Method org xerial snappy SnappyNative arrayCopy SnappyNative java org xerial snappy Snappy arrayCopy Snappy java org xerial snappy SnappyInputStream rawRead SnappyInputStream java org xerial snappy SnappyInputStream read SnappyInputStream java java io DataInputStream readFully DataInputStream java java io DataInputStream readLong DataInputStream java org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java org apache spark util collection unsafe sort UnsafeSorterSpillMerger loadNext UnsafeSorterSpillMerger java org apache spark sql execution UnsafeExternalRowSorter next UnsafeExternalRowSorter java org apache spark sql execution UnsafeExternalRowSorter next UnsafeExternalRowSorter java The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data which is expensive We should fix Snappy java to use with non JNI based System arrayCopy method in this case Significant amount of CPU is being consumed in SnappyNative arrayCopy method
Yes, Reimplement TypedAggregateExpression to DeclarativeAggregate
Yes,Currently a method submitStage for waiting stages is called on every iteration of the event loop in DAGScheduler to submit all waiting stages but most of them are not necessary because they are not related to Stage status The case we should try to submit waiting stages is only when their parent stages are successfully completed This elimination can improve DAGScheduler performance Eliminate unnecessary submitStage call
Yes, Execute multiple Python UDFs in single batch
No,It would be very helpful for network performance investigation if we log the time spent on connecting and resolving host Add logs to help investigate the network performance
No,SPARK adds a programatic way to dump generated Add SQL command for printing out generated code for debugging
Yes, remove trait Queryable
Yes,remove consumeChild always create code for UnsafeRow and variables Simplify whole stage codegen interface
Yes,We only parse create function command In order to support native drop function command we need to parse it too Parse Drop Function DDL command
Yes,In naive Bayes we expect inputs to be individual observations In practice people may have the frequency table instead It is useful for us to support instance weights to handle this case Support weighted instances in naive Bayes
Yes,It would be nice to refactor the MemoryStore so that it can be unit tested without constructing a full BlockManager or needing to mock tons of things Refactor MemoryStore to be testable independent of BlockManager
Yes, Improve SparkStatusTracker to also track executor information
Yes,They were kept in SQLContext implicits object for binary backward compatibility in the Spark x series It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits Move StringToColumn implicit class into SQLImplicits
Yes,Currently for the key that can not fit within a long we build a hash map for UnsafeHashedRelation it s converted to BytesToBytesMap after serialization and deserialization We should build a BytesToBytesMap directly to have better memory efficiency Build BytesToBytesMap in HashedRelation
Yes,Currently spark history server REST API provides functionality to query applications by application start time range based on minDate and maxDate query parameters but it lacks support to query applications by their end time In this Jira we are proposing optional minEndDate and maxEndDate query parameters and filtering capability based on these parameters to spark history server REST API This functionality can be used for following queries Applications finished in last x minutes Applications finished before y time Applications finished between x time to y time Applications started from x time and finished before y time For backward compatibility we can keep existing minDate and maxDate query parameters as they are and they can continue support filtering based on start time range Add functionality in spark history sever API to query applications by end time
Yes,Per our discussion on the mailing list please see here http mail archives apache org mod mbox spark dev mbox CCA g F aVRBH WyyK nvBSLCMPtSdUuL Ge WW DnmnvY SXg mail gmail com E it would be nice to specify a custom coalescing policy as the current coalesce method only allows the user to specify the number of partitions and we cannot really control much The need for this feature popped up when I wanted to merge small files by coalescing them by size Add support for custom coalescers
Yes, make SubqueryHolder an inner class
Yes,When SortOrder does not contain any reference it has no effect on the sorting Remove the noop SortOrder in Optimizer Remove noop SortOrder in Sort
Yes,Per nongli s suggestions We should do these things Remove the non vectorized parquet reader code Support the remaining types just big decimals Move the logic to determine if our parquet reader can be used to planning Only complex types should fall back to the parquet mr reader Cleanup Extend the Vectorized Parquet Reader
Yes,We should add support for caching serialized data off heap within the same process i e using direct buffers or sun misc unsafe I ll expand this JIRA later with more detail filing now as a placeholder Add support for off heap caching
Yes,When a block is persisted in the MemoryStore at a serialized storage level the current MemoryStore putIterator code will unroll the entire iterator as Java objects in memory then will turn around and serialize an iterator obtained from the unrolled array This is inefficient and doubles our peak memory requirements Instead I think that we should incrementally serialize blocks while unrolling them A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk However I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case Incrementally serialize blocks while unrolling them in MemoryStore
No, hive tests should fail if SQL generation failed
No,The patch for SPARK added a number of MiMa excludes for cases which were missed due to our old way of programatically generating excludes in GenerateMIMAIgnore Before Spark we need to audit the additional excludes that I added to make sure that none represent unintentional incompatibilities which should be fixed Audit MiMa excludes added in SPARK to make sure none are unintended incompatibilities
Yes,This is to support order by position in SQL e g This should be controlled by config option spark sql groupByOrdinal Support group by ordinal in SQL
Yes,Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment We can call the new module mllib local which might contain local models in the future The major issue is to remove dependencies on user defined types The package name will be changed from mllib to ml For example Vector will be changed from org apache spark mllib linalg Vector to org apache spark ml linalg Vector The return vector type in the new ML pipeline will be the one in ML package however the existing mllib code will not be touched As a result this will potentially break the API Also when the vector is loaded from mllib vector by Spark SQL the vector will automatically converted into the one in ml package Separate out local linear algebra as a standalone module without Spark dependency
Yes,Recently the fast serialization has been introduced to collecting DataFrame Dataset The same technology can be used on collect limit operator too Apply fast serialization on collect limit
Yes,Logging was made private in Spark If we move it then users would be able to create a Logging trait themselves to avoid changing their own code Alternatively we can also provide in a compatibility package that adds logging Move org apache spark Logging into org apache spark internal Logging
Yes,Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo e g RDDs whose key value and or combiner types are primitives arrays of primitives or strings This is likely to result in a large performance gain for many RDD API workloads Automatically use Kryo serializer when shuffling RDDs with simple types
Yes,This continues the work of SPARK SPARK and SPARK to expose R like model summary in more family and link functions Expose R like summary statistics in SparkR glm for more family and link functions
No,It s common for many SQL operators to not care about reading null values for correctness Currently this is achieved by performing isNotNull checks for all relevant columns on a per row basis Pushing these null filters in parquet vectorized reader should bring considerable benefits especially for cases when the underlying data doesn t contain any nulls or contains all nulls Filter rows with null attributes in parquet vectorized reader
Yes,Instead of storing serialized blocks in individual ByteBuffers the BlockManager should be capable of storing a serialized block in multiple chunks each occupying a separate ByteBuffer This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks Our current serialization code uses a ByteBufferOutputStream which doubles and re allocates its backing byte array this increases the peak memory requirements during serialization since we need to hold extra memory while expanding the array In addition we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array so a megabyte serialized block may actually consume megabytes of memory After switching to storing blocks in multiple chunks we ll be able to efficiently trim the backing buffers so that no space is wasted This change is also a prerequisite to being able to cache blocks which are larger than GB although full support for that depends on several other changes which have not bee implemented yet Store serialized blocks as multiple chunks in MemoryStore
Yes,Currently Spark allows only a few cluster managers viz Yarn Mesos and Standalone But as Spark is now being used in newer and different use cases there is a need for allowing other cluster managers to manage spark components One such use case is embedding spark components like executor and driver inside another process which may be a datastore This allows colocation of data and processing Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again So this JIRA requests two functionalities Support for external cluster managers Allow a cluster manager to clean up the tasks without taking the parent process down Add support for pluggable cluster manager
Yes,Our code can go through SessionState catalog This brings two small benefits Reduces internal dependency on SQLContext Removes another public method in Java Java does not obey package private visibility More importantly according to the design in SPARK we d need to claim this catalog function for the user facing public functions rather than having an internal field Remove SQLContext catalog internal method
Yes,In general it is better for internal classes to not depend on the external class in this case SQLContext to reduce coupling between user facing APIs and the internal implementations Remove some internal classes dependency on SQLContext
Yes,DescribeCommand should just take a TableIdentifier and ask the metadata catalog for table s information Remove DescribeCommand s dependency on LogicalPlan
Yes,We introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these We still plan to implement a full local mode but it s probably going to be fairly different from what the current iterator based local mode would look like Let s just remove them for now and we can always re introduced them in the future by looking at branch Remove org apache spark sql execution local
Yes,When we generate code for join we copy the output row because there could be multiple output row from single input row We could avoid this copy when there is no join or the join will not generate multiple output rows from single input row Avoid the copy in whole stage codegen when there is no joins
Yes,When reading data from the DiskStore and attempting to cache it back into the memory store we should guard against race conditions where multiple readers are attempting to re cache the same block in memory Guard against race condition when re caching spilled bytes in memory
No,Given two task with M result on each it take more than seconds to fetch the results The RPC may be not designed to handle large block we should use block manager for that But currently this is based on spark rpc message maxSize which is usually very large M for safe it s too large for handling results We also counting the time to fetch the direct result also deserialize it as schedule delay it also make sense to only fetch much smaller blocks via DirectResult Fetch large directly result from executor is very slow
Yes,When generated code accesses a ColumnarBatch object it is possible to get values of each column from ColumnVector instead of calling getRow Direct consume ColumnVector in generated code when ColumnarBatch is used
Yes,If the Window does not have any window expression it is useless It might happen after column pruning Eliminate Unnecessary Window
Yes,We are using SELECT as a dummy table when the table is used for SQL statements in which a table reference is required but the contents of the table are not important For example In this case we will see a useless Project whose projectList is empty after executing ColumnPruning rule Remove Project when its projectList is Empty
Yes,RandomSampler sample currently accepts iterator as input and output another iterator This makes it inappropriate to use in wholestage codegen of Sampler operator We should add non iterator interface to RandomSampler Add non iterator interface to RandomSampler
Yes,Push down the predicate through the Window operator In this JIRA predicates are pushed through Window if and only if the following conditions are satisfied Predicate involves one and only one column that is part of window partitioning key Window partitioning key is just a sequence of attributeReferences i e none of them is an expression Predicate must be deterministic Predicate Push Down Through Window Operator
Yes,projectList is useless Remove it from the class Window It simplifies the codes in Analyzer and Optimizer Remove projectList from Windows
Yes,Today both the MemoryStore and DiskStore implement a common BlockStore API but I feel that this API is inappropriate because it abstracts away important distinctions between the behavior of these two stores For instance the disk store doesn t have a notion of storing deserialized objects so it s confusing for it to expose object based APIs like putIterator and getValues instead of only exposing binary APIs and pushing the responsibilities of serialization and deserialization to the client As part of a larger BlockManager interface cleanup I d like to remove the BlockStore API and refine the MemoryStore and DiskStore interfaces to reflect more narrow sets of responsibilities for those components Remove BlockStore interface to more cleanly reflect different memory and disk store responsibilities
Yes,When a cached block is spilled to disk and read back in serialized form i e as bytes the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch Therefore I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels There are two places where we request serialized bytes from the BlockStore getLocalBytes which is only called when reading local copies of TorrentBroadcast pieces Broadcast pieces are always cached using a serialized storage level so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store the non shuffle block branch in getBlockData which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms since those blocks seem more likely to be read in local computation Therefore I think this is a safe change Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills
Yes, QueryPlan expressions should always include all expressions
Yes,Right now we use PhysicalRDD for both existing RDD and data sources they are becoming much different we should use different physical plans for them Use different physical plan for existing RDD and data sources
Yes,A majority of Spark SQL queries likely run though HadoopFSRelation however there are currently several complexity and performance problems with this path Simplify and Speedup HadoopFSRelation
Yes,In preparation for larger refactorings I think that we should remove the confusing returnValues option from the BlockStore put APIs returning the value is only useful in one place caching and in other situations such as block replication it s simpler to put and then get Remove returnValues from BlockStore APIs
Yes,We need to submit another PR against Spark to call the task failure callbacks before Spark calls the close function on various output streams For example we need to intercept an exception and call TaskContext markTaskFailed before calling close in the following Changes to Spark should include unit tests to make sure this always work in the future Invoke task failure callbacks before calling outputstream close
Yes,Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin it has the same implementation as LeftSemiJoinBNL we should remove that Remove LeftSemiJoinBNL
Yes,After SPARK we should add Python API for generalized linear regression Python API for GeneralizedLinearRegression
Yes,We already have internal APIs for Hive to do this We should do it for SQLContext too so we can merge these code paths one day Track current database in SQL HiveContext
Yes,Remove all the deterministic conditions in a Filter that are contained in the Child Prune Filters based on Constraints
Yes,This is just a clean up task Today there are all these fields in SQLContext that are not organized in any particular way However since each SQLContext is a session many of these fields are actually isolated per session To minimize the size of these context files and provide a logical grouping that makes more sense I propose that we move these fields into its own class called SessionState Refactor Move SQLContext HiveContext per session state to separate class
Yes,After SPARK we should add Python API for MaxAbsScaler Python API for MaxAbsScaler
Yes,As part of Spark we want to create a stable API foundation for Dataset to become the main user facing API in Spark This ticket tracks various tasks related to that The main high level changes are Merge Dataset DataFrame Create a more natural entry point for Dataset SQLContext HiveContext are not ideal because of the name SQL Hive and SparkContext is not ideal because of its heavy dependency on RDDs First class support for sessions First class support for some system catalog See the design doc for more details Dataset oriented API evolution in Spark
Yes,With column pruning rule in optimizer we will introduce redundant project for some cases We should prevent it Remove redundant project in colum pruning rule
Yes,TaskContext supports task completion callback which gets called regardless of task failures However there is no way for the listener to know if there is an error This ticket proposes adding a new listener that gets called when a task fails Add a task failure listener to TaskContext
Yes,Following SPARK we can add a wrapper for naive Bayes in SparkR R s naive Bayes implementation is from package e with signature It should be easy for us to match the parameters Naive Bayes wrapper in SparkR
Yes,I think model summary interface which is available in Spark s scala Java and R interfaces should also be available in the python interface Similar to SPARK https issues apache org jira browse SPARK Expose ml summary function in PySpark for classification and regression models
Yes,Support queries that JOIN tables with USING clause SELECT from table JOIN table USING Support USING clause in JOIN
Yes,Currently the sbin start stop mesos dispatcher scripts only assume there is one mesos dispatcher launched but potentially users that like to run multi tenant dispatcher might want to launch multiples It also helps local development to have the ability to launch multiple ones Add support for launching multiple Mesos dispatchers
No, Add benchmark codes for Encoder compress in CompressionSchemeBenchmark
Yes,An broadcasted table could be used multiple times in a query we should cache them Avoid duplicated broadcasts
Yes,The current implementation of statistics of UnaryNode does not considering output for example Project we should considering it to have a better guess Considering output for statistics of logical plan
Yes,This bug is reported by Stuti Awasthi https www mail archive com user spark apache org msg html The lossSum has possibility of infinity because we do not standardize the feature before fitting model we should support feature standardization Another benefit is that standardization will improve the convergence rate AFTSurvivalRegression should support feature standardization
No,As per discussion in https github com apache spark pull discussion r we should improve the error message Better error message if path is not specified
Yes,Union Distinct has two Distinct that generate two Aggregation in the plan Remove an Extra Distinct in Union
Yes,For lots of SQL operators we have metrics for both of input and output the number of input rows should be exactly the number of output rows of child we could only have metrics for output rows After we improve the performance using whole stage codegen the overhead of SQL metrics are not trivial anymore we should avoid that if it s not necessary Some of the operator does not have SQL metrics we should add that for them For those operators that have the same number of rows from input and output for example Projection we may don t need that Remove duplicated SQL metrics
Yes,in newMutableProjection it will fallback to InterpretedMutableProjection if failed to compile Since we remove the configuration for codegen we are heavily reply on codegen also TungstenAggregate require the generated MutableProjection to update UnsafeRow should remove the fallback which could make user confusing see the discussion in SPARK Remove fallback in codegen
Yes,Spark SQL should collapse adjacent Repartition operators and only keep the last one Collapse adjacent Repartition operations
No,try to avoid the suffix unique id remove multiple empty lines in generated improve readability of generated code
Yes,We currently delegate most DDLs directly to Hive through NativePlaceholder in HiveQl scala In Spark we want to provide native implementations for DDLs for both SQLContext and HiveContext The first step is to properly parse these DDLs and then create logical commands that encapsulate them The actual implementation can still delegate to HiveNativeCommand As an example we should define a command for RenameTable with the proper fields and just delegate the implementation to HiveNativeCommand we might need to track the original sql query in order to run HiveNativeCommand but we can remove the sql query in the future once we do the next step Once we flush out the internal persistent catalog API we can then switch the implementation of these newly added commands to use the catalog API Create native DDL commands
No,Best time should be more stable than average time in benchmark together with average time they could show more information Use best time and average time in micro benchmark
No,Use lower case Change long prefixes to something shorter in this case I am changing only one TungstenAggregate agg Make whole stage codegen variable names slightly easier to read
Yes,When you define a class inside of a package object the name ends up being something like org mycompany project package MyClass However when reflect on this we try and load org mycompany project MyClass Support for classes defined in package objects
Yes, remove GenericInternalRowWithSchema
No,Dataset aggregators with complex types fail with unable to find encoder for type stored in a Dataset Though Datasets with these complex types are supported No encoder implicits for Seq Primitive
Yes,As of Spark Spark SQL internally has only a limited catalog and does not support any of the DDLs This is an umbrella ticket to introduce an internal API for a system catalog and the associated DDL implementations using this API Native database table system catalog
No, Improve test coverage for whole stage codegen
Yes,The StateDStream currently does not provide the batch time as input to the state update function This is required in cases where the behavior depends on the batch start time We Conviva have been patching it manually for the past several Spark versions but we thought it might be useful for others as well Add API for updateStateByKey to provide batch time as input
No, fix random generator for map type
Yes,Implement a simple wrapper in SparkR to support k means K means wrapper in SparkR
Yes,Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis Survival analysis in SparkR
Yes,Parquet files benefit from vectorized decoding ColumnarBatches have been designed to support this This means that a single encoded parquet column is decoded to a single ColumnVector Vectorize parquet decoding using ColumnarBatch
Yes,As benchmarked and discussed here https github com apache spark pull files r Benefits from codegen the declarative aggregate function could be much faster than imperative one we should re implement all the builtin aggregate functions as declarative one For skewness and kurtosis we need to benchmark it to make sure that the declarative one is actually faster than imperative one Reimplement stat functions as declarative function
No, Add test suite for EliminateSubQueries
No,This will be only adding new tests as followup PR to https github com apache spark pull When all labels are the same it s a dangerous ground for LogisticRegression without intercept to converge GLMNET doesn t support this case and will just exit GLM can train but will have a warning message saying the algorithm doesn t converge Add tests to make sure that ml classification LogisticRegression returns meaningful result when labels are the same without intercept
No, Visualization and metrics for generated operators
No, simplify bucket tests and add more comments
Yes,I was investingating progress in SPARK and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module Java Scala s examples SPARK use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK Does the class have different name in PySpark maybe Also I couldn t find any JIRA task to saying it need to be implemented Is it by design that the TrainValidationSplit estimator is not ported to PySpark If not that is if the estimator needs porting then I would like to contribute TrainValidationSplit is missing in pyspark ml tuning
No, Add more comment in HiveTypeCoercion for type widening
Yes,CacheManager directly calls MemoryStore unrollSafely and has its own logic for handling graceful fallback to disk when cached data does not fit in memory However this logic also exists inside of the MemoryStore itself so this appears to be unnecessary duplication Thanks to the addition of block level read write locks we can refactor the code to remove the CacheManager and replace it with an atomic getOrElseUpdate BlockManager method Remove CacheManager and replace it with new BlockManager getOrElseUpdate method
Yes,In Spark MLlib provides logistic regression and linear regression with L L elastic net regularization We want to expand the support of generalized linear models GLMs in e g Poisson Gamma families and more link functions SPARK implements a GLM solver for the case when the number of features is small We also need to design an interface for GLMs In SparkR we can simply follow glm or glmnet On the Python Scala Java side the interface should be consistent with LinearRegression and LogisticRegression e g from GeneralizedLinearModel Estimator interface for generalized linear models GLMs
Yes,As a pre requisite to off heap caching of blocks we need a mechanism to prevent pages blocks from being evicted while they are being read With on heap objects evicting a block while it is being read merely leads to memory accounting problems because we assume that an evicted block is a candidate for garbage collection which will not be true during a read but with off heap memory this will lead to either data corruption or segmentation faults To address this we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely I propose to do this in two phases first add a safe conservative approach in which all BlockManager get calls implicitly increment the reference count of blocks and where tasks references are automatically freed upon task completion This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions In phase two we should incrementally add release calls in order to fix the eviction of unreferenced blocks The latter change may need to touch many different components which is why I propose to do it separately in order to make the changes easier to reason about and review Use reference counting to prevent blocks from being evicted during reads
Yes,It would be easier to fix bugs and maintain the ec script separately from Spark releases For more information see https issues apache org jira browse SPARK Move spark ec scripts to AMPLab
Yes,Right now We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions the result projection of join could be very expensive if they generate lots of rows could be reduce mostly by condition SortMergeJoin and BroadcastHashJoin should support condition
Yes,Our current Intersect physical operator simply delegates to RDD intersect We should remove the Intersect physical operator and simply transform a logical intersect into a semi join This way we can take advantage of all the benefits of join implementations e g managed memory code generation broadcast joins Rewrite Intersect phyiscal plan using semi join
Yes,Add hash function for SparkR SparkR support hash function
Yes,This is beneficial just from a code testability point of view to be able to exercise individual components Also makes it easy to benchmark it It would be able to read data without need to create al the associate hadoop input split etc components Expose API on UnsafeRowRecordReader to just run on files
Yes,Before https github com apache spark pull submitJob would create a separate thread to wait for the job result submitJobThreadPool was a workaround in ReceiverTracker to run these waiting job result threads Now https github com apache spark pull has been merged to master and resolved this blocking issue submitJobThreadPool can be removed now Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result
Yes,MLlib s Transformer uses the deprecated callUDF API Remove the use of the deprecated callUDF in MLlib
Yes,Right now numFields will be passed in by pointTo then bitSetWidthInBytes is calculated making pointTo a little bit heavy It should be part of constructor of UnsafeRow The numFields of UnsafeRow should not changed by pointTo
Yes, Support window functions in SQLContext
Yes, Support intersect except in Hive SQL
Yes,cc nongli please attach the design doc bucketed table support
Yes,We can provides the option to choose JSON parser can be enabled to accept quoting of all character or not For example if JSON file that includes not listed by JSON backslash quoting specification it returns corrupt record This issue similar to HIVE HIVE Add option to accept quoting of all character backslash quoting mechanism
Yes,Right now the Java users cannot use ActorHelper because it uses special Scala syntax This patch just refactored the codes to provide Java API and add an example Refactor ActorReceiver to support Java
No,When I run a sql query in spark sql the Execution page of SQL tab is always blank But the JDBCServer is not blank SQL page of Spark sql is always blank
Yes,Input SELECT FROM jdbcTable WHERE col xxx Current plan Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter
Yes,We should update to the latest version of Zinc in order to match our SBT version Upgrade Zinc from to
Yes,Remove spark deploy mesos zookeeper dir and use existing configuration spark deploy zookeeper dir for Mesos cluster mode Remove spark deploy mesos zookeeper dir and use spark deploy zookeeper dir
Yes,Remove spark deploy mesos zookeeper url and use existing configuration spark deploy zookeeper url for Mesos cluster mode Remove spark deploy mesos zookeeper url and use spark deploy zookeeper url
Yes,Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode configuration for cluster mode Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode
Yes,We should add SQLUserDefinedType support for encoder Add SQLUserDefinedType support for encoder
Yes,CSV is the most common data format in the small data world It is often the first format people want to try when they see Spark on a single node Making this built in for the most common source can provide a better experience for first time users We should consider inlining https github com databricks spark csv Have a built in CSV data source implementation
No,We can point them to spark packages org to find them Improve error messages for data sources when they are not found
Yes,Creating an actual logical physical operator for range for matching the performance of RDD Range APIs Compared with the old Range API the new version is times faster than the old version Improve performance of Range APIs via adding logical physical operators
No,Add ML example for SparkR Add ML example for SparkR
Yes,As discussed here https github com apache spark pull it might need to implement unhandledFilter to remove duplicated Spark side filtering Implement unhandledFilter interface
Yes,distinct and unique drop duplicated rows on all columns While dropDuplicates can drop duplicated rows on selected columns Implement dropDuplicates method of DataFrame in SparkR
Yes,Like shuffle file encryption in SPARK spills data should also be encrypted Support shuffle spill encryption in Spark
Yes,IsNotNull filter is not being pushed down for JDBC datasource It looks it is SQL standard according to SQL SQL SQL and SQL x and I believe most databases support this isnotnull operator not pushed down for JDBC datasource
Yes,Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases Use sqlContext from MLlibTestSparkContext for spark ml test suites
Yes,There have been continuing requests e g SPARK for allowing users to extend and modify MLlib models and algorithms If you are a user who needs these changes please comment here about what specifically needs to be modified for your use case Remove final from classes in spark ml trees and ensembles where possible
Yes, Support UnsafeRow in LocalTableScan
Yes, Support UnsafeRow in Coalesce Except Intersect
Yes, Support UnsafeRow in MapPartitions MapGroups CoGroup
Yes,There are still some SparkPlan does not support UnsafeRow or does not support well Support UnsafeRow in all SparkPlan if possible
No,We need to document the new off heap memory limit configurations which were added in Spark add simple configuration validation for instance you shouldn t be able to enable off heap execution when the off heap memory limit is zero and alias the old and confusing spark unsafe offHeap configuration to something that lives in the spark memory namespace Document Spark s off heap memory configurations and add config validation
Yes,Similar to Dataset transform DataFrame transform function
Yes,mutate in the dplyr package supports adding new columns and replacing existing columns But currently the implementation of mutate in SparkR supports adding new columns only Also make the behavior of mutate more consistent with that in dplyr Throw error message when there are duplicated column names in the DataFrame being mutated when there are duplicated column names in specified columns by arguments the last column of the same name takes effect Enhance mutate to support replace existing columns
Yes,Starting from Hive the derby metastore can use a in memory backend Since our execution hive is a fake metastore if we use in memory mode we can reduce the time that is used on creating the execution hive Use in memory for execution hive s derby metastore
Yes, Implement drop method for DataFrame in SparkR
Yes,Created a new private variable boundTEncoder that can be shared by multiple functions RDD select and collect Replaced all the queryExecution analyzed by the function call logicalPlan A few API comments are using wrong class names e g DataFrame or parameter names e g n A few API descriptions are wrong e g mapPartitions SQL Code refactoring and comment correction in Dataset APIs
Yes,Kafka already released and it introduce new consumer API that not compatible with old one So I added new consumer api I made separate classes in package org apache spark streaming kafka v with changed API I didn t remove old classes for more backward compatibility User will not need to change his old spark applications when he uprgade to new Spark version Please rewiew my changes Update KafkaDStreams to new Kafka Consumer API
No,This is an umbrella issue addressing all SparkR related issues corresponding to Spark being planned SparkR
No,So far we are using comma separated decimal format to output the encoded contents This way is rare when the data is in binary This could be a common issue when we use Dataset API For example SQL Display the binary encoded values
Yes,Currently we support read df write df jsonFile parquetFile in SQLContext we should support more external data source API such as read json read parquet read orc read jdbc read csv and so on Some of the exist API is deprecated and will remove at Spark we should also deprecate them at SparkR Note we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style DataFrameReader API http spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API http spark apache org docs latest api scala index html org apache spark sql DataFrameWriter Support more external data source API in SparkR
Yes, Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver
Yes, Change numPartitions in RDD to be getNumPartitions to be consistent with pyspark scala
Yes,I think that we should upgrade from Tachyon to in order to get the fix for https tachyon atlassian net browse TACHYON Upgrade Tachyon dependency to
Yes,Add isNaN to Column for SparkR Column should has three related variable functions isNaN isNull isNotNull Replace DataFrame isNaN with DataFrame isnan at SparkR side Because DataFrame isNaN has been deprecated and will be removed at Spark Add isnull to DataFrame for SparkR DataFrame should has two related functions isnan isnull Fix usage of isnan isNaN
Yes,Change cumeDist cume dist denseRank dense rank percentRank percent rank rowNumber row number There are two reasons that we should make this change We should follow the naming convention rule of R http www inside r org node Spark DataFrame has deprecated the old convention such as cumeDist and will remove it in Spark It s better to fix this issue before release otherwise we will make breaking API change Rename some window rank function names for SparkR
Yes,Spark SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum Stddev Variance etc should support columnName as arguments
Yes, unify GetStructField and GetInternalRowField
Yes,repartition Returns a new Dataset that has exactly numPartitions partitions coalesce Returns a new Dataset that has exactly numPartitions partitions Similar to coalesce defined on an RDD this operation results in a narrow dependency e g if you go from partitions to partitions there will not be a shuffle instead each of the new partitions will claim of the current partitions SQL Support coalesce and repartition in Dataset APIs
No,When invFunc argument of reduceByKeyAndWindow is None checkpointing should not be required Scala implementation of reduceByKeyAndWindow handles this correctly and does not require checkpointing Python version requires that checkpoint directory be specified it is a bug pyspark reduceByKeyAndWindow with invFunc None requires checkpointing
No,Right now the table of contents gets generated on a page by page basis which makes it hard to navigate between different topics in a project We should make use of the empty space on the left of the documentation to put a navigation menu A picture is worth a thousand words Add a menu to the documentation of MLlib
Yes,Implement struct encode decode in SparkR as documented at http spark apache org docs latest api scala index html org apache spark sql functions Implement struct encode decode in SparkR
Yes,Collection functions documented at http spark apache org docs latest api scala index html org apache spark sql functions are size explode array contains and sort array size explode are already implemented array contains and sort array are to be implemented Implement collection functions in SparkR
Yes,We want to support JSON serialization of vectors in order to support SPARK JSON serialization of Vectors
Yes, consolidate ExpressionEncoder tuple and Encoders tuple
Yes,RowEncoder doesn t support UserDefinedType now We should add the support for it. Database application is important to focus in this issue Add UserDefinedType support to RowEncoder
No, Show batch failures in the Streaming UI landing page
Yes,Random Forests have feature importance but GBT do not It would be great if we can add feature importance to GBT as well Perhaps the code in Random Forests can be refactored to apply to both types of ensembles See https issues apache org jira browse SPARK Feature Importance for GBT
Yes, Add Java API for trackStateByKey
Yes,If it returns Text we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF String without extra string decoding and encoding WholeTextFileRDD should return Text rather than String
Yes, Remove OpenHashSet for the old aggregate
Yes,We don t sufficiently test the path work well Remove the option to turn off unsafe and codegen
No,In spark sql when we create a table using the command as follwing create table tablename col char Hive will support for creating the table but when we desc the table desc tablename spark will report the error org apache spark sql types DataTypeException Unsupported dataType char If you have a struct and a field name of it has any special characters please use backticks to quote that field name e g x y Please note that backtick itself is not supported in a field name spark sql do not support for column datatype of CHAR
Yes,Implement Python API for bisecting k means Python API for bisecting k means
No,currently spark sql would not flush command history when exiting flush spark sql command line history to history file
Yes,We deprecated runs in Spark SPARK In we can either remove runs or make it no effect with warning messages So we can simplify the implementation I prefer the latter for better binary compatibility Make runs no effect in k means
Yes, Remove the internal implicit conversion from Expression to Column in functions scala
Yes, Remove implicit conversion from Expression to Column
Yes,DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame This has been fairly confusing to a few new contributors Since it doesn t buy us much we should just remove that implicit conversion Remove the internal implicit conversion from LogicalPlan to DataFrame
Yes,Umbrella ticket to walk through all newly introduced APIs to make sure they are consistent SQL API audit for Spark
No,The HP Fortify Opens Source Review team https www hpfod com open source review project reported a handful of potential resource leaks that were discovered using their static analysis tool We should fix the issues identified by their scan Fix potential socket file handle leaks identified via static analysis
Yes,Since we have bytes as number of records in the beginning of a page then the address can not be zero so we do not need the bitset Remove Bitset in BytesToBytesMap
Yes,We should remove methods for variance stddev skewness GroupedData should only keep common first order statistics
Yes,These two classes should be public since they are used in public code Make DataFrameHolder and DatasetHolder public
Yes, Add R API for stddev variance
Yes,Add Java friendly API for StreamingListener Add JavaStreamingListener
Yes,Since SPARK is resolved MapPartitionWithPrepare is not needed anymore Remove PrepareRDD
Yes,DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications Add a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY
Yes,In order to lay the groundwork for proper off heap memory support in SQL Tungsten we need to extend our MemoryManager to perform bookkeeping for off heap memory Add support for off heap memory to MemoryManager
Yes,We use scala collection mutable BitSet in BroadcastNestedLoopJoin now We should use Spark s BitSet Use Spark BitSet in BroadcastNestedLoopJoin
Yes,runs introduces extra complexity and overhead in MLlib s k means implementation I haven t seen much usage with runs not equal to We can deprecate this method in and remove or void it in It helps us simplify the implementation Deprecate runs in k means
No,When saving data to S e g saving to parquet if there is an error during the query execution the partial file generated by the failed task will be uploaded to S and the retries of this task will throw file already exist error It is very confusing to users because they may think that file already exist error is the error causing the job failure They can only find the real error in the spark ui in the stage page Provide more informative error message when direct parquet output committer is used and there is a file already exists error
Yes,Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made S does not support flushing of data data is written once the stream is actually closed In case of failure the data for the last minute default rolling interval will not be properly written Therefore we need a flag to close the stream after the write so that we achieve read after write consistency Flag to close Write Ahead Log after writing
No,There is a fairly old page in our docs that contains a bunch of assorted information regarding running Spark on Hadoop clusters I think this page should be removed and merged into other parts of the docs because the information is largely redundant and somewhat outdated http spark apache org docs latest hadoop third party distributions html There are three sections Compile time Hadoop version this information I think can be removed in favor of that on the building spark page These days most advanced users are building without bundling Hadoop so I m not sure giving them a bunch of different Hadoop versions sends the right message Linking against Hadoop this doesn t seem to add much beyond what is in the programming guide Where to run Spark redundant with the hardware provisioning guide Inheriting cluster configurations I think this would be better as a section at the end of the configuration page Remove Third Party Hadoop Distributions Doc Page
Yes,We should add text to DataFrameReader and DataFrameWriter Python API for text data source
Yes,This is the first cut implementation of trackStateByKey new improvement state management method in Spark Streaming See the epic jira for more details https issues apache org jira browse SPARK Implement trackStateByKey for improved state management
Yes, Mark all Stage ResultStage ShuffleMapStage internal state as private
Yes,Update the tachyon client dependency from to There are no new dependencies added or Spark facing APIs changed Upgrade Tachyon dependency to
Yes,As part of the work to implement SPARK it would be nice to have the network library efficiently stream data over a connection Currently all it has is the shuffle data protocol which is not very efficient for large files it requires the whole file to be buffered on the receiver side before the receiver can do anything For large files that comes at a huge cost in memory You can chunk large files but that requires the client to ask for each chunk separately Instead a similar approach but allowing the data to be processed as it arrives would be a lot more efficient and make it easier to implement the file server in the referenced bug Support streaming data using network library
Yes,Right now we stack a new URLClassLoader when a user add a jar through SQL s add jar command This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar For example In this case when we lookup class B we will not be able to find class A because Jar is the parent of Jar Use a single URLClassLoader for jars added through SQL s ADD JAR command
Yes,We use KCL in the current master KCL added integration with Kinesis Producer Library KPL and support auto de aggregation It would be great to upgrade KCL to the latest stable version Note that the latest version is and restored compatibility with dynamodb streams kinesis adapter which was broken in See https github com awslabs amazon kinesis client release notes tdas brkyvz Please recommend a version for upgrade Upgrade Kinesis Client Library to the latest stable version
Yes,Also SQLContext newSession Add getOrCreate for SparkContext SQLContext for Python
Yes,For a variety of reasons we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi Remove DeveloperApi annotation from private classes
Yes,In the spirit of SPARK we should clean up the use of SPARK HOME and if possible remove it entirely We need to look through what this is used for One use was allowing applications to run different versions of Spark in standalone mode For instance someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark This use case is not widely used and maybe should just be removed The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose If there are other legitimate reasons for SPARK HOME we can keep it around we need to audit the uses of it Clean up and clarify use of SPARK HOME
Yes,For some use cases it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts At here root SQLContext means the first SQLContext that gets created Introduce a mechanism to ban creating new root SQLContexts in a JVM
Yes,We should share the SQLTab across sessions SQLTab should be shared by across sessions
Yes,We should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel This will be a temp fix until we have proper evaluators defined for clustering Add computeCost to KMeansModel in spark ml
Yes,The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier Add computeCost and clusterCenters to KMeansModel in spark ml package
Yes,The new netty RPC still behaves too much like akka it requires both client e g an executor and server e g the driver to listen for incoming connections That is not necessary since sockets are full duplex and RPCs should be able to flow either way on any connection Also because the semantics of the netty based RPC don t exactly match akka you get weird issues like SPARK Supporting a client only mode also reduces the number of ports Spark apps need to use Netty based RPC env should support a client only mode
Yes,The serialize will be called by actualSize and append we should use UnsafeProjection before unrolling Avoid the serialization multiple times during unrolling of complex types
Yes,Matches more closely with ImperativeAggregate Rename ExpressionAggregate DeclarativeAggregate
Yes,typeId is not needed in columnar cache it s confusing to having them Remove typeId in columnar cache
Yes,There is support for message handler in Direct Kafka Stream which allows arbitrary T to be the output of the stream instead of Array Byte This is a very useful function therefore should exist in Kinesis as well Add MessageHandler to KinesisUtils createStream similar to Direct Kafka
Yes,Currently we try to support multiple sessions in SQL within a Spark Context but it s broken and not complete We should isolate these for each session current database of Hive SQLConf UDF UDAF UDTF temporary table For added jar and cached tables they should be accessible for all sessions Improve session management for SQL
Yes,SPARK uses network module to implement RPC However there are some configurations named with spark shuffle prefix in the network module We should refactor them and make sure the user can control them in shuffle and RPC separately Separate configs between shuffle and RPC
Yes,Refactoring Instance case class out from LOR and LIR and also cleaning up some code Refactoring Instance out from LOR and LIR and also cleaning up some code
Yes,The idea is that most of the logic of calling Python actually has nothing to do with RDD it is really just communicating with a socket there is nothing distributed about it and it is only currently depending on RDD because it was written this way If we extract that functionality out we can apply it to area of the code that doesn t depend on RDDs and also make it easier to test Refactor PythonRDD to decouple iterator computation from PythonRDD
Yes,The following method in LogisticRegressionModel is marked as private which prevents users from creating a summary on any given data set Check here https github com feynmanliang spark blob d fa c e f b a d cd mllib src main scala org apache spark ml regression LinearRegression scala L This method is definitely necessary to test model performance By the way the name evaluate is already pretty good for me mengxr Could you check this Thx Make Logistic Linear Regression Model evaluate method public
Yes,After SPARK we should add Python API for AFTSurvivalRegression Python API for AFTSurvivalRegression
Yes,Bagel has been deprecated and we haven t done any changes to it There is no need to run those tests Remove Bagel test suites
No,See SPARK The code was added but the docs were never updated Update documentation with instructions to enable block manager wire encryption
No,Saw many failures of this test recently e g https amplab cs berkeley edu jenkins view Spark QA Test job Spark Master SBT AMPLAB JENKINS BUILD PROFILE hadoop label spark test testReport junit org apache spark network sasl SaslIntegrationSuite testNoSaslClient Flaky test network sasl SaslIntegrationSuite testNoSaslClient
Yes,As of https issues apache org jira browse SPARK we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them We should remove the legacy log syncing code since this is a blocker to disabling Worker Master SSH on Jenkins Remove legacy SCP based Jenkins log archiving code
No,Job descriptions will help distinguish jobs of one batch from the other in the Jobs and Stages pages in the Spark UI Set meaningful job descriptions for streaming related jobs
Yes,Hive already supports this according to https issues apache org jira browse HIVE Currently Spark sql still supports only primitive types collect list and collect set should accept struct types as argument
Yes,The BlockMatrix multiply sends each block to all the corresponding columns of the right BlockMatrix even though there might not be any corresponding block to multiply with Some optimizations we can perform are Simulate the multiplication on the driver and figure out which blocks actually need to be shuffled Send the block once to a partition and join inside the partition rather than sending multiple copies to the same partition Decrease communication in BlockMatrix multiply and increase performance
Yes,The name weights becomes confusing as we are supporting weighted instanced As discussed in https github com apache spark pull we want to deprecate weights and use coefficients instead Deprecate but do not remove weights Only make changes under spark ml deprecate weights and use coefficients instead in ML models
Yes,Really simple request to upgrade fastutil to x The current version x has some minor API s in the Object xxOpenHashMap structures which is used in many places in Spark and has been marked deprecated Plus there is a conflict with another library we are using saddle http saddle github io which uses a newer version of fastutil I d be happy to send a PR I guess a bigger question is do you want to keep using fastutil SPARK but Spark uses more than just hashmaps so that probably requires another discussion Remove Fastutil
No,We should document options in public API doc Otherwise it is hard to find out the options without looking at the code Document LIBSVM data source options in public doc and minor improvements
Yes,We implemented dspr with sparse vector support in RowMatrix This method is also used in WeightedLeastSquares and other places It would be useful to move it to linalg BLAS move RowMatrix dspr to BLAS
Yes,SPARK need to generate random data which follow Weibull distribution Add WeibullGenerator for RandomDataGenerator
Yes,Currently There will be ConvertToSafe for PythonUDF that s not needed actually PythonUDF could process UnsafeRow
Yes,This was recently released and it has many improvements especially the following quote Python side IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes objects and members This makes Py J an ideal tool to explore complex Java APIs e g the Eclipse API Thanks to jonahkichwacoders quote Normally we wrap all the APIs in spark but for the ones that aren t this would make it easier to offroad by using the java proxy objects Upgrade pyspark to use py j
Yes,Currently the method join right DataFrame usingColumns Seq String only supports inner join It is more convenient to have it support other join types Support to specify join type when calling join with usingColumns
Yes,They don t bring much value since we now have better unit test coverage for hash joins This will also help reduce the test time Remove HashJoinCompatibilitySuite
Yes,Python s since is defined under pyspark sql It would be nice to move it under pyspark to be shared by all components Move since annotator to pyspark to be shared by all components
Yes,In ML pipelines each transformer estimator appends new columns to the input DataFrame For example it might produce DataFrames like the following columns a b c d where a is from raw input b udf b a c udf c b and d udf d c Some UDFs could be expensive However if we materialize c and d udf b and udf c are triggered twice i e value c is not re used It would be nice to detect this pattern and re use intermediate values Optimize sequential projections
No,Sometimes when we have dependency changes it can be pretty unclear what transitive set of things are changing If we enumerate all of the dependencies and put them in a source file in the repo we can make it so that it is very explicit what is changing Enumerate Spark s dependencies in a file and diff against it for new pull requests
No,improve ml guide replace ML Dataset by DataFrame to simplify the abstraction remove links to Scala API doc in the main guide change ML algorithms to pipeline components Improve Spark ML user guide
Yes,In codegen we didn t consider nullability of expressions Once considering this we can avoid lots of null check reduce the size of generated code also improve performance Before that we should double check the correctness of nullablity of all expressions and schema or we will hit NPE or wrong results Consider nullability of expression in codegen
No,Clean up user guides to address some minor comments in https github com apache spark pull https github com apache spark pull Some code examples were introduced in before createDataFrame We should switch to that Update user guide to address minor comments during code review
No, HiveComparision test should print out dependent tables
Yes,It is convenient to implement data source API for LIBSVM format to have a better integration with DataFrames and ML pipeline API This JIRA covers the following Read LIBSVM data as a DataFrame with two columns label Double and features Vector Accept numFeatures as an option The implementation should live under org apache spark ml source libsvm Implement SQL data source API for reading LIBSVM data
Yes,Add a column function on a DataFrame like ifelse in R to SparkR I guess we could implement it with a combination with when and otherwise h Example If df x is TRUE then return otherwise return Add ifelse Column function to SparkR
Yes,Right now we have QualifiedTableName TableIdentifier and Seq String to represent table identifiers We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name database name return unquoted string and return quoted string There will be TODOs having SPARK in it Those places need to be updated Consolidate different forms of table identifiers
Yes,ML Evaluator currently requires that metrics be maximized bigger is better That is counterintuitive for some metrics Currently we hackily negate some metrics in RegressionEvaluator which is weird Instead we should Return the metric as expected e g rmse should return RMSE not its negation Provide an indicator of whether the metric should be maximized or minimized Model selection algorithms can use the indicator as needed ML Evaluator should indicate if metric should be maximized or minimized
Yes,make MultilayerPerceptronClassifier layers and weights public make MultilayerPerceptronClassifier layers and weights public
Yes,currently in SparkR collect on a DataFrame collects the data within the DataFrame into a local data frame R users are used to using data frame However collect currently can t collect data of nested types from a DataFrame because The serializer in JVM backend does not support nested types collect in R side assumes each column is of simple atomic type that can be combinded into a atomic vector Improve the implementation of collect on DataFrame in SparkR
Yes,The stat functions are defined in http spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions Currently only crosstab is supported Functions to be supported include corr cov freqItems Add support for DataFrameStatFunctions in SparkR
Yes,Add Python API for mllib fpm PrefixSpan Add Python API for PrefixSpan
No, prepare architecture documentation
No,HADOOP should be backported to branch so it can also benefit from whitespace trimming in configuration Backport HADOOP to branch
Yes,This JIRA is to define commands for Hadoop token The scope of this task is highlighted as following Token init authenticate and request an identity token then persist the token in token cache for later reuse Token display show the existing token with its info and attributes in the token cache Token revoke revoke a token so that the token will no longer be valid and cannot be used later Token renew extend the lifecycle of a token before it s expired Hadoop Token Command
Yes,Once JarFinder getJar is invoked by a client app it would be really useful to destroy the generated JAR after the JVM is destroyed by setting tempJar deleteOnExit In order to preserve backwards compatibility a configuration setting could be implemented e g test build dir purge on exit JarFinder getJar should delete the jar file upon destruction of the JVM
No,hadoop common project hadoop common src main docs releasenotes html shows up as modified even though I haven t touched it and I can t check it out or reset to a previous version to make that go away The only thing I can do to neutralize it is to put it in a dummy commit but I have to do this every time I switch branches or rebase This appears to have began after the release notes commit c bb b dc c cd dd a and must be due to a line endings change use svn eol style native for html to prevent line ending issues
No,On our system it s not uncommon to get MB of logs with each MapReduce job It would be very helpful if it were possible to configure Hadoop daemons to write logs only when major things happen but the only conf options I could find are for increasing the amount of output The disk is really a bottleneck for us and I believe that short jobs would run much more quickly with less disk usage We also believe that the high disk usage might be triggering a kernel bug on some of our machines causing them to crash If the MB of logs went down to KB we would probably still have all of the information we needed Thanks huge log files
No,Add documentation to BUILDING txt describing dependencies and instructions for building on Windows add instructions to BUILDING txt describing how to build on Windows
Yes,Currently different IPC servers in Hadoop use the same config variables names starting with ipc server This makes it difficult and confusing to maintain configuration for different IPC servers Support per server IPC configuration
Yes,The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided I have defined multiple local disks defined for a datanode dfs data dir data dfs dn data dfs dn data dfs dn data dfs dn data dfs dn data dfs dn true When one of those disks breaks and is unmounted then the mountpoint such as data in this example becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting When this situation happens the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed The only way around this is to alter the configuration and omit that specific disk configuration To my opinion It would be more practical to let Hadoop daemons start when at least disks partition in the provided list is in a usable state This prevents having to roll out custom configurations for systems which have temporarily a disk and therefor directory layout missing This might also be configurable that at least X partitions out of he available ones are in OK state Allow daemon startup when at least or configurable disk is in an OK state
Yes,This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous e g reject file c Windows Valid file URI syntax explained at http blogs msdn com b ie archive file uris in windows aspx Also see https issues apache org jira browse HADOOP Reject invalid Windows URIs
Yes,Quantcast has released QFS http quantcast github com qfs a C distributed filesystem based on Kosmos File System KFS QFS comes with various feature performance and stability improvements over KFS A hadoop fs shim needs be added to support QFS through qfs URIs Need to add fs shim to use QFS
Yes,Currently FileUtil unTar spawns tar utility to do the work Tar may not be present on all platforms by default eg Windows So changing this to use JAVA API s would help make it more cross platform FileUtil unZip uses the same approach Change untar to use Java API on Windows instead of spawning tar process
No,Change website to reflect new user hadoop apache org mailing list since we ve merged the user lists per discussion on general http s apache org hv Change website to reflect new user hadoop apache org mailing list
No,Most of the code changes to make Hadoop branch work natively on Windows are done This jira in intended to track the work needed to achieve test pass for the dev tests Stabilize branch win
Yes,Because of reasons listed here http findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE comparators should be serializable To make deserialization work it is required that all superclasses have no arg constructor http findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR Simply add no arg constructor to WritableComparator WritableComparator must implement no arg constructor
Yes,As of now hadoop streaming uses old Hadoop M R API This JIRA ports it to the new M R API Port StreamInputFormat to new Map Reduce API
Yes,This is to track changes for restoring security in branch Restore security in Hadoop branch
Yes,HADOOP introduces a configure flag to prevent potential status inconsistency between zkfc and namenode by making auto and manual failover mutually exclusive However as described in section of design doc at HDFS we should allow manual and auto failover co exist by adding some rpc interfaces at zkfc manual failover shall be triggered by haadmin and handled by zkfc if auto failover is enabled Auto HA Allow manual failover to be invoked from zkfc
Yes,To keep the initial patches manageable kerberos security is not currently supported in the ZKFC implementation This JIRA is to support the following important pieces for security integrate with ZK authentication kerberos or password based allow the user to configure ACLs for the relevant znodes add keytab configuration and login to the ZKFC daemons ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons Security support for ZK Failover controller
Yes,Currently when the ZK session expires it results in a fatal error being sent to the application callback This is not the best behavior for example in the case of HA if ZK goes down we would like the current state to be maintained rather than causing either NN to abort When the ZK clients are able to reconnect they should sort out the correct leader based on the normal locking schemes Improve ActiveStandbyElector s behavior when session expires
Yes,If there are known failures test patch will bail out as soon as it sees them This causes the precommit builds to potentially not find real issues with a patch because the tests that would fail might come after a known failure We should add fn to just the mvn test command in test patch to get the full list of failures test patch should run tests with fn to avoid masking test failures
Yes,One thing that the original patch for HADOOP didn t address is the need for those curated jars to be visible in the final tarball make hadoop client set of curated jars available in a distribution tarball
Yes,The Syncable sync was deprecated in We should remove it Remove the deprecated Syncable sync method
Yes, Access Control support for Non secure deployment of Hadoop on Windows
Yes,We should be able to start a kdc server for unit tests so that security could be turned on This will greatly improve the coverage of unit tests Add capability to turn on security in unit tests
Yes,Given that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves unless a tool that manages for you is used it would be good to also validate the provided config XML site xml files with a tool like xmllint or maybe Xerces somehow when running a command or at least when starting up daemons We should use this only if a relevant tool is available and optionally be silent if the env requests Validate XMLs if a relevant tool is available when using scripts
Yes,HDFS added a new public API SequenceFile syncFs we need to forward port this for compatibility Looks like it might have introduced other APIs that need forward porting as well eg LocaltedBlocks setFileLength and DataNode getBlockInfo Forward port SequenceFile syncFs and friends from Hadoop x
Yes,Currently the check done in the hasSufficientTimeElapsed method is hardcoded to mins wait The wait time should be driven by configuration and its default value for clients should be min Kerberos relogin interval in UserGroupInformation should be configurable
No,The docs version is hard coded It should be updated automatically Automatically update doc versions
Yes, RPC Layer improvements to support protocol compatibility
Yes, create a script to setup application in order to create root directories for application such hbase hcat hive etc
Yes,See this thread http markmail org thread cxtz i lvztfgfxn We need to get things up and running for a top level hadoop tools module DistCpV will be the first resident of this new home Things we need The module itself and a top level pom with appropriate dependencies Integration with the patch builds for the new module Integration with the post commit and nightly builds for the new module Set things up for a top level hadoop tools module
Yes,hadoop common src main bin hadoop config sh needs to be updated post mavenization eg it still refers to build classes etc hadoop config sh needs to be updated post mavenization
Yes,hadoop common src main bin hadoop config sh needs to be updated post MR eg the layout of mapred home has changed hadoop config sh needs to be updated post MR
No,The Hudson pre commit tests are presently only capable of testing a patch against trunk It d be nice if this could be extended to automatically run against the correct branch Make pre commit checks run against the correct branch
Yes,In a nutshell ls needs the ability to list a directory but not its contents W o d it is impossible to list the root directory s owner permissions etc See the original hdfs bug for details hadoop dfs ls Do not expand directories was HDFS
Yes,In HADOOP and HDFS it was agreed that FileSystem listStatus should throw FileNotFoundException instead of returning null when the target directory did not exist However in LocalFileSystem implementation today FileSystem listStatus still may return null when the target directory exists but does not grant read permission This causes NPE in many callers for all the reasons cited in HADOOP and HDFS See HADOOP and its linked issues for examples FileSystem listStatus should throw IOE upon access error
Yes, IPC Wire Compatibility
Yes,When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump the client currently just gets a non useful error message like EOFException Instead the IPC server code can speak just enough of prior IPC protocols to send back a fatal message indicating the version mismatch Send back nicer error to clients using outdated IPC version
Yes,When you have a key value class that s non Writable and you forget to attach io serializers for the same an NPE is thrown by the tasks with no information on why or what s missing and what led to it I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones When a serializer class is missing return null not throw an NPE
Yes,When setting up a compression codec in an MR job the full class name of the codec must be used To ease usability compression codecs should be resolved by their codec name ie gzip deflate zlib bzip instead their full codec class name Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name it could simplify how HBase resolves loads the codecs Add capability to resolve compression codec based on codec name
Yes,We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson The script would execute the following and take just the password as an argument Create a test patch script for Hudson
Yes, Deprecate metrics v
Yes,The FsShell has many chains if then else chains for instantiating and running commands A dynamic mechanism is needed for registering commands such that FsShell requires no changes when adding new commands Add command factory to FsShell
Yes,Our project Pig exposes FsShell functionality to our end users through a shell command We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign for instance removing a non existent directory We have asks related to this issue Meaningful error code returned from FsShell we use java class so that we can take different actions on different errors Unix like ways to tell the command to ignore certain behavior Here are the commands that we would like to be expanded implemented rm f rmdir ignore fail on non empty mkdir p Extensions to FsShell
Yes,DaemonFactory class is defined in hdfs util common would be a better place for this class DaemonFactory should be moved from HDFS to common
No, Fix hadoop patch testing using jira cli tool
Yes,New file system API HADOOP should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations MAPREDUCE security implementation for new FileSystem FileContext API
No,A number of subprojects have left Hadoop yet the website s not been fully updated to reflect that Update website for recent subproject departures
Yes,We should allow users to use the more compact form of xml elements For example we could allow The old format would also be supported Allow compact property description in xml
Yes,While working HADOOP I noticed that our metrics naming style is all over the place Capitalized camel case e g FilesCreated in namenode metrics and some rpc metrics uncapitalized camel case e g threadsBlocked in jvm metrics and some rpc metrics lowercased underscored e g bytes written in datanode metrics and mapreduce metrics Let s make them consistent How about uncapitalized camel case My main reason for the camel case some backends have limits on the name length and underscore is wasteful Once we have a consistent naming style we can do Metric Number of INodes created MutableCounterLong filesCreated instead of the more redundant Metric FilesCreated Number of INodes created MutableCounterLong filesCreated Make metrics naming consistent
Yes,As Hadoop widespreads and matures the number of tools and utilities for users keeps growing Some of them are bundled with Hadoop core some with Hadoop contrib some on their own some are full fledged servers on their own For example just to name a few distcp streaming pipes har pig hive oozie Today there is no standard mechanism for making these tools available to users Neither there is a standard mechanism for these tools to integrate and distributed them with each other The lack of a common foundation creates issues for developers and users Common foundation for Hadoop client tools
Yes,Whenever a new patch is submitted for verification test patch process has to make sure that none of Herriot bindings were broken test patch needs to verify Herriot integrity
No, Create Wiki document about Herriot Test Framework and test development guide
No,Add documentation on our interface classification scheme to thew common site Update Hadoop Common Sites
Yes,Per discussions with Arun Chris Hong and Rajiv et al we concluded that the current metrics framework needs an overhaul to Allow multiple plugins for different monitoring systems simultaneously see also HADOOP Refresh metrics plugin config without server restart Including filtering of metrics per plugin Support metrics schema for plugins The jira will be resolved when core hadoop components hdfs mapreduce are updated to use the new framework Updates to external components that use the existing metrics framework will be tracked by different issues The current design wiki http wiki apache org hadoop HADOOP MetricsV Overhaul metrics framework
No,We ve created a new logo for Hadoop security topics that we wanted to share with the community Create a new logo for Hadoop security related uses
No,Currently The Hadoop command guide http hadoop apache org common docs r commands manual html just lists all the available command line options with a description It should include examples for each command for more clarity Hadoop commands guide should include examples
No,A user document should be added for secure impersonation feature The document should cover a code example about how UserGroupInformation doAs should be used for secure impersonation User document for UserGroupInformation doAs
Yes,Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on The versions of these libraries are also present in ivy libraries properties so that when a library is updated it must be updated in two places which is error prone We should instead only specify library versions in a single place versions of dependencies should be specified in a single place
No,The current documentation for rack awareness http hadoop apache org common docs r cluster setup html Hadoop Rack Awareness should be augmented to include a sample script Improve documentation for rack awareness
Yes,Now that we are adding the serialized form of delegation tokens into the http interfaces we should include some version information Should add version to the serialization of DelegationToken
Yes,When token is used for authentication over RPC information other than username may be needed for access authorization This information is typically specified in TokenIdentifier This is especially true for block tokens used for client to datanode accesses where authorization is based on access permissions specified in TokenIdentifier and not on username Block tokens used to be called access tokens and one can think of them as capability tokens See HADOOP for more info Add authenticated TokenIdentifiers to UGI so that they can be used for authorization
Yes,The UserGroupInformation should contain authentication method in its subject This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients UGI should contain authentication method
Yes,We need a configurable mapping from full user names eg omalley APACHE ORG to local user names eg omalley For many organizations it is sufficient to just use the prefix however in the case of shared clusters there may be duplicated prefixes A configurable mapping will let administrators resolve the issue Need mapping from long principal names to local OS user names
Yes,The way metrics are currently exposed to the JMX in the NameNode is not helpful since only the current counters in the record can be fetched and without any context those number mean little For example the number of files created equal to only means that in the last period there were files created but when the new period will end is unknown so fetching again will either mean another files or we are fetching the same time period One of the solutions for this problem will be to have a JMX context that will accumulate the data being child class of AbstractMetricsContext and expose different records to the JMX through custom MBeans This way the information fetched from the JMX will represent the state of things in a more meaningful way JMX Context for Metrics
Yes,Common tests are functional tests or end to end It makes sense to have Mockito framework for the convenience of true unit tests development Add unit tests framework Mockito
Yes,FileSystem should have mkdir and create file apis which do not create parent path The usecase is illustrated at this https issues apache org jira browse HDFS focusedCommentId page com atlassian jira plugin system issuetabpanels Acomment tabpanel action FileSystem should have mkdir and create file apis which do not create parent path
Yes,Configuration objects send a DEBUG level log message every time they re instantiated which include a full stack trace This is more appropriate for TRACE level logging as it renders other debug logs very hard to read Configuration sends too much data to log j
Yes,For HDFS we need to use unix domain sockets This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android apache license Add support for unix domain sockets to JNI libs
Yes,There s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson test patch script has to verify if the version of FindBugs is correct test patch should verify that FindBugs version used for verification is correct one
No,New Hadoop Common Site Set up site initial pass May need to add more content May need to update some links New Hadoop Common Site
No,The runtime of test patch has increased to min test patch takes min
Yes,Currently with quota turned on user cannot call rmr on large directory that causes over quota Besides from error message being unfriendly how should this be handled Handling of Trash with quota
Yes,We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances In particular we often end up with calls to rpcs being wrapped with retry loops for timeouts We should be able to make a retrying proxy that will call the rpc and retry in some circumstances we need some rpc retry framework
Yes,Adding an option to FsShell stat to get a file s block location information will be very useful we can print the block location information in this format blockID XXXXX byte range YYYY ZZZZ location dn dn blockID XXXXX byte range YYYY ZZZZ location dn dn Add a command to FsShell stat to get a file s block location information
Yes,The following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP Some of them are not related to hdfs e g TestFTPFileSystem These files should be moved out from hdfs and should not use hdfs codes Some of them are testing hdfs features e g TestStickyBit They should be defined under org apache hadoop hdfs package fs tests should not be placed in hdfs
No,On the Hadoop Core site page http hadoop apache org core add Chukwa to the list of Related Projects in the menu bar Related JIRA https issues apache org jira browse CHUKWA Hadoop Core Site add Chukwa as related project in menu
No,Add a link to training videos from the getting started section like Pig see http hadoop apache org pig Add link to training from website
Yes,In an effort to simplify the in the capacity scheduler We would reintroduce this possibly with some revisions to the original design after a while This will be an incompatible change Any objections Remove pre emption from the capacity scheduler code base
No, Add link to distributions wiki page from releases page
No,Git doesn t save empty directories so it would be nice to make a placeholder touch file that would cause git to create the directory builds from git checks fail because of src contrib chukwa opt not being there
No,Since ant javadoc does not generate all core javadocs some javadocs e g HDFS javadocs are not checked by Hudson Not all core javadoc are checked by Hudson
Yes,A specific sub case of the general priority inversion problem noted in HADOOP is when many lower priority jobs are submitted and are waiting for mappers to free up Even though they haven t actually done any work they will be assigned any free reducers If a higher priority job is submitted priority inversion results not just due to the lower priority tasks that are in the midst of completing but also due to the ones that haven t yet started but have claimed all the free reducers A simple workaround is to require a job to complete some useful work before assigning it a reducer This can be done in a tunable and backwards compatible manner by adding a minimum map progress percentage before assigning a reducer option to the JobConf Setting this to would eliminate the common case above and setting it to would technically eliminate the inversion of HADOOP though likely at an unacceptably high cost JobConf option for minimum progress threshold before reducers are assigned
No,It would be great to collect analytics about the visitors to the website and to do so we need to create a privacy policy that tells visitors what we will collect Create a privacy policy for the Hadoop website
Yes,to increase productivity in our current project which makes a heavy use of Hadoop we wrote a small Eclipse based GUI application which basically consists in views a HDFS explorer adapted from Eclipse filesystem explorer example For now it includes the following features o classical tree based browsing interface with directory content being detailed in a columns table file name file size file type o refresh button o delete file or directory with confirm dialog select files in the tree or table and click the Delete button o rename file or directory simple click on the file in the table type the new name and validate o open file with system editor select the file in the table and click Open button works on Windows not on Linux o internal drag drop o external drag drop from the local filesystem to the HDFS the opposite doesn t work a MapReduce very simple job launcher o select the job XML configuration file o run the job o kill the job o visualize map and reduce progress with progress bars o open a browser on the Hadoop job tracker web interface INSTALLATION NOTES Eclipse JDK import the archive in Eclipse copy your hadoop conf file hadoop default xml in src folder this step should be moved in the GUI later right click on the project and Run As Eclipse Application enjoy Eclipse based GUI DFS explorer and basic Map Reduce job launcher
Yes,already has quotas for HDFS namespace HADOOP HADOOP implements similar quotas for disk space on HDFS in This jira proposes to port HADOOP to Port HDFS space quotas to
Yes,Watchdog is watching for ChukwaAgent only once every minutes so there s no point in retrying more than once every mins In practice if the watchdog is not able to automatically restart the agent it will take more than minutes to get Ops to restart it Also Ops want us to limit the number of communications between Hadoop and Chukwa that s why minutes ChukwaAgent controller should retry to register for a longer period but not as frequent as now
Yes, to optimize hudsonBuildHadoopNightly sh script
Yes, Split build script for building core hdfs and mapred separately
Yes,When data need to be reprocessed in the database there is currently no manual method to reload the chukwa sequence files into database A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this Add utilities to load chukwa sequence file to database
Yes,It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime This would allow us to stop relying on exec ing bash to get access to information such as user groups process limits etc and for features such as chown chgrp org apache hadoop util Shell Implement a native OS runtime for Hadoop
Yes,Amongst other things JUnit has better support for class wide set up and tear down via BeforeClass and AfterClass annotations and more flexible assertions http junit sourceforge net doc ReleaseNotes html It would be nice to be able to take advantage of these features in tests we write JUnit can run tests written for JUnit without any changes Upgrade to JUnit
Yes,In the FileTailingAdaptor if when trying to read a file a File does not exist Permission denied exception is throws then that file should be log and removed When the FileTailingAdaptor is unable to read a file it should take action instead of trying times
Yes, Add Hadoop native library to java library path compression
No,Configuration directories can be specified by either setting HADOOP CONF DIR or using the config command line option However the hadoop daemon sh script will not start the daemons unless the PID directory is separate for each configuration The issue is that the code for generating PID filenames is not dependent on the configuration directory While the PID directory can be changed in hadoop env sh it seems a little unnecessary to have this restriction Startup scripts will not start instances of Hadoop daemons w different configs w o setting separate PID directories
Yes,Ability to test endToEnd chukwa pipeline From log file to dataSink From dataSink to demux output Chukwa test framework
Yes,An external application an Ops script or some CLI based tool can change the configuration of the Capacity Scheduler change the capacities of various queues for example by updating its config file This application then needs to tell the Capacity Scheduler that its config has changed which causes the Scheduler to re read its configuration It s possible that the Capacity Scheduler may need to interact with external applications in other similar ways Capacity Scheduler needs to re read its configuration
Yes,Create some utility classes to dump both Archive and ChukwaRecords files Utility classes for Archive and ChukwaRecord files
Yes,In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output avoid immediate merging if there s already file for the same time range create a spill file instead merge all raw files every hours merge all hourly files every days Rolling mechanism for demux output
Yes,Update Chukwa parsers Update Chukwa parsers
Yes,simplify Parsers implementation add map and reduce side to the demux add dynamic link between RecordType and Parsers using configuration file and alias encapsulate data files creation location and naming convention inside the core demux classes sort all data by TimePartition Machine Timestamp by default Update Chukwa Demux process
Yes,Problem We need to have Object Serialization Deserialization support for TFile ObjectFile on top of TFile
Yes,Besides the default JT scheduling algorithm there is work going on with at least two more schedulers HADOOP HADOOP HADOOP makes it easier to plug in new schedulers into the JT Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment and easy for developers to add in more schedulers into the framework without inundating it Hadoop Core should support source filesfor multiple schedulers
No,It would be good to have a test case for hadoop metrics We could use FileContext or derive something out of NullContext to check the values returned via metrics are correct Create tests for Hadoop metrics
Yes,SequenceFile s block compression format is too complex and requires codecs to compress or decompress It would be good to have a file format that only needs New binary file format
Yes,Should we change the hash function for Text to something that handles non ascii characters better http bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup Replace Text hashcode with a better hash function for non ascii strings
Yes,Utility will collapse the contents of a directory into a small number of files compaction utility for directories
Yes,Currently only way to find files with all replica being corrupt is when we read those files Instead can we have fsck report those Using the corrupted blocks found by the periodic verification fsck to show checksum corrupted files
No,HADOOP added a new logo for Core to the released documenation The same logo should also be used on the Core project web site add new logo to project site
Yes,Checkpoint to verify the fsimage each time it creates the new one Keep two generations of fsimage
Yes,this is something that we have implemented in the application layer may be useful to have in hadoop itself long term log storage systems often keep data sorted by some sort key future computations on such files can often benefit from this sort order if the job requires grouping by the sort key then it should be possible to do reduction in the map stage itself this is not natively supported by hadoop except in the degenerate case of map file per task since splits can span the sort key however aligning the data read by the map task to sort key boundaries is straightforward and this would be a useful capability to have in hadoop the definition of the sort key should be left up to the application it s not necessarily the key field in a Sequencefile through a generic interface but otherwise the sequencefile and text file readers can use the extracted sort key to align map task data with key boundaries align map splits on sorted files with key boundaries
No,I think we should add a community section to the TLP website that deep links to the corresponding pages I ll attach the generated pdf of the site TLP site should have a community section
Yes,This jira is intended to enhance IPC s scalability and robustness Currently an IPC server can easily hung due to a disk failure or garbage collection during which it cannot respond to the clients promptly This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously the server may be swarmed by requests and cannot work responsively The proposed changes aim to provide a better client server coordination Server should be able to throttle client during burst of requests A slow client should not affect server from serving other clients A temporary hanging server should not cause catastrophic failures to clients Client server should detect remote side failures Examples of failures include the remote host is crashed the remote host is crashed and then rebooted the remote process is crashed or shut down by an operator Fairness Each client should be able to make progress Improve the Scalability and Robustness of IPC
No,Currently the PoweredBy page is hard to find I think we should add link from the left navigation bar We should add a link to the PoweredBy page from the left navigation bar
Yes,Move hbase out of hadoop core Move its JIRA issues and move it in svn from https svn apache org repos asf hadoop core trunk src contrib hbase to https svn apache org repos asf hadoop hbase trunk Move hbase out of hadoop core
No,I did some tests on the throughput of scanning block compressed sequence files The sustained throughput was bounded at MB sec per process with the cpu of each process maxed at It seems to me that the cpu consumption is too high and the throughput is too low for just scanning files Reading sequence file consumes cpu with maximum throughput being about MB sec per process
No,The Hadoop website should link to the following pages http www apache org foundation thanks html http www apache org foundation sponsorship html This should be done in the boilerplate of the site so that every page links to these pages This is a requirement of all ASF sites website should link to ASF sponsor page
Yes,Currently when a block is transfered to a data node the client interleaves data chunks with the respective checksums This requires creating an extra copy of the original data in a new buffer interleaved with the crcs We can avoid extra copying if the data and the crc are fed to the socket one after another Non interleaved checksums would optimize block transfers
Yes,A user was moving from to and was invoking randomwriter with a config on the command line like bin hadoop jar hadoop examples jar randomwriter output conf xml which worked in but in it ignores the conf xml without complaining The equivalent is bin hadoop jar hadoop examples jar randomwriter conf conf xml output randomwriter should complain if there are too many arguments
Yes,The metrics system in the JobTracker is defaulting to every seconds computing all of the counters for all of the jobs This work is a substantial amount of work showing up as running in of the snapshots that I ve seen I d like to lower the default interval to once every seconds and make it a low priority thread the metrics system in the job tracker is running too often
No,With significant changes to hadoop s configuration since HADOOP the documentation for it needs to be completely overhauled a Exhaustive and accurate javadocs including some specific examples b Update the wiki http wiki apache org lucene hadoop HowToConfigure to c Importantly Put up a page describing hadoop s configuration on the hadoop website via forrest Any thing else folks can think of Update documentation for hadoop s configuration post HADOOP
Yes,Currently max queue size for IPC server is set to handlers Usually when RPC failures are observed e g HADOOP we increase number of handlers and the problem goes away I think a big part of such a fix is increase in max queue size I think we should make maxQsize per handler configurable with a bigger default than There are other improvements also HADOOP Server keeps reading RPC requests from clients When the number in flight RPCs is larger than maxQsize the earliest RPCs are deleted This is the main feedback Server has for the client I have often heard from users that Hadoop doesn t handle bursty traffic Say handler count is default and Server can handle RPCs a sec quite conservative low for a typical server it implies that an RPC can wait for only for sec before it is dropped If there clients and all of them send RPCs around the same time not very rare with heartbeats etc will be dropped In stead of dropping the earliest RPCs if the server delays reading new RPCs the feedback to clients would be much smoother I will file another jira regd queue management For this jira I propose to make queue size per handler configurable with a larger default may be IPC server max queue size should be configurable
Yes,Hudson should kill long running tests I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up It would be nice if when the timer goes off Hudson did a See the section Killing a hung test at http wiki apache org lucene hadoop HudsonBuildServer Hudson should kill long running tests
Yes,Unlike gzip the bzip file format supports splitting Compression is by blocks k by default and blocks are separated by a synchronization marker a bit approximation of Pi This would permit very large compressed files to be split into multiple map tasks which is not currently possible unless using a Hadoop specific file format want InputFormat for bzip files
No,Following a user discussion mapred system dir parameter needs documentation
Yes,It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings It could then be hooked up to bin hadoop fs cat and the web ui to textify sequence and compressed files Create a utility to convert binary sequence and compressed files to strings
No,I m looking at the alpha website and can t find a link to the Aliyun OSS documentation Under the Hadoop Compatible File Systems header there are links to S Azure blob ADLS and Swift but not Aliyun OSS Aliyun OSS documentation missing from website
No,We are using Hadoop delegation token authentication functionality in Apache Solr As part of the integration testing I found following issue with the delegation token cancelation functionality Consider a setup with Solr servers S and S which are configured to use delegation token functionality backed by Zookeeper Now invoke following steps Step Send a request to S to create a delegation token Delegation token DT is created successfully Step Send a request to cancel DT to S DT is canceled successfully client receives HTTP response Step Send a request to cancel DT to S again DT cancelation fails client receives HTTP response Step Send a request to cancel DT to S At this point we get two different responses DT cancelation fails client receives HTTP response DT cancelation succeeds client receives HTTP response Also as per the current implementation each server maintains an in memory cache of current tokens which is updated using the ZK watch mechanism e g the ZK watch on S will ensure that the in memory cache is synchronized after step After investigation I found the root cause for this behavior is due to the race condition between step and the firing of ZK watch on S Whenever the watch fires before the step we get HTTP response as expected When that is not the case we get HTTP response along with following ERROR message in the log From client perspective the server should return HTTP error when the cancel request is sent out for an invalid token Ref Here is the relevant Solr unit test for reference https github com apache lucene solr blob cdb ce ed ed b d ab c solr core src test org apache solr cloud TestSolrCloudWithDelegationTokens java L Synchronization issue in delegation token cancel functionality
No,The issue was found after HADOOP by Hadoop HDFS test TestAclsEndToEnd Sorry both Jenkins and I was not able to catch it HADOOP fixed the issue for KMSClientProvider secure proxy user token use case But the non secure proxy user case should not be affected by the new logic The ticket is open to fix it Fix KMSClientProvider for non secure proxyuser use case
Yes,is missing in pom xml That way mvn versions set does not work for the project Missing hadoop cloud storage project module in pom xml
Yes,Doing some Tomcat performance tuning on a loaded cluster we found that acceptCount acceptorThreadCount and protocol can be useful Let s make these configurable in the kms startup script Since the KMS is Jetty in x this is targeted at just branch Make additional KMS tomcat settings configurable
No,Found some build issues while doing some test runs with the create release sh script Fix some release build issues
Yes,Other people aren t seeing this yet but unless you explicitly exclude v of commons lang from the azure build which HADOOP does then the dependency declaration of commons lang v is creating a resolution conflict That s a dependency only needed for the local dynamodb tests I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one excluding that you get for free It doesn t impact anything shipped in production but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common explicitly declare the commons lang dependency as
No,Let s update the website release notes for alpha s changes Update project release notes for alpha
Yes,the share hadoop component template directories are from when RPM and such were built as part of the build system that no longer happens and now those files cause more harm than good since they are in the classpath let s remove them Remove vestigal templates directories creation
Yes,ADLS has multiple upgrades since the version we are using and Change list https github com Azure azure data lake store java blob master CHANGES md Update ADLS SDK to
No,mvn install fails for me on trunk on a new environment with the following This works on an existing dev setup likely because I have the jar in my m cache Fix compilation failure from missing hadoop kms test jar
Yes,Read ADLS credentials using Hadoop CredentialProvider API See https hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html Read ADLS credentials from Credential Provider
Yes,The FTP transfer mode used by FTPFileSystem is BLOCK TRANSFER MODE FTP Data connection mode used by FTPFileSystem is ACTIVE LOCAL DATA CONNECTION MODE This jira makes them configurable Make FTPFileSystem s data connection mode and transfer mode configurable
No,From HADOOP building javadoc of hadoop azure module failed Though these error is not related to the patch of HADOOP directly it can be fixed anyway Building whole project fails with JDK ref https builds apache org job PreCommit HADOOP Build artifact patchprocess patch javadoc hadoop tools hadoop azure txt Build failure due to errors of javadoc build in hadoop azure
No,The java version used by hadoop is controlled by JAVA HOME variable defined in hadoop env sh We log this information when HDFS starts in the log file This just means we are printing out the java version in the current shell path This jira proposes adding a new simple command or an extension to existing hadoop version command where the current java version used by hadoop is also printed out This avoids customer confusion when they are looking at if the java stack is properly configured For example checking if JCE is installed correctly This is a very minor change that can be done by modifying hdfs cmd or hdfs shell script in the bin directory Thanks to sujit for bringing this to my attention A command to print JAVA VERSION used by Hadoop HDFS
Yes,Currently we have one command to get state of namenode It will be good to have command which will give state of all the namenodes Add haadmin getAllServiceState option to get the HA state of all the NameNodes ResourceManagers
No,nformat Tests run Failures Errors Skipped Time elapsed sec FAILURE in org apache hadoop fs adl live TestAdlFileContextMainOperationsLive testGetFileContext org apache hadoop fs adl live TestAdlFileContextMainOperationsLive Time elapsed sec ERROR java lang RuntimeException java lang reflect InvocationTargetException at org apache hadoop fs AbstractFileSystem newInstance AbstractFileSystem java at org apache hadoop fs AbstractFileSystem createFileSystem AbstractFileSystem java at org apache hadoop fs AbstractFileSystem get AbstractFileSystem java at org apache hadoop fs FileContext run FileContext java at org apache hadoop fs FileContext run FileContext java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop fs FileContext getAbstractFileSystem FileContext java at org apache hadoop fs FileContext getFSofPath FileContext java at org apache hadoop fs FSLinkResolver resolve FSLinkResolver java at org apache hadoop fs FileContext create FileContext java at org apache hadoop fs FileContextMainOperationsBaseTest testGetFileContext FileContextMainOperationsBaseTest java at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java at java lang reflect Method invoke Method java at org junit runners model FrameworkMethod runReflectiveCall FrameworkMethod java at org junit internal runners model ReflectiveCallable run ReflectiveCallable java at org junit runners model FrameworkMethod invokeExplosively FrameworkMethod java at org junit internal runners statements InvokeMethod evaluate InvokeMethod java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit internal runners statements RunAfters evaluate RunAfters java at org junit runners ParentRunner runLeaf ParentRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners ParentRunner run ParentRunner java at org junit runners ParentRunner schedule ParentRunner java at org junit runners ParentRunner runChildren ParentRunner java at org junit runners ParentRunner access ParentRunner java at org junit runners ParentRunner evaluate ParentRunner java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit runners ParentRunner run ParentRunner java at org apache maven surefire junit JUnit Provider execute JUnit Provider java at org apache maven surefire junit JUnit Provider executeTestSet JUnit Provider java at org apache maven surefire junit JUnit Provider invoke JUnit Provider java at org apache maven surefire booter ForkedBooter invokeProviderInSameClassLoader ForkedBooter java at org apache maven surefire booter ForkedBooter runSuitesInProcess ForkedBooter java at org apache maven surefire booter ForkedBooter main ForkedBooter java Caused by java lang reflect InvocationTargetException null at sun reflect GeneratedConstructorAccessor newInstance Unknown Source at sun reflect DelegatingConstructorAccessorImpl newInstance DelegatingConstructorAccessorImpl java at java lang reflect Constructor newInstance Constructor java at org apache hadoop fs AbstractFileSystem newInstance AbstractFileSystem java at org apache hadoop fs AbstractFileSystem createFileSystem AbstractFileSystem java at org apache hadoop fs AbstractFileSystem get AbstractFileSystem java at org apache hadoop fs FileContext run FileContext java at org apache hadoop fs FileContext run FileContext java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop fs FileContext getAbstractFileSystem FileContext java at org apache hadoop fs FileContext getFSofPath FileContext java at org apache hadoop fs FSLinkResolver resolve FSLinkResolver java at org apache hadoop fs FileContext create FileContext java at org apache hadoop fs FileContextMainOperationsBaseTest testGetFileContext FileContextMainOperationsBaseTest java at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java at java lang reflect Method invoke Method java at org junit runners model FrameworkMethod runReflectiveCall FrameworkMethod java at org junit internal runners model ReflectiveCallable run ReflectiveCallable java at org junit runners model FrameworkMethod invokeExplosively FrameworkMethod java at org junit internal runners statements InvokeMethod evaluate InvokeMethod java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit internal runners statements RunAfters evaluate RunAfters java at org junit runners ParentRunner runLeaf ParentRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners ParentRunner run ParentRunner java at org junit runners ParentRunner schedule ParentRunner java at org junit runners ParentRunner runChildren ParentRunner java at org junit runners ParentRunner access ParentRunner java at org junit runners ParentRunner evaluate ParentRunner java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit runners ParentRunner run ParentRunner java at org apache maven surefire junit JUnit Provider execute JUnit Provider java at org apache maven surefire junit JUnit Provider executeTestSet JUnit Provider java at org apache maven surefire junit JUnit Provider invoke JUnit Provider java at org apache maven surefire booter ForkedBooter invokeProviderInSameClassLoader ForkedBooter java at org apache maven surefire booter ForkedBooter runSuitesInProcess ForkedBooter java at org apache maven surefire booter ForkedBooter main ForkedBooter java Caused by java lang IllegalArgumentException No value for dfs adls oauth access token provider found in conf file at org apache hadoop fs adl AdlFileSystem getNonEmptyVal AdlFileSystem java at org apache hadoop fs adl AdlFileSystem getCustomAccessTokenProvider AdlFileSystem java at org apache hadoop fs adl AdlFileSystem getAccessTokenProvider AdlFileSystem java at org apache hadoop fs adl AdlFileSystem initialize AdlFileSystem java at org apache hadoop fs DelegateToFileSystem DelegateToFileSystem java at org apache hadoop fs adl Adl Adl java at sun reflect GeneratedConstructorAccessor newInstance Unknown Source at sun reflect DelegatingConstructorAccessorImpl newInstance DelegatingConstructorAccessorImpl java at java lang reflect Constructor newInstance Constructor java at org apache hadoop fs AbstractFileSystem newInstance AbstractFileSystem java at org apache hadoop fs AbstractFileSystem createFileSystem AbstractFileSystem java at org apache hadoop fs AbstractFileSystem get AbstractFileSystem java at org apache hadoop fs FileContext run FileContext java at org apache hadoop fs FileContext run FileContext java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop fs FileContext getAbstractFileSystem FileContext java at org apache hadoop fs FileContext getFSofPath FileContext java at org apache hadoop fs FSLinkResolver resolve FSLinkResolver java at org apache hadoop fs FileContext create FileContext java at org apache hadoop fs FileContextMainOperationsBaseTest testGetFileContext FileContextMainOperationsBaseTest java at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java at java lang reflect Method invoke Method java at org junit runners model FrameworkMethod runReflectiveCall FrameworkMethod java at org junit internal runners model ReflectiveCallable run ReflectiveCallable java at org junit runners model FrameworkMethod invokeExplosively FrameworkMethod java at org junit internal runners statements InvokeMethod evaluate InvokeMethod java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit internal runners statements RunAfters evaluate RunAfters java at org junit runners ParentRunner runLeaf ParentRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners BlockJUnit ClassRunner runChild BlockJUnit ClassRunner java at org junit runners ParentRunner run ParentRunner java at org junit runners ParentRunner schedule ParentRunner java at org junit runners ParentRunner runChildren ParentRunner java at org junit runners ParentRunner access ParentRunner java at org junit runners ParentRunner evaluate ParentRunner java at org junit internal runners statements RunBefores evaluate RunBefores java at org junit runners ParentRunner run ParentRunner java at org apache maven surefire junit JUnit Provider execute JUnit Provider java at org apache maven surefire junit JUnit Provider executeTestSet JUnit Provider java at org apache maven surefire junit JUnit Provider invoke JUnit Provider java at org apache maven surefire booter ForkedBooter invokeProviderInSameClassLoader ForkedBooter java at org apache maven surefire booter ForkedBooter runSuitesInProcess ForkedBooter java at org apache maven surefire booter ForkedBooter main ForkedBooter java noformat TestAdlFileContextMainOperationsLive testGetFileContext runtime error
No, ADLS TestAdlContractRootDirLive testRmNonEmptyRootDirNonRecursive failed
No,As discussed in HADOOP comment https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment and following comments there are still dependencies on the now removed hadoop client jar The current code builds only because an obsolete snapshot of the jar is found on the repository server Changing the project version to something new exposes the problem While the build currently dies at hadoop tools hadoop sls I m seeing issues with some Hadoop Client modules too I m filing a new bug because I can t reopen HADOOP Some modules have dependencies on hadoop client jar removed by HADOOP
Yes,Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency This JIRA removes the SDK snapshot dependency to released SDK candidate There is not functional change in the SDK and no impact to live contract test Remove snapshot version of SDK dependency from Azure Data Lake Store File System
No,HADOOP introduced an incompatible check that disallowed principal like HTTP host from being used as SPNEGO SPN This breaks the following test in trunk TestWebDelegationToken TestKMS TestTrashWithSecureEncryptionZones and TestSecureEncryptionZoneWithKMS because they used HTTP localhost as SPNEGO SPN assuming the default realm This ticket is opened to bring back the support of HTTP host as valid SPNEGO SPN KerberosName parsing bug was discovered fixed and included as a necessary part of this ticket along with additional unit test to cover parsing different form of principals Jenkins URL https builds apache org job hadoop qbt trunk java linux x testReport https builds apache org job PreCommit HADOOP Build testReport Maintain HTTP host as SPNEGO SPN support and fix KerberosName parsing
Yes,ViewFileSystem doesn t override FileSystem getLinkTarget So when view filesystem is used to resolve the symbolic links the default FileSystem implementation throws UnsupportedOperationException The proposal is to define getLinkTarget for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links Path thus returned is preferred to be a viewfs qualified path so that it can be used again on the ViewFileSystem handle Implement getLinkTarget for ViewFileSystem
Yes,Current implementation of WASB only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers Added to the fact that WASB does not inherently support ACL s WASB is its current implementation cannot be securely used for environments like secure hadoop cluster This JIRA is created to add a new mode in WASB which operates on Azure Storage SAS keys which can provide fine grained timed access to containers and blobs providing a segway into supporting WASB for secure hadoop cluster More details about the issue and the proposal are provided in the design proposal document Azure Add a new SAS key mode for WASB
No,Found a set of spelling errors in the logging and exception messages Examples Bufer Buffer princial principal existance existence Spelling errors in logging and exceptions for code
No,In YARN two new default values are added in YarnConfiguration for two timeline service properties that are skipped in TestConfigurationFieldsBase TestConfigurationFieldsBase fails as it mistakenly treats the two newly added default values as regular properties TestConfigurationFieldsBase fails for fields that are DEFAULT values of skipped properties
No,TestGridmixMemoryEmulation testTotalHeapUsageEmulatorPlugin fails on Mac The following tests fail on Mac as well TestResourceUsageEmulators testCumulativeCpuUsageEmulatorPlugin TestResourceUsageEmulators testCpuUsageEmulator TestResourceUsageEmulators testResourceUsageMatcherRunner TestGridmixMemoryEmulation and TestResourceUsageEmulators fail on the environment other than Linux or Windows
No,REF https builds apache org job hadoop qbt trunk java linux x testReport junit org apache hadoop fs TestSymlinkHdfsFileSystem testCreateLinkUsingPartQualPath TestSymlinkHdfsFileSystem testCreateLinkUsingPartQualPath fails after HADOOP
Yes,This is the KMS part Please refer to HDFS for the design doc Add reencryptEncryptedKey interface to KMS
No,The FsShell error handling assumes in displayError that the message argument is not null However in the case where it is this leads to a NPE which results in suppressing the actual error information since a higher level of error handling kicks in and just dumps the stack trace of the NPE instead e g This is deeply unhelpful because depending on what the underlying error was there may be no stack dumped logged for it as HADOOP provides since FsShell doesn t explicitly dump traces for IllegalArgumentException which appears to be the underlying cause of my issue Line is where displayError is called for IllegalArgumentException handling and that catch clause does not log the error FsShell can suppress the real error if no error message is present
Yes,KMS and HttpFS currently uses Tomcat propose to upgrade to the latest version is Upgrade Tomcat to
Yes,Currently SequenceFiles put in sync blocks every bytes It would be much better if it was configurable with a much higher default mb or so The distance between sync blocks in SequenceFiles should be configurable
Yes,Shell java has a hardcoded path to bin ls which is not correct on all platforms eg not on NixOS see HADOOP for a similar issue Remove hardcoded absolute path for ls
No,Command hadoop fs fs mkdir p usr hduser Results in a Stack Overflow The Problem is the Slash at the End of the IP Address When I remove it the command is executed correctly Stackoverflow for schemeless defaultFS with trailing slash
No,HADOOP cleaned up the javadoc build but as a result we now have a few javadoc dirs being created outside target folders Thanks to aw for finding this issue over on HADOOP Output javadoc inside the target directory
Yes,Currently the MutableRates metrics class serializes all writes to metrics it contains because of its use of MetricsRegistry add i e even two increments of unrelated metrics contained within the same MutableRates object will serialize w r t each other This class is used by RpcDetailedMetrics which may have many hundreds of threads contending to modify these metrics Instead we should allow updates to unrelated metrics objects to happen concurrently To do so we can let each thread locally collect metrics and on a snapshot aggregate the metrics from all of the threads I have collected some benchmark performance numbers in HADOOP https issues apache org jira secure attachment benchmark results which indicate that this can bring significantly higher performance in high contention situations Make MutableRates metrics thread local write aggregate on read
No,Shell checkIsBashSupported creates a bash shell command to verify if the system supports bash However its error message is misleading and the logic should be updated If the shell command throws an IOException it does not imply the bash did not run successfully If the shell command process was interrupted its internal logic throws an InterruptedIOException which is a subclass of IOException An example of it appeared in a recent jenkins job https builds apache org job PreCommit HADOOP Build testReport org apache hadoop ipc TestRPCWaitForProxy testInterruptedWaitForProxy The test logic in TestRPCWaitForProxy testInterruptedWaitForProxy starts a thread wait it for second and interrupt the thread expecting the thread to terminate However the method Shell checkIsBashSupported swallowed the interrupt and therefore failed The original design is not desirable as it swallowed a potential interrupt causing TestRPCWaitForProxy testInterruptedWaitForProxy to fail Unfortunately Java does not allow this static method to throw exception We should removed the static member variable so that the method can throw the interrupt exception The node manager should call the static method instead of using the static member variable This fix has an associated benefit the tests could run faster because it will no longer need to spawn a bash process when it uses a Shell static method variable which happens quite often for checking what operating system Hadoop is running on Shell checkIsBashSupported swallowed an interrupted exception
Yes,To track user level connections How many connections for each user in busy cluster where so many connections to server Expose NumOpenConnectionsPerUser as a metric
Yes,DiskChecker can fail to detect total disk controller failures indefinitely We have seen this in real clusters DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted A simple improvement is to write some data and flush it to the disk DiskChecker should perform some disk IO
Yes,The DiskChecker class has a few unused public methods We can remove them Cleanup DiskChecker interface
Yes,To make our tests robust against timing problems and eventual consistent stores we need to do more spin wait for state We have some we ve examples to follow Some of that work has been reimplemented slightly in S ATestUtils eventually I propose adding a class in the test tree Eventually to be a successor replacement for these has an eventually waitfor operation taking a predicate that throws an exception has an evaluate exception which tries to evaluate an answer until the operation stops raising an exception again from scalatest plugin backoff strategies from Scalatest lets you do exponential as well as linear option of adding a special handler to generate the failure exception e g run more detailed diagnostics for the exception text etc be Java lambda expression friendly be testable and tested itself Add LambdaTestUtils class for tests fix eventual consistency problem in contract test setup
Yes,The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned We need to allow for the subprocess to be interrupted and killed when the shell process gets killed Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed Ability to clean up subprocesses spawned by Shell when the process exits
No,In HttpServer hasAdministratorAccess it uses hadoop security authorization to detect whether HTTP is authenticated It s not correct because enabling Kerberos and HTTP SPNEGO are two steps If Kerberos is enabled while HTTP SPNEGO is not some links cannot be accessed such as logs and it will return error message as below quote HTTP ERROR Problem accessing logs Reason User dr who is unauthorized to access this page quote We should make sure HttpServletRequest getAuthType is not null before we invoke HttpServer hasAdministratorAccess getAuthType means to get the authorization scheme of this request If kerberos is enabled while HTTP SPNEGO is not configured some links cannot be accessed
Yes,Per discussion on HADOOP I d like to revert HADOOP It removes a deprecated API but the x line does not have a release with the new replacement API This places a burden on downstream applications Revert HADOOP Remove unused TrashPolicy getInstance and initialize code
Yes,Add a new instrumented read write lock in hadoop common so that the HDFS can use this to improve the locking in FsDatasetImpl Add a new instrumented read write lock
No,I think we can catch the exception in the main method and dump a log error message instead of throw the stack which may frustrate users LogLevel main throws exception if no arguments provided
No,Looking at maven output when running with Pdist the source plugin test jar and jar goals are invoked twice This is because it s turned on by both the dist profile and on by default Outside of the release context it s not that important to have javadoc and source JARs so I think we can turn it off by default Do not attach javadoc and sources jars during non dist build
Yes,We generate source code with line numbers for inclusion in the javadoc JARs Given that there s github and other online viewers this doesn t seem so useful these days Disabling the linkSource option saves us MB for the hadoop common javadoc jar Stop bundling HTML source code in javadoc JARs
Yes,Currently downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S A need to list dependencies on each one This creates an ongoing maintenance burden for those projects because they need to update their build whenever a new Hadoop compatible file system is introduced This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems Similar to hadoop client this new artifact will consist of just a pom xml listing the individual dependencies Downstream users can depend on this artifact to sweep in everything and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop
No,HADOOP introduced safely option to prevent accidental deletion of large directories with lots of files with rmr skipTrash This ticket is opened to track document the usage of this feature in FileSystemShell html https hadoop apache org docs r alpha hadoop project dist hadoop common FileSystemShell html rm Document safely option of rm command
Yes,The newly added Kafka module defines the Kafka dependency as Reduce Kafka dependencies in hadoop kafka module
No,Current implementation of WASB does not correctly handle multiple threads clients calling delete on the same file The expected behavior in such scenarios is only one of the thread should delete the file and return true while all other threads should receive false However in the current implementation even though only one thread deletes the file multiple clients incorrectly get true as the return from delete call Bug in return value for delete calls in WASB
Yes,As work continues on HADOOP it s become evident that we need better hooks to start daemons as specifically configured users Via the command subcommand USER environment variables in x we actually have a standardized way to do that This in turn means we can make the sbin scripts super functional with a bit of updating Consolidate start dfs sh and start secure dns sh into one script Make start sh and stop sh know how to switch users when run as root Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users Update scripts to be smarter when running with privilege
No,The maven project info reports plugin version depends on maven shared jar which uses bcel This does not work well with the new lamda expression The depends on maven shared jar which works around this problem by using the custom release of bcel Fix ClassFormatException in trunk build
No,When committing to branch we need to edit CHANGES txt However there are some recent commits to branch without editing CHANGES txt We need to update the change log Update CHANGES txt to reflect all the changes in branch
No,In some recent investigation it turns out when KMS throws an exception into tomcat it s not logged anywhere and we can only see the exception message from client side but not the stacktrace Logging the stacktrance would help debugging KMS Server should log exceptions before throwing
No,Fix typing mistake of inline document in hadoop metrics properties And also could add examples into the inline document for easier understanding of metrics tag related configuration Fix typing mistake of inline document in hadoop metrics properties
No,Sometimes the NodeResourceMonitor tries to read the system utilization from winutils exe and this return empty values This triggers the following exception java lang StringIndexOutOfBoundsException String index out of range at java lang String substring String java at org apache hadoop util SysInfoWindows refreshIfNeeded SysInfoWindows java at org apache hadoop util SysInfoWindows getPhysicalMemorySize SysInfoWindows java at org apache hadoop yarn util ResourceCalculatorPlugin getPhysicalMemorySize ResourceCalculatorPlugin java at org apache hadoop yarn server nodemanager NodeResourceMonitorImpl MonitoringThread run NodeResourceMonitorImpl java Index out of range in SysInfoWindows
Yes,We re currently pulling in version incubating I think we should upgrade to the latest incubating Upgrade HTrace version
Yes,We re currently pulling in version I think we should upgrade to the latest Upgrade commons configuration version to
No,Reported by Arpit on HADOOP quote INFO org apache hadoop maven plugin versioninfo VersionInfoMojo getSvnUriInfo String uses String indexOf String instead of String indexOf int org apache hadoop maven plugin versioninfo VersionInfoMojo At VersionInfoMojo java lines quote Fix findbugs warning in VersionInfoMojo java
Yes,add a metadata file giving the FS impl of swift remove the entry from core default xml swift FS to add a service load metadata file
No,We can t easily debug FS instantiation problems as there isn t much detail in what was going on We can add more logging but cannot simply switch FileSystem LOG to SLF J the class is used in too many places including tests which cast it Instead add a new private SLF J Logger LOGGER and switch logging to it While working in the base FileSystem class take the opportunity to clean up javadocs and comments add the list of exceptions including indicating which base classes throw UnsupportedOperationExceptions cut bits in the comments which are not true The outcome of this patch is that IDEs shouldn t highlight most of the file as flawed in some way or another Clean up FileSystem javadocs logging improve diagnostics on FS load
No,The packages related to the DockerLinuxContainerRuntime all exceed the char line length limit enforced by checkstyle This causes every build to fail with a I would like to exclude this rule from causing a failure Alternatively we could look to restructure the packages here but I question what value this check really provides Ignore package line length checkstyle rule
No, Fix some warnings by findbugs in hadoop maven plugin
Yes,The UGI has a background thread to renew the tgt On exception it terminates itself https github com apache hadoop blob bee f f ca f ade c fd b dad a hadoop common project hadoop common src main java org apache hadoop security UserGroupInformation java L L If something temporarily goes wrong that results in an IOE even if it recovered no renewal will be done and client will eventually fail to authenticate We should retry with our best effort until tgt expires in the hope that the error recovers before that Retry until TGT expires even if the UGI renewal thread encountered exception
Yes,Based on discussion at YETUS this code can t go there but it s still very useful for release managers A similar variant of this script has been used for a while by Apache HBase and Apache Kudu and IMO JACC output is easier to understand than JDiff Incorporate checkcompatibility script which runs Java API Compliance Checker
No,Reported by chiwanpark bq Since release Client get setPingInterval is changed from public to package private bq Giraph is one of broken examples for this changes https github com apache giraph blob release giraph core src main java org apache giraph job GiraphJob java L Fix source level compatibility after HADOOP
Yes,ZStandard https github com facebook zstd has been used in production for months by facebook now v was recently released Create a codec for this library Add Codec for ZStandard Compression
No,http hadoop apache org releases html shows released on January which should rather be August Website shows incorrect January as the release date for
No,The UGI checkTGTAndReloginFromKeytab method checks certain conditions and if they are met it invokes the reloginFromKeytab The reloginFromKeytab method then fails with an IOException loginUserFromKeyTab must be done first because there is no keytab associated with the UGI The checkTGTAndReloginFromKeytab method checks if there is a keytab isKeytab UGI instance variable associated with the UGI if there is one it triggers a call to reloginFromKeytab The problem is that the keytabFile UGI instance variable is NULL and that triggers the mentioned IOException The root of the problem seems to be when creating a UGI via the UGI loginUserFromSubject Subject method this method uses the UserGroupInformation Subject constructor and this constructor does the following to determine if there is a keytab or not If the Subject given had a keytab then the UGI instance will have the isKeytab set to TRUE It sets the UGI instance as it would have a keytab because the Subject has a keytab This has problems First it does not set the keytab file and this having the isKeytab set to TRUE and the keytabFile set to NULL is what triggers the IOException in the method reloginFromKeytab Second and even if the first problem is fixed this still is a problem it assumes that because the subject has a keytab it is up to UGI to do the relogin using the keytab This is incorrect if the UGI was created using the UGI loginUserFromSubject Subject method In such case the owner of the Subject is not the UGI but the caller so the caller is responsible for renewing the Kerberos tickets and the UGI should not try to do so UserGroupInformation created from a Subject incorrectly tries to renew the Kerberos ticket
No,RetryInvocationHandler logs a warning for any exception that it does not retry There are many exceptions that the client can automatically handle like FileNotFoundException UnresolvedPathException etc so now every one of these generates a scary looking stack trace as a warning then the program continues normally RetryInvocationHandler logs all remote exceptions
Yes,Leveraging HADOOP will allow non rpc calls to be added to the call queue This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS Support external calls in the RPC call queue
Yes,This patch adds to fs shell Stat java the missing options of a and A FileStatus already contains the getPermission method required for returning symbolic permissions FsPermission contains the method to return the binary short but nothing to present in standard Octal format Most UNIX admins base their work on such standard octal permissions Hence this patch also introduces one tiny method to translate the toShort return into octal Build has already passed unit tests and javadoc Add A and a formats for fs stat command to print permissions
No,org apache hadoop security ssl TestReloadingX TrustManager checks the key store file s last modified time to decide whether to reload This is to avoid unnecessary reload if the key store file is not changed To do this it maintains an internal state lastLoaded whenever it tries to reload a file It also updates the lastLoaded variable in case of exception so failing reload will not be retried until the key store file s last modified time changes again Chances are that the reload happens when the key store file is being written The reload fails probably with EOFException and won t load until key store files s last modified time changes After a short period the key store file is closed after update However the last modified time may not be updated as if it s in the same precision period e g second In this case the updated key store file is never reloaded A simple fix is to update the lastLoaded only when the reload succeeds ReloadingX TrustManager will keep reloading in case of exception Thoughts ReloadingX TrustManager should keep reloading in case of exception
No,FsPermissions s string constructor breaks on valid permission strings like This is because FsPermission class na vely uses UmaskParser to do it s parsing of permissions from source code public FsPermission String mode this new UmaskParser mode getUMask The mode string UMask accepts is subtly different esp wrt sticky bit so parsing Umask is not the same as parsing FsPermission FsPermission string constructor does not recognize sticky bit
No,In SaslRpcClient getServerPrincipal it only printed out server advertised principal The actual principal we expect from configuration is quite useful while debugging security related issues It should also be logged Improve SaslRpcClient failure logging
No,This bug was discovered when trying to run Impala libhdfs so with s a and Java KeyStore credentials Because JNI threads have a different classloader bootstrap we fail to load JavaKeyStoreProvider quote jni util cc java util ServiceConfigurationError org apache hadoop security alias CredentialProviderFactory Provider org apache hadoop security alias JavaKeyStoreProvider Factory not found at java util ServiceLoader fail ServiceLoader java at java util ServiceLoader access ServiceLoader java at java util ServiceLoader LazyIterator next ServiceLoader java at java util ServiceLoader next ServiceLoader java at org apache hadoop security alias CredentialProviderFactory getProviders CredentialProviderFactory java at org apache hadoop conf Configuration getPasswordFromCredentialProviders Configuration java at org apache hadoop conf Configuration getPassword Configuration java at org apache hadoop fs s a S AFileSystem getAWSAccessKeys S AFileSystem java at org apache hadoop fs s a S AFileSystem getAWSCredentialsProvider S AFileSystem java at org apache hadoop fs s a S AFileSystem initialize S AFileSystem java quote CredentialProviderFactory fails at class loading from libhdfs JNI
No,Looks like the test has been failing since HADOOP was committed https builds apache org job PreCommit HDFS Build testReport org apache hadoop tracing TestTracing testTracing Tracing in IPC Server is broken
Yes,Introduce an AutoCloseableLock class that is a thin wrapper over RentrantLock It allows using RentrantLock with try with resources syntax The wrapper functions perform no expensive operations in the lock acquire release path Add an AutoCloseableLock class
Yes,The RPC layer supports QoS but other protocols ex webhdfs are completely unconstrained Generalizing Server Call to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols Design Server Call to be extensible for unified call queue
Yes,Shell java has a hardcoded path to bin bash which is not correct on all platforms Pointed out by aw while reviewing HADOOP Remove hardcoded absolute path for shell executable
No,After HADOOP RawLocalFileSystem does react on changing umask but FileContext does not react on changing umask via configuration TestDirectoryCollection fails by the inconsistent behavior FileContext does not react on changing umask via configuration
No,Hadoop common failed to generate JDiff We need to fix that Fix hadoop common to generate jdiff
Yes,We re cleaning up Hive and Spark s use of FileSystem exists because it is often the case we see code of exists open exists delete when the exists probe is needless Against object stores expensive needless Hadoop can set an example here by stripping them out It will also show where there are opportunities to optimise things better and or improve reporting Eliminate needless uses of FileSystem exists isFile isDirectory
Yes,Umbrella jira for y optimizations to reduce object allocations more efficiently use protobuf APIs unified ipc and webhdfs callq to enable QoS etc IPC layer optimizations
No,There s a race in the globals The non global APIs from ZOOKEEPER are not available yet in a stable ZK version and there s no timeline for availability so for now it would help to make SM aware of other users of the global config ZKDelegationTokenSecretManager JaasConfig does not work well with other ZK users in process
Yes,Currently KMS audit log is using log j to write a text format log We should refactor this so that people can easily add new format audit logs The current text format log should be the default and all of its behavior should remain compatible Allow pluggable audit loggers in KMS
No,org apache hadoop fs s a TestS ATemporaryCredentials testSTS throws a AccessDenied when run without any AWS credentials access key and secret key in the config It fails because the InstanceProfileCredentialsProvider in the credentials chain on line is used but an instance profile always provides a temporary credential and GetSessionToken requires a long term not temporary credential Suggestion on how to fix this test case TestS ATemporaryCredentials testSTS error when using IAM credentials
No,Per the release instructions https wiki apache org hadoop HowToRelease we need to update hadoop project src site markdown index md vm to reflect the right versions new features and big improvements I can put together some notes for HADOOP and HDFS depending on others for YARN and MR Update release notes for alpha
Yes,In branch and later the patches for various child and related bugs listed in HADOOP most recently including HADOOP HADOOP HADOOP HADOOP and HDFS eliminate all use of commons httpclient from Hadoop and its sub projects except for hadoop tools hadoop openstack see HADOOP However after incorporating these patches commons httpclient is still listed as a dependency in these POM files hadoop project pom xml hadoop yarn project hadoop yarn hadoop yarn registry pom xml We wish to remove these but since commons httpclient is still used in many files in hadoop tools hadoop openstack we ll need to add the dependency to hadoop tools hadoop openstack pom xml We ll add a note to HADOOP to undo this when commons httpclient is removed from hadoop openstack In this was mostly done by HADOOP but the version info formerly inherited from hadoop project pom xml also needs to be added so that is in the branch version of the patch Other projects with undeclared transitive dependencies on commons httpclient previously provided via hadoop common or hadoop client may find this to be an incompatible change Of course that also means such project is exposed to the commons httpclient CVE and needs to be fixed for that reason as well remove unneeded commons httpclient dependencies from POM files in Hadoop and sub projects
No,h Error Message bq expected but was h Stacktrace quote java lang AssertionError expected but was at org junit Assert fail Assert java at org junit Assert failNotEquals Assert java at org junit Assert assertEquals Assert java at org junit Assert assertEquals Assert java at org junit Assert assertEquals Assert java at org apache hadoop security TestGroupsCaching testBackgroundRefreshCounters TestGroupsCaching java quote o a h security TestGroupsCaching testBackgroundRefreshCounters seems flaky
Yes,Update WASB driver to use the latest version of SDK for Microsoft Azure Storage Clients We are currently using version of the SDK Version brings some breaking changes Need to fix code to resolve all these breaking changes and certify that everything works properly Update WASB driver to use the latest version of SDK for Microsoft Azure Storage Clients
No,When IOException throws in getPassword getPassword return null String this will cause setConf throws java lang NullPointerException when check isEmpty on null string LdapGroupsMapping getPassward shouldn t return null when IOException throws
No,Fix up LICENSE and NOTICE after HADOOP Additional fix to LICENSE and NOTICE
Yes,Right now the git repo has a branch named master in addition to our trunk branch Since master is the common place name of the most recent branch in git repositories this is misleading to new folks It looks like the branch is from months ago We should remove it delete spurious master branch
Yes,Big features like YARN demonstrate that even senior level Hadoop developers forget that daemons need a custom OPTS env var We can replace all of the custom vars with generic handling just like we do for the username check For example with generic handling in place Old Var New Var HADOOP NAMENODE OPTS HDFS NAMENODE OPTS YARN RESOURCEMANAGER OPTS YARN RESOURCEMANAGER OPTS n a YARN TIMELINEREADER OPTS n a HADOOP DISTCP OPTS n a MAPRED DISTCP OPTS HADOOP DN SECURE EXTRA OPTS HDFS DATANODE SECURE EXTRA OPTS HADOOP NFS SECURE EXTRA OPTS HDFS NFS SECURE EXTRA OPTS HADOOP JOB HISTORYSERVER OPTS MAPRED HISTORYSERVER OPTS This makes it a consistent across the entire project b consistent for every subcommand c eliminates almost all of the custom appending in the case statements It s worth pointing out that subcommands like distcp that sometimes need a higher than normal client side heapsize or custom options are a huge win Combined with hadooprc and or dynamic subcommands it means users can easily do customizations based upon their needs without a lot of weirdo shell aliasing or one line shell scripts off to the side Deprecate HADOOP SERVERNAME OPTS replace with command subcommand OPTS
No,Hive does not compile against Hadoop SNAPSHOT Looks like the change in HADOOP causes this Incompatible change to SortedMapWritable
Yes, Update maven enforcer plugin version to
No,After HADOOP we are seeing mvn install DskipTests failing in branch branch and branch This failure is caused by the followings hadoop project module depends on hadoop build tools module but hadoop project module does not declare hadoop build tools as its submodule Therefore hadoop build tools is not built before building hadoop project hadoop build tools pom and jar are not uploaded to the snapshot repository https repository apache org content repositories snapshots org apache hadoop hadoop build tools The build failure occurs if the both of the above conditions are satisfied Add missing dependency in setting maven remote resource plugin to fix builds
Yes,Current ViewFileSystem does not support storage policy related API it will throw UnsupportedOperationException ViewFileSystem should support storage policy related API
No,Unit test TestTextInputFormat testSplitableCodecs failed when the seed is Stacktrace java lang AssertionError Key in multiple partitions at org junit Assert fail Assert java at org junit Assert assertTrue Assert java at org junit Assert assertFalse Assert java at org apache hadoop mapred TestTextInputFormat testSplitableCodecs TestTextInputFormat java BZip CompressionInputStream finds the same compression marker twice in corner case causing duplicate data blocks
Yes,In HADOOP the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background avoiding many slow group lookups Even with this change I have seen quite a few clusters with issues due to slow group lookups The problem is most prevalent in HA clusters where a slow group lookup on the hdfs user can fail to return for over seconds causing the Failover Controller to kill it The way the current Guava cache implementation works is approximately On initial load the first thread to request groups for a given user blocks until it returns Any subsequent threads requesting that user block until that first thread populates the cache When the key expires the first thread to hit the cache after expiry blocks While it is blocked other threads will return the old value I feel it is this blocking thread that still gives the Namenode issues on slow group lookups If the call from the FC is the one that blocks and lookups are slow if can cause the NN to be killed Guava has the ability to refresh expired keys completely in the background where the first thread that hits an expired key schedules a background cache reload but still returns the old value Then the cache is eventually updated This patch introduces this background reload feature There are two new parameters hadoop security groups cache background reload default false to keep the current behaviour Set to true to enable a small thread pool and background refresh for expired keys hadoop security groups cache background reload threads only relevant if the above is set to true Controls how many threads are in the background refresh pool Default is which is likely to be enough Reload cached groups in background after expiry
No,RetryInvocationHandler need wrap InterruptedException in IOException when call Thread sleep Otherwise InterruptedException can t be handled correctly by other components such as HDFS RetryInvocationHandler need wrap InterruptedException in IOException when call Thread sleep
No,truncate will fail when use viewFS Path should be like below return res targetFileSystem truncate f newLength should be return res targetFileSystem truncate res remainingPath newLength truncate will fail when we use viewfilesystem
Yes,This JIRA is to address Jing s comments https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment in HADOOP AsyncCallHandler should use an event driven architecture to handle async calls
Yes,In current Async DFS implementation file system calls are invoked and returns Future immediately to clients Clients call Future get to retrieve final results Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB ProtobufRpcEngine and ipc Client The callback path bypasses the original retry layer logic designed for synchronous DFS This proposes refactoring to make retry also works for Async DFS Support async call retry and failover
No,In single cluster setup document the grep job fails Grep job in Single Cluster document fails
Yes,This is a follow up jira from HADOOP Now with the findbug warning As discussed in HADOOP bq Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away we will add the findbugsExcludeFile xml and will get rid of this given kerby rc release Add the kerby version hadoop project pom xml bq hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop under dependencyManagement Only here version will be mentioned All other Hadoop Modules will inherit hadoop project so all submodules will use the same version In submodule version need not be mentioned in pom xml This will make version management easier Follow on fixups after upgraded mini kdc using Kerby
Yes,slaves sh and the slaves file should get replace with workers sh and a workers file replace slaves with workers
Yes,Upgrade yetus wrapper to be now that it has passed vote Upgrade to Apache Yetus
No,Currently there are two ways to achieve KMS HA The first one and the only documented one is running multiple KMS instances behind a load balancer https hadoop apache org docs stable hadoop kms index html The other way is make use of LoadBalancingKMSClientProvider which is added in HADOOP However the usage is undocumented I think we should update the KMS document to introduce LoadBalancingKMSClientProvider provide examples and also update kms site xml to explain it Mention LoadBalancingKMSClientProvider in KMS HA documentation
No,Many ASF projects include Apache in their logo We should add it to Hadoop Add Apache to Hadoop project logo
Yes,The hadoop ant code is an ancient kludge unlikely to have any users still We can delete it from trunk as a scream test for x Remove hadoop ant from hadoop tools
Yes,Currently the Future returned by ipc async call only support Future get but not Future get timeout unit We should support the latter as well Support Future get with timeout in ipc async calls
Yes,We should slim down the Docker image by removing JDK now that trunk no longer supports it remove JDK from Dockerfile
No,HADOOP pulled the dist copynativelibs script into an external file The call to this script is failing when running a distro build on Windows Windows distro build fails on dist copynativelibs
Yes,After DistCp copies a file it calls getFileStatus to get the FileStatus from the destination so that it can compare to the source and update metadata if necessary If the DistCp command was run without the option to preserve metadata attributes then this additional getFileStatus call is wasteful In DistCp prevent unnecessary getFileStatus call when not preserving metadata
Yes,We want to rename to alpha for the first alpha release However the version number is also encoded outside of the pom xml s so we need to update these too Change project version from to alpha
No,It s been noticed that Mini HDFS Cluster fails to start on trunk blocking unit tests and Jenkins Mini HDFS Cluster fails to start on trunk
No,Hi I m trying to use the append functionnality to an existing SequenceFile If I set Compression NONE it works when the file is created but when the file already exists I ve a NullPointerException by the way it works if I specify a compression with a codec Thansk Micka l Unable to append to a SequenceFile with Compression NONE
No,When I run hadoop trace command for a Kerberized NameNode it failed with the following error hdfs weichiu encryption root hadoop trace list host weichiu encryption vpc cloudera com WARN ipc Client Exception encountered while connecting to the server java lang IllegalArgumentException Failed to specify server s Kerberos principal name WARN security UserGroupInformation PriviledgedActionException as hdfs VPC CLOUDERA COM auth KERBEROS cause java io IOException java lang IllegalArgumentException Failed to specify server s Kerberos principal name Exception in thread main java io IOException Failed on local exception java io IOException java lang IllegalArgumentException Failed to specify server s Kerberos principal name Host Details local host is weichiu encryption vpc cloudera com destination host is weichiu encryption vpc cloudera com at org apache hadoop net NetUtils wrapException NetUtils java at org apache hadoop ipc Client call Client java at org apache hadoop ipc Client call Client java at org apache hadoop ipc ProtobufRpcEngine Invoker invoke ProtobufRpcEngine java at com sun proxy Proxy listSpanReceivers Unknown Source at org apache hadoop tracing TraceAdminProtocolTranslatorPB listSpanReceivers TraceAdminProtocolTranslatorPB java at org apache hadoop tracing TraceAdmin listSpanReceivers TraceAdmin java at org apache hadoop tracing TraceAdmin run TraceAdmin java at org apache hadoop tracing TraceAdmin main TraceAdmin java Caused by java io IOException java lang IllegalArgumentException Failed to specify server s Kerberos principal name at org apache hadoop ipc Client Connection run Client java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop ipc Client Connection handleSaslConnectionFailure Client java at org apache hadoop ipc Client Connection setupIOstreams Client java at org apache hadoop ipc Client Connection access Client java at org apache hadoop ipc Client getConnection Client java at org apache hadoop ipc Client call Client java more Caused by java lang IllegalArgumentException Failed to specify server s Kerberos principal name at org apache hadoop security SaslRpcClient getServerPrincipal SaslRpcClient java at org apache hadoop security SaslRpcClient createSaslClient SaslRpcClient java at org apache hadoop security SaslRpcClient selectSaslClient SaslRpcClient java at org apache hadoop security SaslRpcClient saslConnect SaslRpcClient java at org apache hadoop ipc Client Connection setupSaslConnection Client java at org apache hadoop ipc Client Connection access Client java at org apache hadoop ipc Client Connection run Client java at org apache hadoop ipc Client Connection run Client java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop ipc Client Connection setupIOstreams Client java more It is failing because TraceAdmin does not set up the property CommonConfigurationKeys HADOOP SECURITY SERVICE USER NAME KEY Fixing it may require some restructuring as the NameNode principal dfs namenode kerberos principal is a HDFS property but TraceAdmin is in hadoop common Or specify it with a new command principal Any suggestions Thanks TraceAdmin should support Kerberized cluster
No,An Oozie job with a single shell action fails may not be important but if you needs the exact details I can provide them with an error message coming from NodeManager The unsafe cast is here https github com apache hadoop blob e d ff e b c d b ed bc a hadoop common project hadoop common src main java org apache hadoop crypto key kms LoadBalancingKMSClientProvider java L Because of this ClassCastException an uncaught exception is raised we do not see the exact caused by exception message the oozie job fails YARN logs are not reported saved Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider
No,User Hadoop on secure mode login as kdc user kinit start firefox and enable Kerberos access http localhost logs Get authorization errors only hdfs user could access logs Would expect as a user to be able to web interface logs link Same results if using curl curl v negotiate u tester http localhost logs HTTP User tester is unauthorized to access this page so either don t show links if hdfs user is able to access provide mechanism to add users to web application realm note that we are pass authentication so the issue is authorization to logs suspect that logs path is secure in webdescriptor so suspect users by default don t have access to secure paths Add ability to secure log servlet using proxy users
Yes,LdapGroupsMapping currently does not set timeouts on the LDAP queries This can create a risk of a very long infinite wait on a connection Support timeouts in LDAP queries in LdapGroupsMapping
No,Please have a look following PreCommit build on branch https builds apache org job PreCommit HDFS Build artifact patchprocess patch asflicense problems txt Fix ASF License warnings in branch
No,We are generating a lot of javadoc warnings with jdk Right now the number is limited to Enlarge this limitation can probably reveal more problems in one batch for our javadoc generation process The number of javadocs warnings is limited to
No,hackage haskell org is pretty unreliable switch to fpcomplete s mirror quiet some of the output to make jenkins debugging easier some other random fixes cleanup the dockerfile
Yes,Currently FileSystem Statistics exposes the following statistics BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks There is logic within DfsClient to map operations to these counters that can be confusing for instance mkdirs counts as a writeOp Proposed enhancement Add a statistic for each DfsClient operation including create append createSymlink delete exists mkdirs rename and expose them as new properties on the Statistics object The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS For example we can use them to identify jobs that end up creating a large number of files Once this information is available in the Statistics object the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary Add a new interface for retrieving FS and FC Statistics
No,None of our bundled javascript dependencies are mentioned in LICENSE txt Let s fix that Add LICENSE txt entries for bundled javascript dependencies
No,As noted on HADOOP we lost the leveldbjni related NOTICE and LICENSE updates done in YARN when HADOOP was committed Let s restore them Restore lost leveldbjni LICENSE and NOTICE changes
Yes,The jira proposes an improvement over HADOOP to remove webhdfs dependencies from the ADL file system client and build out a standalone client At a high level this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake The scheme used for accessing the file system will continue to be adl azuredatalake net path to file The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store Refactor Azure Data Lake Store as an independent FileSystem
No,When doing distcp of raw data using delete feature following bug appears The issue is not with the distributed copy the issue is when it tries to delete things in the target that no longer exist in the source it revalidates to make sure NONE is in the reserved raw domain Distcp with delete feature on raw data not implemented
No,HDFS showed that HDFS was return on read buf when there was no data left in the stream Java IO says bq If len is zero then no bytes are read and is returned otherwise there is an attempt to read at least one byte Review the implementations of IOStream buffer offset bytes and where necessary and considered safe add a fast exit if the length is Implementations of InputStream read buffer offset bytes to exit if bytes
Yes,Cross Frame Scripting XFS prevention for UIs can be provided through a common servlet filter This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs if appropriate Add XFS Filter for UIs to Hadoop Common
No,Some work on S a has shown up that there aren t tests catching regressions in readFully reviewing the documentation shows that its specification could be improved review the spec review the implementations add tests proposed to the seek contract streams which support seek should support positioned readable fix code where it differs significantly from HDFS or LocalFS Specify PositionedReadable add contract tests fix problems
No,An example Some tests in org apache hadoop fs shell find occasionally time out
Yes,This allows metrics collector such as AMS to collect it with MetricsSink The per user RPC call counts schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues Support MetricsSource interface for DecayRpcScheduler Metrics
No,Update https hadoop apache org docs current hadoop project dist hadoop common FileSystemShell html with information about relative path and current working directory as suggested by yzhangal during HADOOP discussion FileSystemShell doc should explain relative path
Yes,As per comment https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment from wheat in HADOOP Need to remove FileUtil copyMerge CC to wheat Remove FileUtil copyMerge
No,Creating a key that contains special character s in its name will result in failure when creating while that key is in fact created ok on the underlying key provider E g KMS key names are incorrectly encoded when creating key
No,I saw an OOM that appears to have been caused by the phantom references introduced for file system statistics management I ll post details in a followup comment PhantomReference for filesystem statistics can trigger OOM
Yes,In async RPC if the callers don t read replies fast enough the buffer storing replies could be used up This is to propose limiting the number of outstanding async calls to eliminate the issue Limit the number of outstanding async calls
No,Ref the comment here https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment When run hadoop checknative it also failed Got something like below from log Fix bugs in the initialization of the ISA L library JNI bindings
No,andrew wang suggested that the current KMS ACL page is not very user focused and hard to come by without reading the and I agree So this jira puts more documentation to explain the current implementation Improve documentation on KMS ACLs and delegation tokens
Yes,HADOOP added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook For each of the shutdown hook registered we currently don t have an upper bound for its execution time We have seen namenode failed to shutdown completely waiting for shutdown hook to finish after failover for a long period of time which breaks the namenode high availability scenarios This ticket is opened to allow specifying a timeout value for the registered shutdown hook ShutdownHookManager should have a timeout for each of the Registered shutdown hook
Yes,Currently the dfs test command only supports d e f s z options It would be helpful if we add w r to verify permission of r w before actual read or write This will help script programming Add w r options in dfs test command
Yes,Umbrella for converting hadoop hdfs mapred and yarn to allow for dynamic subcommands See first comment for more details Umbrella Dynamic subcommands for hadoop shell scripts
Yes,Currently back off policy from HADOOP is hard coded to base on whether call queue is full This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities Allow RPC scheduler callqueue backoff using response times
Yes,As discussed in the mailing list we d like to introduce Apache Kerby into Hadoop Initially it s good to start with upgrading Hadoop MiniKDC with Kerby offerings Apache Kerby https github com apache directory kerby as an Apache Directory sub project is a Java Kerberos binding It provides a SimpleKDC server that borrowed ideas from MiniKDC and implemented all the facilities existing in MiniKDC Currently MiniKDC depends on the old Kerberos implementation in Directory Server project but the implementation is stopped being maintained Directory community has a plan to replace the implementation using Kerby MiniKDC can use Kerby SimpleKDC directly to avoid depending on the full of Directory project Kerby also provides nice identity backends such as the lightweight memory based one and the very simple json one for easy development and test environments Upgrade Hadoop MiniKDC with Kerby
Yes,In ipc Client the underlying mechanism is already supporting asynchronous calls the calls shares a connection the call requests are sent using a thread pool and the responses can be out of order Indeed synchronous call is implemented by invoking wait in the caller thread in order to wait for the server response In this JIRA we change ipc Client to support asynchronous mode In asynchronous mode it return once the request has been sent out but not wait for the response from the server Change ipc Client to support asynchronous calls
Yes,Update Yetus to Update Yetus to
No,HADOOP added support for RPC congestion control by sending retriable server too busy exceptions to clients However every backoff results in a log message We ve seen these log messages slow down the NameNode We already have a metric that tracks the number of backoff events This log message adds nothing useful The IPC Server should also allow services to skip logging certain exception types altogether IPC Server should allow suppressing exception logging by type not log server too busy messages
No,The Javadocs in AuthenticationFilter say However the string implementation is no longer available because HADOOP moved it to be a test only artifact This also doesn t mention anything about the file backed secret provider FileSignerSecretProvider JavaDocs for SignerSecretProvider are out of date in AuthenticationFilter
No,In HADOOP we pulled the dist layout stitching and dist tar stitching scripts out of hadoop dist pom xml and into external files It appears this change is not working correctly on Windows External distribution stitching scripts do not work correctly on Windows
No,We have many bundled dependencies in both the source and the binary artifacts that are not in LICENSE txt and NOTICE txt Verify LICENSE txt and NOTICE txt
Yes,We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker Remove MRv terms from HttpAuthentication md
No,Attempting to compile openstack on a fairly fresh maven repo fails due to commons httpclient not being a declared dependency After that is fixed doing a maven dependency analyze shows other problems Fix hadoop openstack undeclared and unused dependencies
Yes,gridmix shouldn t require a raw java command line to run add a subcommand for gridmix
Yes,When o a h record was moved bin rcc was never updated to pull those classes from the streaming jar Remove bin rcc script
Yes,As discussed in mailing list this will disable style checks in class setters like the following Disable hiding field style checks in class setters
Yes,As hadoop tools grows bigger and bigger it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows Let s rework this to be smarter Rework hadoop tools
Yes,Let s pull the shell code out of the hadoop dist pom xml pull shell code out of hadoop dist
No,Error Message expected but was Stacktrace quote java lang AssertionError expected but was at org junit Assert fail Assert java at org junit Assert failNotEquals Assert java at org junit Assert assertEquals Assert java at org junit Assert assertEquals Assert java at org junit Assert assertEquals Assert java at org apache hadoop fs SymlinkBaseTest testSetTimesDanglingLink SymlinkBaseTest java at org apache hadoop fs TestSymlinkLocalFS testSetTimesDanglingLink TestSymlinkLocalFS java at sun reflect NativeMethodAccessorImpl invoke Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java at java lang reflect Method invoke Method java at org junit runners model FrameworkMethod runReflectiveCall FrameworkMethod java at org junit internal runners model ReflectiveCallable run ReflectiveCallable java at org junit runners model FrameworkMethod invokeExplosively FrameworkMethod java at org junit internal runners statements InvokeMethod evaluate InvokeMethod java at org junit internal runners statements FailOnTimeout StatementThread run FailOnTimeout java quote It happens in recent builds https builds apache org job PreCommit HADOOP Build testReport org apache hadoop fs TestSymlinkLocalFSFileSystem testSetTimesDanglingLink https builds apache org job PreCommit HADOOP Build testReport org apache hadoop fs TestSymlinkLocalFSFileSystem testSetTimesSymlinkToFile https builds apache org job PreCommit HADOOP Build artifact patchprocess patch unit hadoop common project hadoop common jdk txt TestSymlinkLocalFSFileSystem fails intermittently
No,There are findbugs warnings in branch Fix findbugs warnings in hadoop common branch
No,Logging slow name resolutions would be useful in identifying DNS performance issues in a cluster Most resolutions go through org apache hadoop security SecurityUtil getByName see attached call graph Adding additional logging to this method would expose such issues Log slow name resolutions
No,HADOOP collects the node network usage for Linux this JIRA does it for Windows Collect network and disk usage on the node running Windows
Yes,Java supports TLSv and TLSv which are more secure than TLSv which was all that was supported in Java so we should add those to the default list for hadoop ssl enabled protocols Enable TLS v and
Yes,The HBase s HMaster port number conflicts with Hadoop kms port number Both uses There might be use cases user need kms and HBase present on the same cluster The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories Users would have to manually override the default port of either application on their cluster It would be nice to have different default ports so kms and HBase could naturally coexist Change kms server port number which conflicts with HMaster port number
No,Added and updated changelog and release notes based upon Yetus SNAPSHOT Update changelog and release notes
No,If the KMS server encounters an unexpected error resulting in an HTTP response it does not log the stack trace This makes it difficult to troubleshoot The client side exception cannot provide further details KMS does not log detailed stack trace for unexpected errors
No,LdapGroupsMapping has lots of configurable properties and is thus fairly complex in nature HDFS Permissions Guide has a minimal introduction to LdapGroupsMapping with reference to More information on configuring the group mapping service is available in the Javadocs However its Javadoc provides no information about how to configure it Core default xml has descriptions for each property but still lacks a comprehensive tutorial Without a tutorial guide these configurable properties would be buried under the sea of properties Both Cloudera and HortonWorks has some information regarding LDAP group mapping http www cloudera com documentation enterprise latest topics cm sg ldap grp mappings html http hortonworks com blog hadoop groupmapping ldap integration But neither cover all configurable features such as using SSL with LDAP and POSIX group semantics Write a new group mapping service guide
No,I found hadoop key command usage is not documented when reviewing HDFS In addition we should document that uppercase is not allowed for key name hadoop key command usage is not documented
Yes,The typical LDAP group name resolution works well under typical scenarios However we have seen cases where a user is mapped to many groups in an extreme case a user is mapped to more than groups The way it s being implemented now makes this case super slow resolving groups from ActiveDirectory The current LDAP group resolution implementation sends two queries to a ActiveDirectory server The first query returns a user object which contains DN distinguished name The second query looks for groups where the user DN is a member If a user is mapped to many groups the second query returns all group objects associated with the user and is thus very slow After studying a user object in ActiveDirectory I found a user object actually contains a memberOf field which is the DN of all group objects where the user belongs to Assuming that an organization has no recursive group relation that is a user A is a member of group G and group G is a member of group G we can use this properties to avoid the second query which can potentially run very slow I propose that we add a configuration to only enable this feature for users who want to reduce group resolution time and who does not have recursive groups so that existing behavior will not be broken Faster LDAP group name resolution with ActiveDirectory
Yes,Remove getaclstatus call for non acl commands in getfacl Remove getaclstatus call for non acl commands in getfacl
No,Currently if a user uses HBase and enables the client job classloader the job fails to load HBase classes For example It is because the HBase classes org apache hadoop hbase meet the system classes criteria which are supposed to be loaded strictly from the base classloader But hadoop does not provide HBase as a dependency We should exclude the HBase classes from the system classes until unless HBase is provided by a future version of hadoop HBase classes fail to load with client job classloader enabled
No,Per the discussion in HDFS this jira is to propose adding a step in the release process https wiki apache org hadoop HowToRelease to update the release year in Web UI footer when creating RC for a release Add a step in the release process to update the release year in Web UI footer
Yes,Various SSL security fixes are needed See CVE CVE CVE CVE update apache httpclient version to httpcore to
Yes,The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in The issue is that HDFS does not update the file size until it s closed HDFS and if no new metrics record comes in then the file size will never be updated This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour RollingFileSystemSink should eagerly rotate directories
Yes,To protect against CSRF attacks HADOOP introduces a CSRF filter that will require a specific HTTP header to be sent with every REST API call This will affect all API consumers from web apps to CLIs and curl Since CSRF is primarily a browser based attack we can try and minimize the impact on non browser clients This enhancement will provide additional configuration for identifying non browser useragents and skipping the enforcement of the header requirement for anything identified as a non browser This will largely limit the impact to browser based PUT and POST calls when configured appropriately Extend CSRF Filter with UserAgent Checks
Yes,Aliyun OSS is widely used among China s cloud users but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application because of no original support for OSS in Hadoop This work aims to integrate Aliyun OSS with Hadoop By simple configuration Spark Hadoop applications can read write data from OSS without any code change Narrowing the gap between user s APP and data storage like what have been done for S in Hadoop Incorporate Aliyun OSS file system implementation
Yes,There is a problem when a user job adds too many dependency jars in their command line The HADOOP CLASSPATH part can be addressed including using wildcards But the same cannot be done with the libjars argument Today it takes only fully specified file paths We may want to consider supporting wildcards as a way to help users in this situation The idea is to handle it the same way the JVM does it expands to the list of jars in that directory It does not traverse into any child directory Also it probably would be a good idea to do it only for libjars i e don t do it for files and archives support wildcard in libjars argument
No,I m seeing this error when running test patch in Apache Hadoop quote Confirming git environment ERROR Users rchiang Dev ah patchprocess is not a git repo quote From a follow up email it s a trivial bug in the yetus wrapper Missing a popd after extraction Simple fix is to run the command twice since the short circuit kicks in after yetus is cached Fix git environment check during test patch
No,When the user doesn t have access permission to the local directory the hadoop fs put command prints a confusing error message No such file or directory Incorrect error message by fs put local dir without permission
No,Encountered an NPE when trying to use the HBase utility ExportSnapshot with Azure as the target It turns out verifyAndConvertToStandardFormat is returning null when determining the hbaseRoot and this is being added to the atomicRenameDirs set java lang NullPointerException at org apache hadoop fs azure AzureNativeFileSystemStore isKeyForDirectorySet AzureNativeFileSystemStore java at org apache hadoop fs azure AzureNativeFileSystemStore isAtomicRenameKey AzureNativeFileSystemStore java at org apache hadoop fs azure NativeAzureFileSystem prepareAtomicFolderRename NativeAzureFileSystem java at org apache hadoop fs azure NativeAzureFileSystem rename NativeAzureFileSystem java at org apache hadoop hbase snapshot ExportSnapshot run ExportSnapshot java at org apache hadoop util ToolRunner run ToolRunner java at com yammer calmie snapshot AbstractSnapshotUtil exportSnapshot AbstractSnapshotUtil java at com yammer calmie snapshot AbstractSnapshotUtil run AbstractSnapshotUtil java at org apache hadoop util ToolRunner run ToolRunner java at com yammer calmie snapshot SnapshotAzureBlobUtil main SnapshotAzureBlobUtil java NPE when trying to rename a directory in Windows Azure Storage FileSystem
No,Fix hadoop mapreduce client nativetask unit test which fails because it is not able to open the glibc bug spill file Fix hadoop mapreduce client nativetask unit test which fails because it is not able to open the glibc bug spill file
Yes,Some of the checkstyle checks are not realistic like the line length leading to spurious in precommit Let s disable Disable spurious checkstyle checks
No,TestLocalFsFCStatistics has been failing sometimes and when it fails it appears to be from FCStatisticsBaseTest testStatisticsThreadLocalDataCleanUp The test is timing out when it fails TestLocalFsFCStatistics testStatisticsThreadLocalDataCleanUp times out occasionally
Yes,We need a metrics sink that can write metrics to HDFS The sink should accept as configuration a directory prefix and do the following in putMetrics Get yyyyMMddHH from current timestamp If HDFS dir dir prefix yyyyMMddHH doesn t exist create it Close any currently open file and create a new file called log in the new directory Write metrics to the current log file Add an HDFS metrics sink
No,I ve seen several failures of testKMSProvider all failed in the following snippet with error message quote Values should be different Actual k quote TestKMS testKMSProvider intermittently fails during test rollover draining
Yes,CSRF prevention for REST APIs can be provided through a common servlet filter This filter would check for the existence of an expected configurable HTTP header such as X XSRF Header The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin Add CSRF Filter for REST APIs to Hadoop Common
No,HADOOP was resolved by replacing return null with throwing IOException This causes several S filesystem operations to fail possibly more sites and allows distcp to succeed S filesystem operations stopped working correctly
No,There are dead links in Compability md The links to MRAppMaster JobHistoryServer REST API are wrong https hadoop apache org docs r hadoop project dist hadoop common Compatibility html REST APIs Fix deadlinks in Compatibility md
No,https builds apache org job Hadoop Common trunk testReport org apache hadoop crypto key kms server TestKMS testKMSRestartSimpleAuth Seems to be introduced by HADOOP Fix TestKMS testKMSRestart failure
No,start build env sh fails in branch Found in MAPREDUCE https builds apache org job PreCommit MAPREDUCE Build console start build env sh fails in branch
Yes,Currently if the value of ipc client rpc timeout ms is greater than the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed RPC timeout should work without effectively disabling IPC ping RPC timeout should not override IPC ping interval
Yes,Currently Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section However the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites This so it can exclude the ciphers supplied through this key Support excluding weak Ciphers in HttpServer through ssl server xml
Yes,h Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store ADL from within Hadoop This would enable existing Hadoop applications such has MR HIVE Hbase etc to use ADL store as input or output ADL is ultra high capacity Optimized for massive throughput with rich management and security features More details available at https azure microsoft com en us services data lake store Support Microsoft Azure Data Lake as a file system in Hadoop
No,Another address in use error I saw the test TestKerberosAuthenticationHandler testNameRules failed due to porting binding error https builds apache org job Hadoop common trunk Java testReport org apache hadoop security authentication server TestKerberosAuthenticationHandler testNameRules Looking at MiniKdc implementation if port is the constructor use ServerSocket to find an unused port assign the port number to the member variable port and close the ServerSocket object later in initKDCServer instantiate a TcpTransport object and bind at that port It appears that the port may be used in between and then throw the exception MiniKdc throws address in use BindException
No,Saw it in a pre commit jenkins job https builds apache org job PreCommit HADOOP Build testReport org apache hadoop http TestHttpServer testBindAddress It also appeared previously in Hadoop common trunk Java jenkins on Oct In the following case the first server bound to port and the second one bound to port which violated the assertion in the test case the second port is supposed to be no more than the first TestHttpServer testBindAddress bind port range is wider than expected
Yes,Now that Yetus has had a release we should rip out the components that make it up from dev support and replace them with wrappers The wrappers should default to a sane version allow for version overrides via an env var download into patchprocess execute with the given parameters Marking this as an incompatible change since we should also remove the filename extensions and move these into a bin directory for better maintenance towards the future Replace dev support with wrappers to Yetus
Yes,Currently the WASB implementation of the HDFS interface does not support Append API This JIRA is added to design and implement the Append API support to WASB The intended support for Append would only support a single writer Adding Append API support for WASB
Yes,HADOOP added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment start build env sh This needs to be fixed Intel ISA L libraries should be added to the Dockerfile
Yes,We should add a config to disable the logs endpoint in HttpServer Listing a directory like this can be dangerous from a security perspective We can keep it enabled by default for compatibility though Add a config to disable the logs endpoints
No,In debugging a NM retry connection to RM non HA the NM log during RM down time is very misleading It actually only log client side retry on NetworkConnection failure but not include any info on RetryInvocationHandler where the real retry policy works From the We should add failAction reason as much as we can in multiple retry policies In addition we should keep consistent in log level for message during the retry attempts now the ipc client is INFO but RetryInvocationHandler is DEBUG if not fail over We should keep them consistent or it could be very confusing RetryPolicies other than FailoverOnNetworkExceptionRetry should put on retry failed reason or the log from RMProxy s retry could be very misleading
No,NPE thrown which is hiding actual error Here it should throw numberformatexpection since count exceeds integer range NPE in TestSequenceFile
No,This failure seems to exist after November rd I am still tracing where this can come from https builds apache org job Hadoop Common trunk testReport org apache hadoop fs shell find TestFind processArguments Error Message test timed out after milliseconds Stacktrace TestFind processArguments occasionally fails
No,https builds apache org job Hadoop Common trunk testReport junit org apache hadoop security authentication util TestZKSignerSecretProvider testMultipleInit Error Message expected null but was Stacktrace java lang AssertionError expected null but was at org junit Assert fail Assert java at org junit Assert failNotNull Assert java at org junit Assert assertNull Assert java at org junit Assert assertNull Assert java at org apache hadoop security authentication util TestZKSignerSecretProvider testMultipleInit TestZKSignerSecretProvider java I think the failure was introduced after HADOOP This is likely where the root cause is ERROR ZKSignerSecretProvider An unexpected exception occurred while pulling data fromZooKeeper java lang IllegalStateException instance must be started before calling this method at com google common base Preconditions checkState Preconditions java at org apache curator framework imps CuratorFrameworkImpl getData CuratorFrameworkImpl java at org apache hadoop security authentication util ZKSignerSecretProvider pullFromZK ZKSignerSecretProvider java at org apache hadoop security authentication util ZKSignerSecretProvider rollSecret ZKSignerSecretProvider java at org apache hadoop security authentication util ZKSignerSecretProvider EnhancerByMockitoWithCGLIB f d CGLIB rollSecret at org apache hadoop security authentication util ZKSignerSecretProvider EnhancerByMockitoWithCGLIB f d FastClassByMockitoWithCGLIB f a invoke at org mockito cglib proxy MethodProxy invokeSuper MethodProxy java at org mockito internal creation AbstractMockitoMethodProxy invokeSuper AbstractMockitoMethodProxy java at org mockito internal invocation realmethod CGLIBProxyRealMethod invoke CGLIBProxyRealMethod java at org mockito internal invocation realmethod FilteredCGLIBProxyRealMethod invoke FilteredCGLIBProxyRealMethod java at org mockito internal invocation Invocation callRealMethod Invocation java at org mockito internal stubbing answers CallsRealMethods answer CallsRealMethods java at org mockito internal MockHandler handle MockHandler java at org mockito internal creation MethodInterceptorFilter intercept MethodInterceptorFilter java at org apache hadoop security authentication util ZKSignerSecretProvider EnhancerByMockitoWithCGLIB f d rollSecret at org apache hadoop security authentication util RolloverSignerSecretProvider run RolloverSignerSecretProvider java at java util concurrent Executors RunnableAdapter call Executors java at java util concurrent FutureTask runAndReset FutureTask java at java util concurrent ScheduledThreadPoolExecutor ScheduledFutureTask access ScheduledThreadPoolExecutor java at java util concurrent ScheduledThreadPoolExecutor ScheduledFutureTask run ScheduledThreadPoolExecutor java at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java TestZKSignerSecretProvider testMultipleInit occasionally fail
No,Users of WASB have raised complaints over the error message returned back from WASB when they are trying to connect to Azure storage with anonymous credentials Current implementation returns the correct message when we encounter a Storage exception however for scenarios like querying to check if a container exists does not throw a StorageException but returns false when URI is directly specified Anonymous access the error message returned does not clearly state that credentials for storage account is not provided This JIRA tracks the fix the error message to return what is returned when a storage exception is hit and also correct spelling mistakes in the error message Fix exception message in WASB when connecting with anonymous credential
No,In KMSClientProvider createConnection so we don t close the same stream twice The one in the finally block may be called after an exception is thrown or not and it may throw exception too we need to be careful not to swallow exception here too Exception may be swallowed in KMSClientProvider
No,I have observed this test failure a few times in the past When it fails the expected access time of the file link is always less than the actual access time Error Message TestSymlinkLocalFSFileContext testSetTimesSymlinkToDir occasionally fail
No,I have seen this test failed a few times in the past Error Message TestMetricsSystemImpl testQSize occasionally fail
Yes,The FileContext class currently is annotated as Evolving However at this point we really need to treat it as a Stable interface FileContext and AbstractFileSystem should be annotated as a Stable interface
No,Jenkins failing on TestCompressorDecompressor The exception is being caught and converted to a fail so there is no stack trace of any value TestCompressorDecompressor failing without stack traces
No,Found this issue on HADOOP quote Tests run Failures Errors Skipped Time elapsed sec FAILURE in org apache hadoop fs shell TestCopyPreserveFlag testDirectoryCpWithP org apache hadoop fs shell TestCopyPreserveFlag Time elapsed sec ERROR java io IOException Mkdirs failed to create d exists false cwd testptch hadoop hadoop common project hadoop common target test data testStat at org apache hadoop fs ChecksumFileSystem create ChecksumFileSystem java at org apache hadoop fs ChecksumFileSystem create ChecksumFileSystem java at org apache hadoop fs FileSystem create FileSystem java at org apache hadoop fs FileSystem create FileSystem java at org apache hadoop fs FileSystem create FileSystem java at org apache hadoop fs FileSystem createNewFile FileSystem java at org apache hadoop fs shell TestCopyPreserveFlag initialize TestCopyPreserveFlag java quote Fix intermittent test failure of TestCopyPreserveFlag
No,Jenkins found this test failure on HADOOP quote Tests run Failures Errors Skipped Time elapsed sec FAILURE in org apache hadoop metrics impl TestGangliaMetrics testGangliaMetrics org apache hadoop metrics impl TestGangliaMetrics Time elapsed sec FAILURE java lang AssertionError Missing metrics test s rec Xxx at org junit Assert fail Assert java at org junit Assert assertTrue Assert java at org apache hadoop metrics impl TestGangliaMetrics checkMetrics TestGangliaMetrics java at org apache hadoop metrics impl TestGangliaMetrics testGangliaMetrics TestGangliaMetrics java quote Fix intermittent test failure of TestGangliaMetrics
Yes,The WriteableRPCEninge depends on Java s serialization mechanisms for RPC requests Without proper checks it has be shown that it can lead to security vulnerabilities such as remote code execution e g COLLECTIONS HADOOP The current implementation has migrated from WriteableRPCEngine to ProtobufRPCEngine now This jira proposes to deprecate WriteableRPCEngine in branch and to remove it in trunk Deprecate WriteableRPCEngine
No,TestRPC testClientBackOff is failing TestRPC testClientBackOff failing
No,Looking at identifier implementations e g AbstractDelegationTokenIdentifier and others I can see that getUser method can return null If debug logging is enabled this NPEs If getUser is not expected to return NULL it should either be checked and erred upon better here or the error should be allowed to happen where it would otherwise happen not in some debug log path NPE in SaslRpcServer
Yes,hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations Additionally the token files that are created use Java serializations which are hard impossible to deal with in other languages It should be replaced with a better utility in common that can read write protobuf based token files has enough flexibility to be used with other services and offers key functionality such as append and rename The old version file format should still be supported for backward compatibility but will be effectively deprecated A follow on JIRA will deprecrate fetchdt Updated utility to create modify token files
Yes,It would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project Make hadoop dockerfile usable by Yetus
No,Fix sprintf warnings in DomainSocket c introduced by HADOOP Fix sprintf warnings in DomainSocket c introduced by HADOOP
No,After HADOOP mvn package Pdist DskipTests fails on JDK noformat ERROR Exit code testptch hadoop hadoop tools hadoop aws src main java org apache hadoop fs s a BlockingThreadPoolExecutorService java error invalid uri https github ERROR This is inspired by JDK Fix javadoc error caused by illegal tag
Yes,It would be good if we could read s creds from a source other than via a java property Hadoop configuration option Read s a creds from a Credential Provider
No,open hadoop share doc hadoop api index html Click on All Classes Click on AccessControlList The page shows This page can t be displayed Same error for DistCp ImpersonationProvider and DefaultImpersonationProvider also Javadoc generated from Trunk has the same problem Hadoop javadoc has broken links for AccessControlList ImpersonationProvider DefaultImpersonationProvider and DistCp
No,HADOOP added several new tests covering functionality of resolving host names based on an alternate network interface These tests are failing on Windows TestDNS fails on Windows after HADOOP
Yes,Make the re j dependency consistent with other parts of Hadoop Seeing some weird rare failures with older versions of maven that appear to be related to this make re j dependency consistent
No,HADOOP changed the behavior of an Azure Storage lease violation during deletes It appears that TestAzureFileSystemInstrumentation testClientErrorMetrics is partly dependent on the old behavior for simulating an error to be tracked by the metrics system I am seeing intermittent failures in this test TestAzureFileSystemInstrumentation testClientErrorMetrics fails intermittently due to assumption that a lease error will be thrown
No,This issue proposes to implement the Hadoop FileSystem contract tests for hadoop azure WASB The contract tests define the expected semantics of the FileSystem so running these for hadoop azure is likely to catch potential problems and improve overall quality Run FileSystem contract tests with hadoop azure
Yes,The hadoop azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account The configuration works by overwriting the src test resources azure test xml file This can be an error prone process The azure test xml file is checked into revision control to show an example There is a risk that the tester could overwrite azure test xml containing the keys and then accidentally commit the keys to revision control This would leak the keys to the world for potential use by an attacker This issue proposes to use XInclude to isolate the keys into a separate file ignored by git which will never be committed to revision control This is very similar to the setup already used by hadoop aws for integration testing Use XInclude in hadoop azure test configuration to isolate Azure Storage account keys for service integration tests
Yes,After HADOOP we should remove metrics v from trunk Remove metrics v
No,OS X JDK has issues with localization that can cause util Shell run to fail This is fixed in JDK but JDK and JDK are still broken Fix posix spawn error on OS X
No,Running bats in a docker container on jenkins fails bash unit tests are failing
No,An originally requested URL that contains a query string gets translated into an originalURL query parameter without the original query string This can cause the redirect back to the requested resource to be invalid JWTRedirectAuthenticationHandler doesn t Retain Original Query String
No,In page TOC of each documentation page is maintained by hand now It should be automatically generated once doxia macro is supported by doxia module markdown In page TOC of documentation should be automatically generated by doxia macro
Yes,kms dt currently does not have its own token identifier class to de Support decoding KMS Delegation Token with its own Identifier
No,The sample code in Tracing md have some problems compilation error by not importing Tracer generic options are not reflected because Tracer is initialized before ToolRunner run it may be confusing to use FsShell in example because it has embedded Tracer now Fix tracing documention reflecting the update to htrace
No,TestTextCommand should use mkdirs rather than mkdir to create the test directory TestTextCommand use mkdirs rather than mkdir to create test directory
No,Clean up some htrace integration issues Clean up some htrace integration issues
Yes,FileSystem createNonRecursive is deprecated However there is no DistributedFileSystem create implementation which throws exception if parent directory doesn t exist This limits clients migration away from the deprecated method For HBase IO fencing relies on the behavior of FileSystem createNonRecursive Variant of create method should be added which throws exception if parent directory doesn t exist Undeprecate createNonRecursive
No,After HADOOP kill command s execution will be failure under Ubuntu After NM restarts it cannot get if a process is alive or not via pid of containers and it cannot kill process correctly when RM AM tells NM to kill a container Logs from NM customized logs Fix kill command behavior under some Linux distributions
No,java util regex classes have performance problems with certain wildcard patterns Namely consecutive characters in a file name not properly escaped as literals will cause commands such as hadoop fs ls file name to consume CPU and probably never return in a reasonable time time scales with number of s Here is an example Not every string of s causes this but the above filename reproduces this reliably GlobPattern regex library has performance issues with wildcard characters
No,Jenkins trunk java saw a failure of TestRPC testRPCInterruptedSimple the interrupt wasn t picked up Race in test or a surfacing of a bug in RPC where at some points interrupt exceptions are not picked up TestRPC testRPCInterruptedSimple fails intermittently
No,The TestWebDelegationToken failed on Jenkins with port in use It looks like the code searches for a free port and then starts jetty but maybe there s enough of a race condition between port location and jetty start to cause intermittent failures TestWebDelegationToken failing with port in use
No,As discovered in BIGTOP hadoop nfs module compilation is broken Looks like that HADOOP is the root cause of it hdfs and nfs builds broken on missing compile time dependency on netty
No,hadoop ipc TestSaslRPC is broken post HADOOP string coming in exception is more detailed than the test expects emergency patch in progress Test failing hadoop ipc TestSaslRPC
Yes,Because CLI is using CommandWithDestination java which add COPYING to the tail of file name when it does the copy For blobstore like S and Swift to create COPYING file and rename it is expensive direct flag can allow user to avoiding the COPYING file Add direct flag option for fs copy so that user can choose not to create COPYING file
No,There are warnings in hadoop datajoin module and warnings in hadoop ant module Fix findbugs warnings in hadoop tools module
No,Usage hadoop fs expunge Empty the Trash Refer to the HDFS Architecture Guide for more information on the Trash feature this description is confusing It gives user the impression that this command will empty trash but actually it only removes old checkpoints If user sets a pretty long value for fs trash interval this command will not remove anything until checkpoints exist longer than this value Description of hdfs expunge command is confusing
Yes,It would be useful for rd party apps to know the locations of things when hadoop is running without explicit path env vars set expose calculated paths
No,The docs at http hadoop apache org docs r hadoop project dist hadoop common FileSystemShell html getmerge say that addnl is a valid parameter but as of HADOOP it s been replaced with nl The docs should be updated hadoop fs getmerge doc is wrong
Yes,We have seen many cases with customers deleting data inadvertently with skipTrash The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though skipTrash is being used Add safely flag to rm to prompt when deleting many files
No,The CPU usage information on Windows is computed incorrectly The proposed patch fixes the issue and unifies the the interface with Linux Fix computing CPU usage statistics on Windows
No,Logging around the WASB component is very limited and it is disabled by default This improvement is created to add logging around Reads Writes and Deletes when Azure Storage Exception to capture the blobs that hit the exception This information is useful while communicating with the Azure storage team for debugging purposes WASB Logging Improve WASB Logging around deletes reads and writes
No,MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter MetricsSourceAdapter expects time unit millisecond for jmxCacheTTL but MetricsSystemImpl passes time unit second to MetricsSourceAdapter constructor MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter
Yes,HADOOP mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival The way this was achieved was by applying an intensive exponential retry when throttling occurred As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries i e we will do a client side copy of the blob and then copy it back to destination This operation will not be subject to throttling and hence should provide a stronger mitigation However it is more expensive hence we do it only in the case we fail after all retries Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries
Yes,This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs Slow RPCs are RPCs that fall at th percentile This is useful to troubleshoot why certain services like name node freezes under heavy load RPC Metrics Add the ability track and log slow RPCs
Yes,The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and even after HADOOP is not thread safe both start and stop are potentially re entrant It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle which for all Yarn services is the YARN app lifecycle as implemented in Hadoop common Making the monitor a subclass of AbstractService and moving the init start stop operations in serviceInit serviceStart serviceStop methods will fix the concurrency and state model issues and make it trivial to add as a child to any YARN service which subclasses CompositeService most the NM and RM apps will be able to hook up the monitor simply by creating one in the ctor and adding it as a child Make JvmPauseMonitor an AbstractService
No,On a debian machine we have seen node manager recovery of containers fail because the signal syntax for process group may not work We see errors in checking if process is alive during container recovery which causes the container to be declared as LOST on a NodeManager restart The application will fail with error The attempts are not retried Applications fail on NM restart on some linux distro because NM container recovery declares AM container as LOST
No,It is observed that after YARN some tests are failing in TestRMAdminService with null pointer exceptions in build build failure https builds apache org job PreCommit YARN Build artifact patchprocess testrun hadoop yarn server resourcemanager txt NPE in JvmPauseMonitor when calling stop before start
No,HADOOP fixed a bug with DelegateToFileSystem using the wrong default port As a side effect of this patch file path URLs that previously had no port now insert for the port as per the default implementation of FileSystem getDefaultPort At runtime this can cause an application to erroneously try contacting port for a remote blob store service The connection fails Ultimately this renders wasb s and probably custom file system implementations outside the Hadoop source tree completely unusable as the default file system Applications using FileContext fail with the default file system configured to be wasb s etc
No,HADOOP introduced a way to set the java static values for POSIX flags this resulted in compilation error in Windows Fix native compilation on Windows after HADOOP
No,In linux setnetgrent returns in linux when something wrong is happen such as out of memory unknown group unavailable service etc So errorMessage should be set and exception should be thrown when setnetgrent returns in linux exception should be thrown
No,provide better visibility of parsing configuration failure by logging full error message and propagate error message of parsing configuration back to client Throw an Exception when fs permissions umask mode is misconfigured
Yes,When using LdapGroupsMapping with Hadoop nested groups are not supported So for example if user jdoe is part of group A which is a member of group B the group mapping currently returns only group A Currently this facility is available with ShellBasedUnixGroupsMapping and SSSD or similar tools but would be good to have this feature as part of LdapGroupsMapping directly Add support for nested groups in LdapGroupsMapping
No,test patch sh does not give any detail on its warning report This has been seen in Jenkins run of HDFS https builds apache org job PreCommit HDFS Build artifact patchprocess newPatchFindbugsWarningshadoop hdfs html test patch sh does not give any detail on its findbugs warning report
Yes,In order to enable significantly better unit testing as well as enhanced functionality large portions of config sh should be pulled into functions See first comment for more pull argument parsing into a function
No,This is a similar issue as HADOOP and HADOOP which I found in a customer s HBase cluster logs but the piece of StorageException complaining no lease ID when updating FolderLastModifiedTime in WASB
No,HADOOP for Yetus quote releasedocmaker py doesn t work behind a proxy because urllib urlopen doesn t care environment varialibes like http proxy or https proxy quote releasedocmaker py doesn t work behind a proxy
No,In this JIRA we propose to collect disks usages on a node This JIRA is part of a larger effort of monitoring resource usages on the nodes Collect disks usages on the node
No,In this JIRA we propose to collect the network usage on a node This JIRA is part of a larger effort of monitoring resource usages on the nodes Collect network usage on the node
Yes,The protoc maven plugin currently generates new Java classes every time which means Maven always picks up changed files in the build It would be better if the protoc plugin only generated new Java classes when the source protoc files change Support for incremental generation in the protoc plugin
Yes,NetworkToplogy uses nodes with a list of children The access to these children is slow as it s a linear search NetworkTopology is not efficient adding getting removing nodes
Yes,Some of the monitoring functions could be moved from YARN to Common for easier sharing Move ResourceCalculatorPlugin from YARN to Common
No,During some tests e g https builds apache org job PreCommit HADOOP Build initial mvn install triggered a full test suite run when Jenkins switches from old test patch to new test patch This is bad copy all of test patch BINDIR prior to re exec
No,We re using syntax like if cygwin then which may be errorounsly evaluated into true if cygwin is unset We need to fix this in branch Fix unrecommended syntax usages in hadoop hdfs yarn script for cygwin in branch
Yes,Given test patch s tendency to get forked into a variety of different projects it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base Umbrella Split test patch off into its own TLP
No,Error Message Argument s are different Wanted printStream println DISK QUOTA REM DISK QUOTA SSD QUOTA REM SSD QUOTA ARCHIVE QUOTA REM ARCHIVE QUOTA PATHNAME at org apache hadoop fs shell TestCount processPathWithQuotasByQTVH TestCount java Actual invocation has different arguments printStream println SSD QUOTA REM SSD QUOTA DISK QUOTA REM DISK QUOTA ARCHIVE QUOTA REM ARCHIVE QUOTA PATHNAME at org apache hadoop fs shell Count processOptions Count java Check the following report for same https builds apache org job PreCommit HADOOP Build testReport org apache hadoop fs shell TestCount fails
No,This is a similar issue to HADOOP HADOOP happens when HBase is doing distributed log splitting This JIRA happens when HBase is deleting old WALs and trying to update hbase oldWALs folder The fix is the same as HADOOP StorageException complaining no lease ID when updating FolderLastModifiedTime in WASB
No,KMSClientProvider call validates the content type returned by going that s not going to work in all locales not if upper case is being returned KMSClientProvider uses equalsIgnoreCase application json
No,mvn package Pdist DskipTests fails with JDK by illegal tags as follows JDK Fix javadoc errors caused by incorrect or illegal tags
Yes,The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter HADOOP added a support to plug in custom authentication scheme in addition to Kerberos via AltKerberosAuthenticationHandler class But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics As per RFC http www w org Protocols rfc rfc html HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information This mechanism is initiated by server sending the Authenticate response with WWW Authenticate header which includes at least one challenge that indicates the authentication scheme s and parameters applicable to the Request URI In case server supports multiple authentication schemes it may return multiple challenges with a Authenticate response and each challenge may use a different auth scheme A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses Negotiate as the challenge as part of WWW Authenticate response header As per the following documentation Negotiate challenge scheme is only applicable to Kerberos and Windows NTLM authentication schemes SPNEGO based Kerberos and NTLM HTTP Authentication http tools ietf org html rfc Understanding HTTP Authentication https msdn microsoft com en us library ms v vs aspx On the other hand for LDAP authentication typically Basic authentication scheme is used Note TLS is mandatory with Basic authentication scheme http httpd apache org docs trunk mod mod authnz ldap html Hence for this feature the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes Kerberos via Negotiate auth challenge and LDAP via Basic auth challenge During the authentication phase it would send both the challenges and let client pick the appropriate one If client responds with an Authorization header tagged with Negotiate it will use Kerberos authentication If client responds with an Authorization header tagged with Basic it will use LDAP authentication Note some HTTP clients e g curl or Apache Http Java client need to be configured to use one scheme over the other e g curl tool supports option to use either Kerberos via negotiate flag or username password based authentication via basic and u flags Apache HttpClient library can be configured to use specific authentication scheme http hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of strength of security e g take a look at the design of Chrome browser for HTTP authentication https www chromium org developers design documents http authentication Support multiple authentication schemes via AuthenticationFilter
No,Currently the bit check in security UserGroupInformation java uses os arch and checks for s x is returned on IBM s z platform s x is bit Without this change if we try to use HDFS with Spark we get a fatal error unable to login as we can t find a login class This address fixes said issue by identifying s x as a bit platform and thus allowing Spark to run on zLinux A simple fix with very big implications Fix UserGroupInformation java to support bit zLinux
No,The default policy in RetryUtils does not retry RetriableExceptions even when defaultRetryPolicyEnabled is true This was discovered via an HDFS client failing to retry getFileBlockLocations after checkNNStartup failed The default retry policy does not handle RetriableException correctly
No,The AbstractJavaKeyStoreProvider class in the CredentialProvider API has a cache member variable and interrogation of it during access but does not populate it Incomplete Cache Mechanism in CredentialProvider API
No,Azure FileSystem PageBlobInputStream does not return on EOF This is some scenarios causes infinite hands on reading files e g copyToLocal can hang forever Azure FileSystem PageBlobInputStream does not return on EOF
No,conftest distch jnipath and trace are not enabled in hadoop cmd kerbname is enabled but does not appear in the help message Some of the bin hadoop subcommands are not available on Windows
Yes,guice doesn t work with lambda statement https github com google guice issues We should upgrade it to which includes the fix JDK Update guice version to
Yes,Right now S Credentials only works with cleartext passwords in configs as a secret access key or the URI The non URI version should use credential providers with a fallback to the clear text option S Credentials should support use of CredentialProvider
No,DiskChecker checkDirs should check null pointer for the return value from File listFiles Based on the document for File listFiles at https docs oracle com javase docs api java io File html listFiles So it will be good to check null pointer and throw DiskErrorException if it is null Use DirectoryStream in DiskChecker checkDirs to detect errors when listing a directory
Yes,Since our min version is now JDK there s hardlink support via Files This means we can deprecate the JNI implementation and discontinue usage Deprecate usage of NativeIO link
Yes,During http authentication a cookie which contains the authentication token is dropped The expiry time of the authentication token can be configured via hadoop http authentication token validity The default value is hours For clusters which require enhanced security it is desirable to have a configurable MaxInActiveInterval for the authentication token If there is no activity during MaxInActiveInterval the authentication token will be invalidated The MaxInActiveInterval will be less than hadoop http authentication token validity The default value will be minutes Enable MaxInactiveInterval for hadoop http auth token
No,t s R and u option for hadoop fs ls are introduced in HADOOP which targeted It means that the feature is unimplemented in but documented in http hadoop apache org docs r hadoop project dist hadoop common FileSystemShell html ls We should fix the document Remove unimplemented option for hadoop fs ls from document in branch
No,HADOOP and related issues implemented the parallel tests Maven profile for running JUnit tests in multiple concurrent processes This issue proposes to activate that profile during pre commit to speed up execution Enable parallel JUnit tests in pre commit
Yes,ThreadLocalRandom should be used when available in place of ThreadLocal For JDK the difference is minimal but JDK starts including optimizations for ThreadLocalRandom Replace uses of ThreadLocal with JDK ThreadLocalRandom
No,Right now the initialization of hte thread local factories for encoder decoder in Text are not marked final This means they end up with a static initializer that is not guaranteed to be finished running before the members are visible Under heavy contention this means during initialization some users will get an NPE ThreadLocal initialization in several classes is not thread safe
No,HADOOP reinstated support for running the bash scripts through Cygwin The logic involves setting a cygwin flag variable to indicate if the script is executing through Cygwin The flag is set in all of the interactive scripts hadoop hdfs yarn and mapred The flag is not set through hadoop daemon sh though This can cause an erroneous overwrite of HADOOP HOME and JAVA LIBRARY PATH inside hadoop config sh Variable cygwin is undefined in hadoop config sh when executed through hadoop daemon sh
No,Some log examples We should get rid of this kind of log in production environment even under debug log level Sasl message with MD challenge text shouldn t be LOG out even in debug level
Yes,AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client This JIRA is created to enable Azure Storage Client Side logging at the Job submission level User should be able to configure Client Side logging on a Per Job bases Enable Azure Storage Client Side logging
No,While investigating YARN it was frustrating that MetricsSystemImpl was logging a ConcurrentModificationException but without any backtrace Logging a backtrace would be very beneficial to tracking down the cause of the problem MetricsSystemImpl fails to show backtrace when an error occurs
Yes,we have the issue matching regex configurable via altering the default add in a cli arg to update it on invocation test patch s issue matching regex should be configurable
Yes,deprecate DistCpV and Logalyzer which are no longer used Deprecate DistCpV and Logalyzer
No,Some of the native components of the build are considered optional and either will not build at all without passing special flags to Maven or will allow a build to proceed if dependencies are missing from the build machine If these components do not get built then pre commit isn t really providing full coverage of the build This issue proposes to update test patch sh so that it does a full build of all native components Guarantee a full build of all native code during pre commit
No,I was attempting to use the LdapGroupsMapping gets the needed passwords and we re using the CredentialProvider so unsurprisingly we get to getPasswordFromCredentialProviders which chooses the JavaKeyStoreProvider like I told it to The JavaKeyStoreProvider in its constructor does fs path getFileSystem conf And guess what we re back in Path getFileSystem where we started at the beginning Please let me know if I ve somehow configured something incorrectly but if I have I can t figure out what it is Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop
No,We ve seen a situation that one RM hangs on stopping the MetricsSinkAdapter looks like the sinkThread interrupt in MetricsSinkAdapter stop doesn t really interrupt the thread which cause it to hang at join This appears only once MetricsSinkAdapter hangs when being stopped
No,javac result should report incremental number of warnings test patch javac warning check reporting total number of warnings instead of incremental
No,When compile hadoop with native support we encounter compile error that undefined reference to dlopen when link libcrypto We d better link libdl explicitly in CMakeList of hadoop pips Fix undefined reference to dlopen error when compiling libhadooppipes
No,cleanup and exit uses the wrong result code check and fails to mv the patchdir when it should and mv s it when it shouldn t test patch sh mv does wrong math
No,Now that we have branch switching smart apply patch sh needs to be updated so that test patch can apply patches to it backport trunk s smart apply patch sh to branch
No,It might be useful to have all of test patch sh s functionality documented how to use it power user hints etc esp for the bug bash test patch sh should be documented
No,BytesWritable setSize increases the buffer size by each time This is an unsafe operation since it restricts the max size to MB since MB GB I didn t write a test case for this case because in order to trigger this I d need to allocate around MB which is pretty expensive to do in a unit test Note that I didn t throw any exception in the case integer overflow as I didn t want to change that behavior callers to this might expect a java lang NegativeArraySizeException BytesWritable fails to support G chunks due to integer overflow
No,Saw this while building the EC branch pretty sure it ll repro on trunk though too hadoop dist dist layout stitching sh does not work with dash
No,Following will come when job failed and deletion service trying to delete the log fiels INFO org apache hadoop yarn server nodemanager DefaultContainerExecutor Deleting absolute path null ERROR org apache hadoop yarn server nodemanager DeletionService Exception during execution of task in DeletionService java lang NullPointerException at org apache hadoop fs FileContext fixRelativePart FileContext java at org apache hadoop fs FileContext delete FileContext java at org apache hadoop yarn server nodemanager DefaultContainerExecutor deleteAsUser DefaultContainerExecutor java at org apache hadoop yarn server nodemanager DeletionService FileDeletionTask run DeletionService java at java util concurrent Executors RunnableAdapter call Executors java at java util concurrent FutureTask run FutureTask java at java util concurrent ScheduledThreadPoolExecutor ScheduledFutureTask access ScheduledThreadPoolExecutor java at java util concurrent ScheduledThreadPoolExecutor ScheduledFutureTask run ScheduledThreadPoolExecutor java at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java FileContext java fixRelativePart should check for not null for a more informative exception
No,Jenkins on Java is failing due to a number of Javadoc violations that are now considered ERRORs in the following classes AuthenticationFilter java CertificateUtil java RolloverSignerSecretProvider java SignerSecretProvider java ZKSignerSecretProvider java KeyAuthorizationKeyProvider java JDK AuthenticationFilter CertificateUtil SignerSecretProviders KeyAuthorizationKeyProvider Javadoc issues
No,Jenkins on Java is failing as JWTRedirectAuthenticationHandler has tags in it something javadoc on java considers illegal JWTRedirectAuthenticationHandler breaks java javadocs
Yes,Set minimum version of trunk to JDK JDK Set minimum version of Hadoop to JDK
No,Jenkins is failing on TestCertificateUtil testCorruptPEM TestCertificateUtil testCorruptPEM failing on Jenkins JDK
Yes,As discussed with aw In AVRO a docker based solution was created to setup all the tools for doing a full build This enables much easier reproduction of any issues and getting up and running for new developers This issue is to copy port that setup into the hadoop project in preparation for the bug squash Make setting up the build environment easier
No,http hadoop apache org docs current still shows the new features of Hadoop The document should be updated Update release note in index md vm
Yes,For very large source trees on s distcp is taking long time to build file listing client code before starting mappers For a dataset I used M files K dirs it was taking minutes before my fix in HADOOP and minutes after the fix Speed up distcp buildListing using threadpool
No,Per https builds apache org job PreCommit MAPREDUCE Build artifact patchprocess newPatchFindbugsWarningshadoop sls html there are warnings to be fixed Fix findbugs warnings in hadoop sls
No,Found two issues when reviewing the patches in HADOOP There is no Test annotation so the test is not executed Clean up some test methods in TestCodec java
Yes,With the commit of HADOOP the CHANGES txt files are now EOLed We should remove them Remove all of the CHANGES txt files
No,With the commit of HADOOP we need to include the new format of release information in trunk This JIRA is about including those old versions in the tree Update src site markdown releases to include old versions of Hadoop
No,I have noticed that our pre commit builds could end up running the wrong set of unit tests for patches For instance YARN changes only YARN files but the test were run against one of the MR modules I suspect there is a race condition when there are multiple builds executing on the same node or remnants from a previous run are getting picked up Fix pre commit builds to execute the right set of tests
No,Seems like we haven t touch the API files from jdiff under dev support for a while For now we re missing the jdiff API files for hadoop We re also missing YARN when generating the jdiff API files jdiff is broken in Hadoop
No,RM fails to start in the non secure mode with the following exception This is likely a regression introduced by HADOOP RM fails to start in non secure mode due to authentication filter failure
Yes,The current way we generate these build artifacts is awful Plus they are ugly and in the case of release notes very hard to pick out what is important Rework the changelog and releasenotes
No,s n attempts to read again when it encounters IOException during read But the current logic does not reopen the connection thus it ends up with no op and committing the wrong truncated output Here s a stack trace as an example quote TezChild INFO org apache pig backend hadoop executionengine tez runtime PigProcessor Starting output org apache tez mapreduce output MROutput dbd to vertex scope TezChild DEBUG org jets t service impl rest httpclient HttpMethodReleaseInputStream Released HttpMethod as its response data stream threw an exception org apache http ConnectionClosedException Premature end of Content Length delimited message body expected received at org apache http impl io ContentLengthInputStream read ContentLengthInputStream java at org apache http conn EofSensorInputStream read EofSensorInputStream java at org jets t service io InterruptableInputStream read InterruptableInputStream java at org jets t service impl rest httpclient HttpMethodReleaseInputStream read HttpMethodReleaseInputStream java at org apache hadoop fs s native NativeS FileSystem NativeS FsInputStream read NativeS FileSystem java at java io BufferedInputStream read BufferedInputStream java at java io BufferedInputStream read BufferedInputStream java at java io DataInputStream read DataInputStream java at org apache hadoop util LineReader fillBuffer LineReader java at org apache hadoop util LineReader readDefaultLine LineReader java at org apache hadoop util LineReader readLine LineReader java at org apache hadoop mapreduce lib input LineRecordReader nextKeyValue LineRecordReader java at org apache pig builtin PigStorage getNext PigStorage java at org apache pig backend hadoop executionengine mapReduceLayer PigRecordReader nextKeyValue PigRecordReader java at org apache tez mapreduce lib MRReaderMapReduce next MRReaderMapReduce java at org apache pig backend hadoop executionengine tez plan operator POSimpleTezLoad getNextTuple POSimpleTezLoad java at org apache pig backend hadoop executionengine physicalLayer PhysicalOperator processInput PhysicalOperator java at org apache pig backend hadoop executionengine physicalLayer relationalOperators POForEach getNextTuple POForEach java at org apache pig backend hadoop executionengine physicalLayer PhysicalOperator processInput PhysicalOperator java at org apache pig backend hadoop executionengine physicalLayer relationalOperators POFilter getNextTuple POFilter java at org apache pig backend hadoop executionengine physicalLayer PhysicalOperator processInput PhysicalOperator java at org apache pig backend hadoop executionengine tez plan operator POStoreTez getNextTuple POStoreTez java at org apache pig backend hadoop executionengine tez runtime PigProcessor runPipeline PigProcessor java at org apache pig backend hadoop executionengine tez runtime PigProcessor run PigProcessor java at org apache tez runtime LogicalIOProcessorRuntimeTask run LogicalIOProcessorRuntimeTask java at org apache tez runtime task TezTaskRunner TaskRunnerCallable run TezTaskRunner java at org apache tez runtime task TezTaskRunner TaskRunnerCallable run TezTaskRunner java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache tez runtime task TezTaskRunner TaskRunnerCallable call TezTaskRunner java at org apache tez runtime task TezTaskRunner TaskRunnerCallable call TezTaskRunner java at java util concurrent FutureTask run FutureTask java at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java at java lang Thread run Thread java TezChild INFO org apache hadoop fs s native NativeS FileSystem Received IOException while reading user hadoop tsato readlarge input cloudian s log attempting to reopen TezChild DEBUG org jets t service impl rest httpclient HttpMethodReleaseInputStream Released HttpMethod as its response data stream is fully consumed TezChild INFO org apache tez dag app TaskAttemptListenerImpTezDag Commit go no go request from attempt TezChild INFO org apache tez dag app dag impl TaskImpl attempt given a go for committing the task output quote It seems this is a regression which was introduced by the following optimizations https issues apache org jira browse HADOOP https issues apache org jira browse HADOOP Also test cases should be reviewed so that it covers this scenario Regression s n read failure recovery broken
No,mvn package Pdist DskipTests fails with JDK caused by incorrect or illegal tags in doc comments JDK Fix javadoc errors caused by incorrect or illegal tags in hadoop tools
Yes,Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request Using JWT provides a number of benefits It is not tied to any specific authentication mechanism so buys us many SSO integrations It is cryptographically verifiable for determining whether it can be trusted Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing validating and parsing JWT tokens Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth
Yes,Currently ViewFileSystem does not dispatch snapshot methods through the mount table All snapshot methods throw UnsupportedOperationException even though the underlying mount points could be HDFS instances that support snapshots We need to update ViewFileSystem to implement the snapshot methods ViewFileSystem should support snapshot methods
Yes,distcpv is pretty much unsupported we should just remove it Remove DistCpV and Logalyzer
No,HADOOP covered the core s a bugs surfacing in Hadoop other enhancements to improve S performance proxy custom endpoints This JIRA covers post issues and enhancements ber jira S a phase II robustness scale and performance
No,Lots of the following messages appeared in NN log quote WARN SecurityLogger org apache hadoop ipc Server Auth failed for null DIGEST MD IO error acquiring password INFO org apache hadoop ipc Server Socket Reader for port readAndProcess from client threw exception org apache hadoop ipc StandbyException Operation category READ is not supported in state standby SecurityLogger org apache hadoop ipc Server Auth failed for null DIGEST MD IO error acquiring password INFO org apache hadoop ipc Server Socket Reader for port readAndProcess from client threw exception org apache hadoop ipc StandbyException Operation category READ is not supported in state standby quote The real reason of failure is the second message about StandbyException However the first message is confusing because it talks about DIGEST MD IO error acquiring password Filing this jira to modify the first message to have more comprehensive information that can be obtained from getCauseForInvalidToken e Improve authentication failure WARN message to avoid user confusion
No,Hadoop recently fixed x build After YARN compiling x results in error X build of libwinutils is broken
Yes,The EMC ViPR ECS object storage platform uses proprietary headers starting by x emc like Amazon does with x amz Headers starting by x emc should be included in the signature computation but it s not done by the Amazon S Java SDK it s done by the EMC S SDK When s a copy an object it copies all the headers but when the object includes x emc headers it generates a signature mismatch Removing the x emc headers from the copy would allow s a to be compatible with the EMC ViPR ECS object storage platform Removing the x which aren t x amz headers from the copy would allow s a to be compatible with any object storage platform which is using proprietary headers Ignore x and response headers when copying an Amazon S object
No,This is similar to HADOOP but in a different place During HBase distributed log splitting multiple threads will access the same folder called recovered edits However lots of places in our WASB StorageException complaining no lease ID during HBase distributed log splitting
Yes,FileUtil copyMerge is currently unused in the Hadoop source tree In branch it had been part of the implementation of the hadoop fs getmerge shell command In branch the code for that shell command was rewritten in a way that no longer requires this method Please check more details here https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment Deprecate FileUtil copyMerge
Yes,According to the discussion in HADOOP we should remove io native lib available from trunk and always use native libraries if they exist Remove io native lib available
No,Fix some issues with HTracedRESTReceiver that are resulting in unit test failures So there were two main issues better way to launch htraced fixes to the HTracedRESTReceiver logic fix TestHTracedRESTReceiver unit test failures
Yes,When a new file is added the source is dev null rather than the root of the tree which would mean a a b prefix Allow for this Allow smart apply patch sh to add new files in binary git patches
No,Convert the rest of the documentation to markdown also doing some other shell script rewrite changes along the way Convert site documentation from apt to markdown stragglers
Yes,With HADOOP now committed we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts converting them to use the new slaves option Additionally the documentation should be updated to reflect these new command options Update sbin commands and documentation to use new slaves option
No,a Execute the command yarn daemonlog setlevel xx xx xx xxx ResourceManager DEBUG b It is not reflecting in process logs even after performing client level operations c Log level is not changed Daemon log documentation is misleading
Yes,Add a slaves shell option to hadoop config sh to trigger the given command on slave nodes This is required to deprecate hadoop daemons sh and yarn daemons sh Add slaves shell option
Yes,HadoopKerberosName has been around as a secret hack for quite a while We should clean up the output and make it official by exposing it via the hadoop command Expose HadoopKerberosName as a hadoop subcommand
No,After HADOOP we need to formally document functions and environment variables that rd parties can expect to be able to exist use Formalize the shell API
Yes,An RPC server handler thread is tied up for each incoming RPC request This isn t ideal since this essentially implies that RPC operations should be short lived and most operations which could take time end up falling back to a polling mechanism Some use cases where this is useful YARN submitApplication which currently submits followed by a poll to check if the application is accepted while the submit operation is written out to storage This can be collapsed into a single call YARN allocate requests and allocations use the same protocol New allocations are received via polling The allocate protocol could be split into a request heartbeat along with a awaitResponse The request heartbeat is sent only when there s a request or on a much longer heartbeat interval awaitResponse is always left active with the RM and returns the moment something is available MapReduce Tez task to AM communication is another example of this pattern The same pattern of splitting calls can be used for other protocols as well This should serve to improve latency as well as reduce network traffic since the keep alive heartbeat can be sent less frequently I believe there s some cases in HDFS as well where the DN gets told to perform some operations when they heartbeat into the NN Allow handoff on the server side for RPC requests
No,checknative should display a nicer error message when openssl support is not compiled in Currently it displays this checknative should display a nicer error message when openssl support is not compiled in
No,HADOOP made a change to include exception h in NativeIO c This header includes use of the non standard gcc attribute declaration and thus fails compilation on Windows hadoop common native compilation fails on Windows due to missing support for attribute declaration
No,HDFS builds are failing in jenkins Checkstyle failing Unable to instantiate DoubleCheckedLockingCheck
Yes,We had an application sitting on top of Hadoop and got problems using jsch once we switched to java Got this exception Upgrading to jsch from jsch fixed the issue for us but then it got in conflict with hadoop s jsch version we fixed this for us by jarjar ing our jsch version So i think jsch got introduce by namenode HA HDFS So you guys should check if the ssh part is properly working for java or preventively upgrade the jsch lib to jsch Some references to problems reported http sourceforge net p jsch mailman jsch users thread loom T post gmane org https issues apache org bugzilla show bug cgi id Upgrade jsch lib to jsch to avoid problems running on java
Yes,This should be hdfs dfsadmin and yarn rmadmin ServiceLevelAuth still references hadoop dfsadmin mradmin
Yes,It would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure This would allow us to pull the HDFS MapReduce and YARN shell functions out of hadoop functions sh Additionally it should let rd parties such as HBase influence things like classpaths at runtime Pluggable shell integration
Yes,Along the same vein as HADOOP there are now several remaining usages of guava APIs that are now incompatible with a more recent version e g This JIRA proposes eliminating those usages With this the hadoop base compatible with guava Remove some uses of obsolete guava APIs from the hadoop codebase
Yes,It is a very common shell pattern in x to effectively replace sub project specific vars with generics We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated Additionally we should use this shell function to deprecate the shell vars that are holdovers already Deprecate shell vars
No,Tried on hadoop src branch and branch trunk win All gave this error ERROR Failed to execute goal org apache maven plugins maven javadoc plugin jar module javadocs on project hadoop annotations MavenReportException Error while creating archive ERROR Exit code E Projects hadoop common hadoop common project hadoop annotations src main java org apache hadoop classification InterfaceStability java error unexpected end tag ERROR ERROR ERROR ERROR Command line was C Program Files Java jdk jre bin javadoc exe options packages ERROR ERROR Refer to the generated Javadoc files in E Projects hadoop common hadoop common project hadoop annotations target dir ERROR Help ERROR ERROR To see the full stack trace of the errors re run Maven with the e switch ERROR Re run Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please read the following articles ERROR Help http cwiki apache org confluence display MAVEN MojoExecutionException ERROR ERROR After correcting the problems you can resume the build with the command ERROR mvn rf hadoop annotations JDK Cannot build on Windows error unexpected end tag
No,That text shouldn t be there Remove the version and author information from distcp s README file
No,The Building on Windows section of BUILDING txt has an obsolete reference to Cygwin It should be removed to avoid confusion Remove obsolete reference to Cygwin in BUILDING txt
No,From following c besides default Hadoop will always treat it as defaultClazz I think it is a bug Please let me know if this is a work as design thing Thanks private static final String defaultClazz org apache hadoop io compress LzoCodec public synchronized boolean isSupported if checked checked true String extClazz conf get CONF LZO CLASS null System getProperty CONF LZO CLASS null String clazz extClazz null extClazz defaultClazz Property io compression codec lzo class does not work with other value besides default
No,HADOOP removes org apache hadoop fs permission AccessControlException causing build break on Hive when compiling against hadoop Hive build failure on hadoop due to HADOOP
No,In HADOOP the documentation source files for hadoop aws were moved from src site to src main site The build is no longer actually generating the HTML site from these source files because src site is the expected path hadoop aws documentation missing
No,Today Windows and parts of the Hadoop source as the Official tm variable still honoring HADOOP PREFIX if it is set Revert HADOOP PREFIX go back to HADOOP HOME
No,As part of HADOOP large extremely useful sections of the Rack Awareness documentation that was added in HADOOP was wiped out We should restore it as a separate document Restore Rack Awareness documentation
Yes,The o a h fs permission AccessControlException has been deprecated for last major releases and it should be removed Removed deprecated o a h fs permission AccessControlException
Yes,The system should be able to read in user defined env vars from hadooprc Add support for hadooprc
No,For some users the spnego token too large for the default header buffer used by Jetty Though the issue is fixed for HTTP connections via HADOOP HTTPS connections needs to be fixed as well The size of header buffer of HttpServer is too small when HTTPS is enabled
Yes, Rewrite sls rumen to use new shell framework
No,Running terasort with the following options hadoop jar hadoop mapreduce examples jar terasort Dio native lib available false Dmapreduce map output compress true Dmapreduce map output compress codec org apache hadoop io compress GzipCodec tmp tera in tmp tera out The job failed with the reducer failed to fetch the output from mappers see the following stacktrace The problem is that in JIRA MAPREDUCE it added support to handle null compressors to default to non compressed output In this case when the io native lib available is set to false the compressor will be null However the decompressor has a Java implementation so when the reducer tries to read the mapper output it uses the decompressor but the output does not have the Gzip header WARN fetcher org apache hadoop mapreduce task reduce Fetcher Failed to shuffle output of attempt m from bdvs java io IOException not a gzip file at org apache hadoop io compress zlib BuiltInGzipDecompressor processBasicHeader BuiltInGzipDecompressor java at org apache hadoop io compress zlib BuiltInGzipDecompressor executeHeaderState BuiltInGzipDecompressor java at org apache hadoop io compress zlib BuiltInGzipDecompressor decompress BuiltInGzipDecompressor java at org apache hadoop io compress DecompressorStream decompress DecompressorStream java at org apache hadoop io compress DecompressorStream read DecompressorStream java at org apache hadoop io IOUtils readFully IOUtils java at org apache hadoop mapreduce task reduce InMemoryMapOutput shuffle InMemoryMapOutput java at org apache hadoop mapreduce task reduce Fetcher copyMapOutput Fetcher java at org apache hadoop mapreduce task reduce Fetcher copyFromHost Fetcher java at org apache hadoop mapreduce task reduce Fetcher run Fetcher java Mapreduce Job Failed due to failure fetching mapper output on the reduce side
No,I found some of our DataNodes will run exceeds the limit of concurrent xciever the limit is K After check the stack I suspect that org apache hadoop net unix DomainSocket writeArray which called by DomainSocketWatcher kick stuck quote DataXceiver for client unix var run hadoop hdfs dn Waiting for operation daemon prio tid x f c nid x d waiting on condition x f d d java lang Thread State WAITING parking at sun misc Unsafe park Native Method parking to wait for a java util concurrent locks ReentrantLock NonfairSync at java util concurrent locks LockSupport park LockSupport java at java util concurrent locks AbstractQueuedSynchronizer parkAndCheckInterrupt AbstractQueuedSynchronizer java at java util concurrent locks AbstractQueuedSynchronizer acquireQueued AbstractQueuedSynchronizer java at java util concurrent locks AbstractQueuedSynchronizer acquire AbstractQueuedSynchronizer java at java util concurrent locks ReentrantLock NonfairSync lock ReentrantLock java at java util concurrent locks ReentrantLock lock ReentrantLock java at org apache hadoop net unix DomainSocketWatcher add DomainSocketWatcher java at org apache hadoop hdfs server datanode ShortCircuitRegistry createNewMemorySegment ShortCircuitRegistry java at org apache hadoop hdfs server datanode DataXceiver requestShortCircuitShm DataXceiver java at org apache hadoop hdfs protocol datatransfer Receiver opRequestShortCircuitShm Receiver java at org apache hadoop hdfs protocol datatransfer Receiver processOp Receiver java at org apache hadoop hdfs server datanode DataXceiver run DataXceiver java DataXceiver for client unix var run hadoop hdfs dn Waiting for operation daemon prio tid x f de c nid x b runnable x f db c java lang Thread State RUNNABLE at org apache hadoop net unix DomainSocket writeArray Native Method at org apache hadoop net unix DomainSocket access DomainSocket java at org apache hadoop net unix DomainSocket DomainOutputStream write DomainSocket java at org apache hadoop net unix DomainSocketWatcher kick DomainSocketWatcher java at org apache hadoop net unix DomainSocketWatcher add DomainSocketWatcher java at org apache hadoop hdfs server datanode ShortCircuitRegistry createNewMemorySegment ShortCircuitRegistry java at org apache hadoop hdfs server datanode DataXceiver requestShortCircuitShm DataXceiver java at org apache hadoop hdfs protocol datatransfer Receiver opRequestShortCircuitShm Receiver java at org apache hadoop hdfs protocol datatransfer Receiver processOp Receiver java at org apache hadoop hdfs server datanode DataXceiver run DataXceiver java at java lang Thread run Thread java DataXceiver for client unix var run hadoop hdfs dn Waiting for operation daemon prio tid x f c nid x a waiting on condition x f d d java lang Thread State WAITING parking at sun misc Unsafe park Native Method parking to wait for a java util concurrent locks AbstractQueuedSynchronizer ConditionObject at java util concurrent locks LockSupport park LockSupport java at java util concurrent locks AbstractQueuedSynchronizer ConditionObject await AbstractQueuedSynchronizer java at org apache hadoop net unix DomainSocketWatcher add DomainSocketWatcher java at org apache hadoop hdfs server datanode ShortCircuitRegistry createNewMemorySegment ShortCircuitRegistry java at org apache hadoop hdfs server datanode DataXceiver requestShortCircuitShm DataXceiver java at org apache hadoop hdfs protocol datatransfer Receiver opRequestShortCircuitShm Receiver java at org apache hadoop hdfs protocol datatransfer Receiver processOp Receiver java at org apache hadoop hdfs server datanode DataXceiver run DataXceiver java at java lang Thread run Thread java Thread daemon prio tid x f c c nid x runnable x f aef e java lang Thread State RUNNABLE at org apache hadoop net unix DomainSocketWatcher doPoll Native Method at org apache hadoop net unix DomainSocketWatcher access DomainSocketWatcher java at org apache hadoop net unix DomainSocketWatcher run DomainSocketWatcher java at java lang Thread run Thread java quote Fix deadlock in DomainSocketWatcher when the notification pipe is full
No,In ZKFailoverController java the Exception caught by the run method does not have a single error log This causes latent problems that are only manifested during failover h The problem we encountered An Exception is thrown from the doRun method during initHM caused by a configuration error If you want to repeat you can set ha health monitor connect retry interval ms to be any nonsensical value Thanks ZKFailoverController does not log Exception when doRun raises errors
No,See details in INFRA Per abayer and cnauroth s feedback there I m creating this jira to investigate the possible bug in dev support test patch sh script Thanks Andrew and Chris Submitting a hadoop patch doesn t trigger jenkins test run
No,In FileSystemShell apt vm Now stat accepts the below formats b Size of file in blocks g Group name of owner n Filename o Block size r replication u User name of owner y UTC date as yyyy MM dd HH mm ss Y Milliseconds since January UTC They should be documented Update the document for hadoop fs stat
No,The command fails because following files include non ascii characters ComparableVersion java CommonConfigurationKeysPublic java ComparableVersion java javadoc mnt build hadoop src hadoop common project hadoop common src main java org apache hadoop fs CommonConfigurationKeysPublic java error unmappable character for encoding ASCII javadoc mvn package Pdist docs DskipTests Dtar fails because of non ascii characters
No,After HADOOP uppercase key names aren t allowed breaking some unit tests Let s fix them Fix unit tests to not use uppercase key names
No,hadoop daemons sh throws command not found hadoop daemons sh is mainly used to start the cluster for ex start dfs sh Without this cluster will not be able to start hadoop daemons sh throws host bash host command not found
No,Hadoop uses an older version of Jetty that allows SSLv We should fix it up Patch up Jetty to disable SSLv
No,I checked several download mirrors They all seem to be missing the common folder The only thing I see there is dist hadoop chukwa This is a blocker since I can t download Hadoop at all Hadoop Common directory is missing from all download mirrors I checked
No,This JIRA is to fix javac warnings which are overlooked in HADOOP format a WARNING home jenkins jenkins slave workspace PreCommit HADOOP Build hadoop hdfs project hadoop hdfs nfs src main java org apache hadoop hdfs nfs conf NfsConfiguration java deprecation NFS USERGROUP UPDATE MILLIS KEY in org apache hadoop nfs nfs Nfs Constant has been deprecated WARNING home jenkins jenkins slave workspace PreCommit HADOOP Build hadoop hdfs project hadoop hdfs nfs src main java org apache hadoop hdfs nfs conf NfsConfiguration java deprecation NFS STATIC MAPPING FILE KEY in org apache hadoop nfs nfs Nfs Constant has been deprecated format Fix a couple javac warnings in NFS
Yes,As hadoop will drop the support of Java the jenkins slaves should be compiling code using Java Move jenkins to Java
No,https builds apache org job PreCommit YARN Build console https builds apache org job PreCommit YARN Build console https builds apache org job PreCommit YARN Build console https builds apache org job PreCommit MAPREDUCE Build console A couple jenkins build failure for the same reason It seems to have been broken by HADOOP Jenkins build seems to be broken by changes in test patch sh
Yes,During heavy shuffle packet loss for IPC packets was observed from a machine Avoid packet loss and speed up transfer by using x QOS bits for the packets Add a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY
No,It s should be good to make Jenkins verify mvn site if the patch contains apt vm changes to avoid some obvious build failure such as YARN It s not the first time that the similar issues have been raised Having an automative verification can inform us an alert before us encounter an actual build failure which involves site lifecycle Jenkins should verify mvn site if the patch contains apt vm changes
No,Per discussion in HDFS creating this jira Thanks aw for the work on HDFS Replace daemon with better name in script subcommands
Yes,DistCp uses ThrottleInputStream which provides a bandwidth throttling on a specified stream Currently Distcp allows the max bandwidth value in Mega Bytes which does not accept fractional values It would be better if it accepts the Max Bandwitdh in fractional MegaBytes Due to this we are not able to throttle the bandwidth in KBs in our prod setup Allow ditscp to accept bandwitdh in fraction MegaBytes
No,When trying Tez out on Windows the tez tar gz failed to localize on Windows env Also reproduced the same issue with a similar tarball when used with distributed cache with an MR sleep job Tarball as local resource type archive fails to localize on Windows
No,When a ZKDTSM tries to renew a token created by a peer It throws an Exception with message bar is trying to renew a token with wrong password ZKDelegationTokenSecretManager fails to renewToken created by a peer
No,KMS DelegationToken operation keeps throwing unable to find valid certification path to requested target It looks very much like the truststore is not being picked up Fix DelegationTokenAuthenticatedURL to pass the connection Configurator to the authenticator
No,TestFairCallQueue testPutBlocksWhenAllFull fails on trunk and branch TestFairCallQueue fails
Yes,Hadoop currently supports one JVM defined through JAVA HOME Since multiple JVMs Java are active it will be helpful if there is an user configuration to choose the custom but supported JVM for her job In other words user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM Allow user to choose JVM for container execution
No,HADOOP adds ZK implementation for DelegationTokenSecretManager This is a follow up JIRA to address review comments there findbugs and order of updates to the currentKey Fix findbugs in ZK DelegationTokenSecretManagers
No, Site build is broken
Yes,Java is coming quickly to various clusters Making sure Hadoop seamlessly works with Java is important for the Apache community This JIRA is to track the issues experiences encountered during Java migration If you find a potential bug please create a separate JIRA either as a sub task or linked into this JIRA If you find a Hadoop or JVM configuration tuning you can create a JIRA as well Or you can add a comment here Umbrella Support Java in Hadoop
No,There are quite a few Java properties that are expected to be set by the shell code These are currently undocumented Document hadoop properties expected to be set by the shell code in env sh
No,When using the TokenDelegationAuthenticationFilter I noticed if I don t specify the hosts for a user groups proxy user and then try to authenticate I get an NPE rather than an AuthorizationException NPE if hosts not specified in ProxyUsers
No,The private native method names and signatures in NativeCrc were changed in HDFS as a result hadoop common JARs get unsatisifed link errors when they try to perform checksums This essentially stops Hadoop applications running on Hadoop unless rebuilt and repackaged with the hadoop JARs UnsatisifedLinkError with hadoop JARs on hadoop due to NativeCRC method changes
No,Windows has a maximum path length of characters KMS includes several long class file names During packaging and creation of the distro these paths get even longer because of prepending the standard war directory structure and our share hadoop etc structure The end result is that the final paths are longer than characters making it impossible to deploy a distro on Windows KMS cannot deploy on Windows because class names are too long
No,If HADOOP CONF DIR is defined but points to a directory that either doesn t exist or isn t actually a viable configuration directory all sorts of weird things happen especially for logging The shell code should do a better job of verifying the directory is valid and exit if it detects if it is broken in some way Missing HADOOP CONF DIR generates strange results
No,In CryptoInputStream for int read ByteBuffer buf if there is unread value in outBuffer then the current return value is incorrect This case happens when caller uses bytes array read firstly and then do the ByteBuffer read Return value of read ByteBuffer buf in CryptoInputStream is incorrect in some cases
No,distcp on mr branch fails with NPE using a short relative source path The failure is at DistCp java makeRelative return null at the following The solution is change root to full path to match child getPath distcp on mr branch fails with NPE using a short relative source path
No,Running start dfs sh script should pick the java specified by JAVA HOME which is defined in my zshrc and bashrc My JAVA HOME is It is expected to use JDK instead This bug only occurs on trunk but not branch shell scripts ignore JAVA HOME on OS X
Yes,There is little to no reason for it to call hadoop daemon sh anymore hadoop daemons sh should just call hdfs directly
Yes,As part of HADOOP java execution across many different shell bits were consolidated down to effectively two routines Prior to calling those two routines the CLASSPATH is exported This export should really be getting handled in the exec function and not in the individual shell bits Additionally it would be good if there was so that bash x would show the content of the classpath or even a debug classpath option that would echo the classpath to the screen prior to java exec to help with debugging CLASSPATH handling should be consolidated debuggable
No,List HADOOP PREFIX bin hadoop fs ls user ericp foo rw ericp hdfs user ericp foo Cat HADOOP PREFIX bin hadoop fs cat user ericp foo Text HADOOP PREFIX bin hadoop fs text user ericp foo text java io EOFException at java io DataInputStream readShort DataInputStream java at org apache hadoop fs shell Display Text getInputStream Display java at org apache hadoop fs shell Display Cat processPath Display java at org apache hadoop fs shell Command processPaths Command java at org apache hadoop fs shell Command processPathArgument Command java at org apache hadoop fs shell Command processArgument Command java at org apache hadoop fs shell Command processArguments Command java at org apache hadoop fs shell Command processRawArguments Command java at org apache hadoop fs shell Command run Command java at org apache hadoop fs FsShell run FsShell java at org apache hadoop util ToolRunner run ToolRunner java at org apache hadoop util ToolRunner run ToolRunner java at org apache hadoop fs FsShell main FsShell java hadoop fs text of zero length file causes EOFException
No,We want the precommit builds to run against the git repo after the transition Fix test patch to work with the git repo
No,When HAServiceProtocol monitorHealth throws a HealthCheckFailedException the actual exception from protocol buffer RPC is a RemoteException that wraps the real exception Thus the state is incorrectly transitioned to SERVICE NOT RESPONDING HAServiceProtocol s health state is incorrectly transitioned to SERVICE NOT RESPONDING
No,INFO INFO maven antrun plugin run make hadoop common INFO Executing tasks main exec The C compiler identification is GNU exec The CXX compiler identification is GNU exec Check for working C compiler usr bin cc exec Check for working C compiler usr bin cc works exec Detecting C compiler ABI info exec Detecting C compiler ABI info done exec Check for working CXX compiler usr bin c exec Check for working CXX compiler usr bin c works exec JAVA HOME JAVA JVM LIBRARY JAVA JVM LIBRARY NOTFOUND exec JAVA INCLUDE PATH usr lib jvm java openjdk include JAVA INCLUDE PATH usr lib jvm java openjdk include linux exec CMake Error at JNIFlags cmake MESSAGE exec Failed to find a viable JVM installation under JAVA HOME exec Call Stack most recent call first exec CMakeLists txt include exec exec exec Detecting CXX compiler ABI info exec Detecting CXX compiler ABI info done exec Configuring incomplete errors occurred exec See also root bigtop build hadoop rpm BUILD hadoop src hadoop common project hadoop common target native CMakeFiles CMakeOutput log INFO INFO Reactor Summary INFO INFO Apache Hadoop Main SUCCESS s INFO Apache Hadoop Project POM SUCCESS s INFO Apache Hadoop Annotations SUCCESS s INFO Apache Hadoop Assemblies SUCCESS s INFO Apache Hadoop Project Dist POM SUCCESS s INFO Apache Hadoop Maven Plugins SUCCESS s INFO Apache Hadoop MiniKDC SUCCESS s INFO Apache Hadoop Auth SUCCESS s INFO Apache Hadoop Auth Examples SUCCESS s INFO Apache Hadoop Common FAILURE s INFO Apache Hadoop NFS SKIPPED INFO Apache Hadoop Common Project SKIPPED INFO Apache Hadoop HDFS SKIPPED INFO Apache Hadoop HttpFS SKIPPED INFO Apache Hadoop HDFS BookKeeper Journal SKIPPED INFO Apache Hadoop HDFS NFS SKIPPED INFO Apache Hadoop HDFS Project SKIPPED INFO hadoop yarn SKIPPED INFO hadoop yarn api SKIPPED INFO hadoop yarn common SKIPPED INFO hadoop yarn server SKIPPED INFO hadoop yarn server common SKIPPED INFO hadoop yarn server nodemanager SKIPPED INFO hadoop yarn server web proxy SKIPPED INFO hadoop yarn server resourcemanager SKIPPED INFO hadoop yarn server tests SKIPPED INFO hadoop yarn client SKIPPED INFO hadoop yarn applications SKIPPED INFO hadoop yarn applications distributedshell SKIPPED INFO hadoop yarn applications unmanaged am launcher SKIPPED INFO hadoop yarn site SKIPPED INFO hadoop yarn project SKIPPED INFO hadoop mapreduce client SKIPPED INFO hadoop mapreduce client core SKIPPED INFO hadoop mapreduce client common SKIPPED INFO hadoop mapreduce client shuffle SKIPPED INFO hadoop mapreduce client app SKIPPED INFO hadoop mapreduce client hs SKIPPED INFO hadoop mapreduce client jobclient SKIPPED INFO hadoop mapreduce client hs plugins SKIPPED INFO Apache Hadoop MapReduce Examples SKIPPED INFO hadoop mapreduce SKIPPED INFO Apache Hadoop MapReduce Streaming SKIPPED INFO Apache Hadoop Distributed Copy SKIPPED INFO Apache Hadoop Archives SKIPPED INFO Apache Hadoop Rumen SKIPPED INFO Apache Hadoop Gridmix SKIPPED INFO Apache Hadoop Data Join SKIPPED INFO Apache Hadoop Extras SKIPPED INFO Apache Hadoop Pipes SKIPPED INFO Apache Hadoop OpenStack support SKIPPED INFO Apache Hadoop Client SKIPPED INFO Apache Hadoop Mini Cluster SKIPPED INFO Apache Hadoop Scheduler Load Simulator SKIPPED INFO Apache Hadoop Tools Dist SKIPPED INFO Apache Hadoop Tools SKIPPED INFO Apache Hadoop Distribution SKIPPED INFO INFO BUILD FAILURE INFO INFO Total time s INFO Finished at T INFO Final Memory M M INFO ERROR Failed to execute goal org apache maven plugins maven antrun plugin run make on project hadoop common An Ant BuildException has occured exec returned ERROR around Ant part in root bigtop build hadoop rpm BUILD hadoop src hadoop common project hadoop common target antrun build main xml ERROR Help ERROR ERROR To see the full stack trace of the errors re run Maven with the e switch ERROR Re run Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please read the following articles ERROR Help http cwiki apache org confluence display MAVEN MojoExecutionException ERROR ERROR After correcting the problems you can resume the build with the command ERROR mvn rf hadoop common error Bad exit status from var tmp rpm tmp suXMUs build hadoop native build fails to detect java libarch on ppc le
No,After HADOOP hadoop common native compilation broken in windows Hadoop Common native compilation broken in windows
No,In org apache hadoop io nativeio NativeIO java the posix fadvise flag parameter is hardcoded to the most common values in fcntl h However not all architectures use the same values in this case System z Linux A better approach would be to not make assumptions about fcntl h values or any other system constants This bug results in calls to posix fadvise failing in zLinux Flags for posix fadvise are not valid in some architectures
No,The globber will sometimes erroneously return a permission denied exception when there is a non terminal wildcard The existing unit tests don t catch this because it doesn t happen for superusers The globber will sometimes erroneously return a permission denied exception when there is a non terminal wildcard
No,The create release script doesn t include docs in the binary tarball We should fix that Fix create release script to include docs and necessary txt files
Yes,Post HADOOP we need to rework how heap is configured for small footprint machines deprecate some options introduce new ones for greater flexibility rework heap management vars
Yes,Write a metrics sink plugin for Hadoop to send metrics directly to Apache Kafka in addition to the current Graphite Hadoop https issues apache org jira browse HADOOP Ganglia and File sinks metrics sink plugin for Apache Kafka
No,Providing a digit octal number for fs permissions leads to a parse error e g Dfs permissions umask mode digit octal umask permissions throws a parse error
No,A null is passed to the impersonation providers for the remote address if it is unresolvable DefaultImpersationProvider will NPE ipc will close the connection immediately correct behavior for such unexpected exceptions client fails on EOFException Proxy user verification NPEs if remote host is unresolvable
No,Touchz ing a file results in a Null Pointer Exception Need to set version name correctly before decrypting EEK
No,HDFS introduced a new native code function for creating hard links The Windows implementation of this function does not compile due to an incorrect call to CreateHardLink Compilation fails in native link function on Windows
No,JMXJsonServlet doGet has the following check Because of this the JMX servlet fails to work in KMS JMXJsonServlet fails when used within Tomcat
No,The release build fails because of an obscure findbugs error Testing reveals that this is related to the findbugs heap size Increase findbugs maxHeap size
No,Failure message TestDFVariations testMount fails intermittently
No,A bunch of TestPathData tests failed intermittently e g https builds apache org job PreCommit HDFS Build testReport Example failure log TestPathData fails intermittently with Mkdirs failed to create d
No,Symlink tests failure happened from time to time https builds apache org job PreCommit HDFS Build testReport https builds apache org job PreCommit HDFS Build testReport RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non curly quotes
No,While JavaKeyStoreProvider is thread safe UserProvider is not thread safe UserProvider is not thread safe
Yes,KMS and HttpFS are using Tomcat we should move it to to get bug fixes and security fixes We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS Update Tomcat version used by HttpFS and KMS to latest x version
No,Now Jenkins is failing with the following message https builds apache org job PreCommit HADOOP Build console Jenkins is failing due to the upgrade of svn client
No,Documents for FileSystem API definition were created in HADOOP but not linked In hadoop project src site site xml Fix dead link in site xml
No,Although the releases page is current the root index html page http hadoop apache org index html stops at release http hadoop apache org index html only lists versions up to
Yes,kms was not rewritten to use the new shell framework It should be reworked to take advantage of it Rewrite kms to use new shell framework
Yes,We should make an effort to clean up the shell env var name space by removing unsafe variables See comments for list Rename remove non HADOOP etc from the shell scripts
No,I am trying hadoop on FreeBSD stable namenode starts up but after first datanode contacts it it throws an exception All limits seem to be high enough limits a Resource limits current cputime infinity secs filesize infinity kB datasize kB stacksize kB coredumpsize infinity kB memoryuse infinity kB memorylocked infinity kB maxprocesses openfiles sbsize infinity bytes vmemoryuse infinity kB pseudo terminals infinity swapuse infinity kB S usr local openjdk bin java Dproc namenode Xmx m Dhadoop log dir var log hadoop Dhadoop log file hadoop hdfs namenode nezabudka log Dhadoop home dir usr local Dhadoop id str hdfs Dhadoop root logger INFO RFA Dhadoop policy file hadoop policy xml Djava net preferIPv Stack true Xmx m Xms m Djava library path usr local lib Xmx m Xms m Djava library path usr local lib Xmx m Xms m Djava library path usr local lib Dhadoop security logger INFO RFAS org apache hadoop hdfs server namenode NameNode From the namenode s log WARN IPC Server handler on ipc Server Server java run IPC Server handler on call org apache hadoop hdfs server protocol Datano deProtocol versionRequest from Call Retry java lang OutOfMemoryError at org apache hadoop security JniBasedUnixGroupsMapping getGroupsForUser Native Method at org apache hadoop security JniBasedUnixGroupsMapping getGroups JniBasedUnixGroupsMapping java at org apache hadoop security JniBasedUnixGroupsMappingWithFallback getGroups JniBasedUnixGroupsMappingWithFallback java at org apache hadoop security Groups getGroups Groups java at org apache hadoop security UserGroupInformation getGroupNames UserGroupInformation java at org apache hadoop hdfs server namenode FSPermissionChecker FSPermissionChecker java at org apache hadoop hdfs server namenode FSNamesystem getPermissionChecker FSNamesystem java at org apache hadoop hdfs server namenode FSNamesystem checkSuperuserPrivilege FSNamesystem java at org apache hadoop hdfs server namenode NameNodeRpcServer versionRequest NameNodeRpcServer java at org apache hadoop hdfs protocolPB DatanodeProtocolServerSideTranslatorPB versionRequest DatanodeProtocolServerSideTranslatorPB java at org apache hadoop hdfs protocol proto DatanodeProtocolProtos DatanodeProtocolService callBlockingMethod DatanodeProtocolProtos java at org apache hadoop ipc ProtobufRpcEngine Server ProtoBufRpcInvoker call ProtobufRpcEngine java at org apache hadoop ipc RPC Server call RPC java at org apache hadoop ipc Server Handler run Server java at org apache hadoop ipc Server Handler run Server java at java security AccessController doPrivileged Native Method at javax security auth Subject doAs Subject java at org apache hadoop security UserGroupInformation doAs UserGroupInformation java at org apache hadoop ipc Server Handler run Server java I did not have such an issue with hadoop hadoop user info alloc fails on FreeBSD due to incorrect sysconf use
Yes,Storm would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS STORM But to do so we need to open up access to some of APIs Most notably FileSystem addDelegationTokens Token renew Credentials getAllTokens and UserGroupInformation but there may be others At a minimum adding in storm to the list of allowed API users But ideally making them public Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR or tools that reuse MR input formats Open up already widely used APIs for delegation token fetching renewal to ecosystem projects
No,mvn compile Pnative ends like this ERROR Failed to execute goal on project hadoop hdfs Could not resolve dependencies for project org apache hadoop hadoop hdfs jar Failure to find org apache hadoop hadoop common jar tests in https repository apache org content repositories snapshots was cached in the local repository resolution will not be reattempted until the update interval of apache snapshots https has elapsed or updates are forced Help Unable to build hadoop FreeBSD
No,Lz Compression fails to identify the PowerPC Little Endian Architecture It recognizes it as Big Endian and several testcases TestCompressorDecompressor TestCodec TestLz CompressorDecompressor fails due to this Running org apache hadoop io compress TestCompressorDecompressor Tests run Failures Errors Skipped Time elapsed sec but was at org junit internal ComparisonCriteria arrayEquals ComparisonCriteria java at org junit Assert internalArrayEquals Assert java at org junit Assert assertArrayEquals Assert java at org apache hadoop io compress CompressDecompressTester CompressionTestStrategy assertCompression CompressDecompressTester java at org apache hadoop io compress CompressDecompressTester test CompressDecompressTester java at org apache hadoop io compress TestCompressorDecompressor testCompressorDecompressor TestCompressorDecompressor java LZ Compression fails to recognize PowerPC Little Endian Architecture
No,Renaming a file to another existing filename says File exists but colliding with a file in a directory results in the cryptic Input output error Renaming a file into a directory containing the same filename results in a confusing I O error
No,When user want to start NameNode user would got the following exception it is caused by missing org mortbay jetty jsp jetty jar in the pom xml INFO http HttpServer Added global filter safety class org apache hadoop http HttpServer QuotingInputFilter INFO http HttpServer Added filter static user filter class org apache hadoop http lib StaticUserWebFilter StaticUserFilter to context hdfs INFO http HttpServer Added filter static user filter class org apache hadoop http lib StaticUserWebFilter StaticUserFilter to context static INFO http HttpServer Added filter static user filter class org apache hadoop http lib StaticUserWebFilter StaticUserFilter to context logs INFO http HttpServer Added filter org apache hadoop hdfs web AuthFilter class org apache hadoop hdfs web AuthFilter INFO http HttpServer addJerseyResourcePackage packageName org apache hadoop hdfs server namenode web resources org apache hadoop hdfs web resources pathSpec webhdfs v INFO http HttpServer Jetty bound to port INFO mortbay log jetty INFO mortbay log NO JSP Support for did not find org apache jasper servlet JspServlet WARN mortbay log EXCEPTION java net ConnectException Connection timed out at java net PlainSocketImpl socketConnect Native Method at java net PlainSocketImpl doConnect PlainSocketImpl java at java net PlainSocketImpl connectToAddress PlainSocketImpl java at java net PlainSocketImpl connect PlainSocketImpl java at java net SocksSocketImpl connect SocksSocketImpl java at java net Socket connect Socket java at java net Socket connect Socket java at sun net NetworkClient doConnect NetworkClient java at sun net www http HttpClient openServer HttpClient java at sun net www http HttpClient openServer HttpClient java at sun net www http HttpClient HttpClient java at sun net www http HttpClient New HttpClient java at sun net www http HttpClient New HttpClient java at sun net www protocol http HttpURLConnection getNewHttpClient HttpURLConnection java at sun net www protocol http HttpURLConnection plainConnect HttpURLConnection java at sun net www protocol http HttpURLConnection connect HttpURLConnection java at sun net www protocol http HttpURLConnection getInputStream HttpURLConnection java at com sun org apache xerces internal impl XMLEntityManager setupCurrentEntity XMLEntityManager java at com sun org apache xerces internal impl XMLEntityManager startEntity XMLEntityManager java at com sun org apache xerces internal impl XMLEntityManager startDTDEntity XMLEntityManager java at com sun org apache xerces internal impl XMLDTDScannerImpl setInputSource XMLDTDScannerImpl java at com sun org apache xerces internal impl XMLDocumentScannerImpl DTDDriver dispatch XMLDocumentScannerImpl java at com sun org apache xerces internal impl XMLDocumentScannerImpl DTDDriver next XMLDocumentScannerImpl java at com sun org apache xerces internal impl XMLDocumentScannerImpl PrologDriver next XMLDocumentScannerImpl java at com sun org apache xerces internal impl XMLDocumentScannerImpl next XMLDocumentScannerImpl java at com sun org apache xerces internal impl XMLNSDocumentScannerImpl next XMLNSDocumentScannerImpl java at com sun org apache xerces internal impl XMLDocumentFragmentScannerImpl scanDocument XMLDocumentFragmentScannerImpl java at com sun org apache xerces internal parsers XML Configuration parse XML Configuration java at com sun org apache xerces internal parsers XML Configuration parse XML Configuration java at com sun org apache xerces internal parsers XMLParser parse XMLParser java at com sun org apache xerces internal parsers AbstractSAXParser parse AbstractSAXParser java at com sun org apache xerces internal jaxp SAXParserImpl JAXPSAXParser parse SAXParserImpl java at javax xml parsers SAXParser parse SAXParser java at org mortbay xml XmlParser parse XmlParser java at org mortbay xml XmlParser parse XmlParser java at org mortbay jetty webapp TagLibConfiguration configureWebApp TagLibConfiguration java at org mortbay jetty webapp WebAppContext startContext WebAppContext java at org mortbay jetty handler ContextHandler doStart ContextHandler java at org mortbay jetty webapp WebAppContext doStart WebAppContext java at org mortbay component AbstractLifeCycle start AbstractLifeCycle java at org mortbay jetty handler HandlerCollection doStart HandlerCollection java at org mortbay jetty handler ContextHandlerCollection doStart ContextHandlerCollection java at org mortbay component AbstractLifeCycle start AbstractLifeCycle java at org mortbay jetty handler HandlerWrapper doStart HandlerWrapper java at org mortbay jetty Server doStart Server java at org mortbay component AbstractLifeCycle start AbstractLifeCycle java at org apache hadoop http HttpServer start HttpServer java at org apache hadoop hdfs server namenode NameNodeHttpServer start NameNodeHttpServer java at org apache hadoop hdfs server namenode NameNode startHttpServer NameNode java at org apache hadoop hdfs server namenode NameNode initialize NameNode java at org apache hadoop hdfs server namenode NameNode NameNode java at org apache hadoop hdfs server namenode NameNode NameNode java at org apache hadoop hdfs server namenode NameNode createNameNode NameNode java at org apache hadoop hdfs server namenode NameNode main NameNode java HttpServer should load jsp DTD from local jars instead of going remote
No,Some patches for fixing build a hadoop native library on os x Fix build native library on mac osx
No,When using HBase ExportSnapshot on a kerberized cluster exporting to s a using HADOOP we see the following problem Caused by java lang IllegalArgumentException java net UnknownHostException patch two at org apache hadoop security SecurityUtil buildTokenService SecurityUtil java The problem seems to be that the patch in HADOOP does not have getCanonicalServiceName ExportSnapshot fails on kerberized cluster using s a
Yes,Hadoop streaming no longer requires many classes in o a h record This jira removes the dead code Remove dead classes in hadoop streaming
Yes,The classes in o a h record have been deprecated for more than a year and a half They should be removed As the first step the jira moves all these classes into the hadoop streaming project which is the only user of these classes Move o a h record to hadoop streaming
No,This is an umbrella jira which addresses few enhancements to proxyUser capability via sub tasks ProxyUser improvements
No,I think it would be worthwhile to add two lines to the top of the welcome website page Stable version Latest version With the number linking off to the respective release like so http www apache org dyn closer cgi lucene hadoop hadoop tar gz We can promote versions from Latest to Stable when they have proven themselves Thoughts Add stable version line to the website front page
No, Add Spark as a related project on the Hadoop page
Yes,Jetty is no longer maintained Update the dependency to jetty Update jetty dependency to version
Yes,As noted in MAPREDUCE and HADOOP LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations LocalDirAllocator should avoid holding locks while accessing the filesystem
Yes,Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods Cleanup TestFilterFileSystem
No,Backport the hadoop openstack module to branch This should require little more than testing backport openstack support to branch
Yes,Our current dataset.registerTempTable does not actually materialize data. So it should be considered as creating a temp view. We can deprecate it and create a new method called dataset.createTempView(replaceIfExists Boolean). The default value of replaceIfExists should be false. For registerTempTable it will call dataset.createTempView(replaceIfExists = true). Deprecate registerTempTable and add dataset.createTempView
No, Comment out Favorites in the menu bar (so that it is not shown as menu item) as long as it is not possible to use it Comment out Favorites in the menu bar
No, Answer the Questionnaire in the Wiki
No, Support CI server Build Demo Demo Setup
No, Supporting build setup pilot and testing
No, Self Learning and Training
No, Knowledge Methods page Develop protractor tests and execute
No, Workbench Artifact tab Develop protractor tests and execute
No, Workbench Page Develop protractor tests and execute
No, Protractor based UI tests
No, Generic Support for User Access
No, Frontend Implementation for Draw the Uploaded image on the Editor
No, Backend Implementation for Draw the Uploaded image on the Editor
No,Learning and evaluating simple deployment of AMELIE Learning Docker Container for deployment
No,Learn and evaluate simple deployment of AMELIE
Advanced feature shall be tried later. Introduce Docker container
No, Workflow for adding the users in Project
No,VAS ADVISE IP address is pointing to our old instance after redeploying AMELIE Point to the correct VAS ADVISE IP address after redeploying AMELIE
No, Test CI server and VM deployments
No,We should always do a backup of the users projects (actually all databases files) so that it is not necessary to create everything new Backup all databases and files
No, When we are deploying a new version to the VM We should always do a backup of the users projects (actually all databases + files) so that it is not necessary to create everything new Please let me know if this is feasible If not also ok J VAS ADVISE IP address is pointing to our old instance after redeploying AMELIE Enhancement Deploying a new version to the VM
No,We require a ReadMe where it is described which software is necessary how it can be executed out of the box A short explanation about frontend and backend (highlevel concept) VAS Skeleton Guide
No,When I add a user I have to put as domain ad001 instead of AD001Should be not casesensitive in future When adding a user domain name should not be casesensitive
No,The User Registration Mail is sometimes marked as Possible Phishing Message which should be changed (1st we have to identify why it is marked as Phising mail) Registration Mail is marked as Possible Phishing Message
No,Fix the bugs linked to this ticket. Log the hours in the parent Task level Bug Fixes for Sprint 16
No, Regular Testing of deployment environment
No,Reviewer Dont show me this on Welcome Page Frontend
No,Reviewers Chandra Mouli Remember Me Option on Login Page Make it functonal
No, Support for Pilot user(s)
No,VM CI Healthcare Munich Maintain information of various deployment environment and their update status
No, Add More APIs to REST test suite
No, Add header for Java files
No,The transition is not smooth(flickers) in between the cloud and the circles display Welcome Page The transition is not smooth
No, VAS Advise Change the table title to Top 5 results
No,The hints message is inconsistent for different facets.
eg. No Hints available vs No hints to display.
The selected facets are not highlighted which might be confusing Inconsistent hint text different facets.
No, Daily Testing of AMELIE application
No,Wiki Postman etc Update Document
No, Review and Rework
No, Please extend your support from UI perspective Evaluation and PoC
No, Refactor Comment and Stabilize Code Anit
No,Pre condition CI Server is available Connectors and required libraries are installed DOD Running System on CI Server which enables user to connect AMELIE to source files Deploy data connectors also on CI server
No, Architecuture Authoring>Documentation of architectural...... = Update text according to ASR The word guidance should be introduced in the context of UC Context aware recommendation mechanims Update of Use Cases in EA Model
No,Please contact the following SWAs out of Business Units after requirements document is updated Seibert Eckhard (PG IE R&D) Guido Seeger (Z0004RZK) The requirements should be presented to them and they should provide us feedback to or a review of the document. Interview BUArchitects
No,User should be able to select and view a project from a set of projects shown to him on the landing page. Provide Project List
Yes,For more details see https wiki.ct.siemens.de display t04mm Data+Access+Layer and https wiki.ct.siemens.de display t04mm General Create Data Access Layer
No,Please upload a profile picture for your JIRA Account Profile Picture Upload
No,Hi Chengliang unfortunately I am not able to start the third sprint in our project (see screenshot). Could you please help me and give me some hint? I find it very strange that this happens a second time again. I just reported this failure with issue https agile.siemens.net browse AMELIE113. Why does this mistake occurs again and again? RegardsFlorian Sprint Management
No,What is included Define a generic model for communication (e.g. how does the GAPService tell the RECService which help he needs?) Provide a place to post this information? (one idea could be to use the blackboard pattern i.e. post the formulated question on a blackboard and services may respond if they have an answer) Define communication approach and solution between services
No, Handle Multiple Browser tabs for Session Logout
Yes,This is necessary for s3 reads and writes to work correctly with some hadoop versions. Add jets3t dependency to Spark Build