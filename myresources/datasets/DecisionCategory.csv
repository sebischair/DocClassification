Structural decision, We should deprecate ConnectionManager in before removing it in Deprecate NIO ConnectionManager
Structural decision,What StringIndexerInverse does is not strictly associated with StringIndexer and the name is not super clear Rename StringIndexerInverse to IndexToString
Non-existence - ban decision,This requires some discussion I m not sure whether runs is a useful parameter It certainly complicates the implementation We might want to optimize the k means implementation with block matrix operations In this case having runs may not be worth the trade offs Remove runs from KMeans under the pipeline API
Behavioral decision,There exists a chance that the prefixes keep growing to the maximum pattern length Then the final local processing step becomes unnecessary Skip local processing in PrefixSpan if there are no small prefixes
Structural decision,parquet mr fixed several issues that affect Spark For example PARQUET SPARK Upgrade parquet mr to
Structural decision,The utilities such as Substring substringBinarySQL and BinaryPrefixComparator computePrefix for binary data are put together in ByteArray for easy to read Move utilities for binary data into ByteArray
Structural decision,PlatformDependent UNSAFE is way too verbose Rename PlatformDependent UNSAFE Platform
Non-existence - ban decision,I took a look at the commit messages in git log it looks like the individual commit messages are not that useful to include but do make the commit messages more verbose They are usually just a bunch of extremely concise descriptions of bug fixes merges etc See mailing list discussions http apache spark developers list n nabble com discuss Removing individual commit messages from the squash commit message td html Remove individual commit messages from the squash commit message
Behavioral decision,Add Python API user guide and example for ml regression IsotonicRegression Add Python API for ml regression IsotonicRegression
Behavioral decision,Add Python API for MultilayerPerceptronClassifier Add Python API for MultilayerPerceptronClassifier
Behavioral decision,Add Python API user guide and example for ml feature CountVectorizerModel Add Python API for ml feature CountVectorizer
Non-existence - ban decision,We introduced the Netty network module for shuffle in Spark and has turned it on by default for releases The old ConnectionManager is difficult to maintain It s time to remove it Remove ConnectionManager
Behavioral decision,see discussion here https github com apache spark pull issuecomment Improve performance of Decimal times and casting from integral
Non-existence - ban decision,TypeCheck no longer applies in the new Tungsten world Remove TypeCheck in debug package
Non-existence - ban decision,In https github com apache spark pull we added FromUnsafe to convert nexted unsafe data like array map struct to safe versions It s a quick solution and we already have GenerateSafe to do the conversion which is codegened So we should remove FromUnsafe and implement its codegen version in GenerateSafe remove FromUnsafe and add its codegen version to GenerateSafe
Behavioral decision,JoinedRow anyNull currently loops through every field to check for null which is inefficient if the underlying rows are UnsafeRows It should just delegate to the underlying implementation JoinedRow anyNull should delegate to the underlying rows
Behavioral decision,This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages Issue brought up by eronwright Descriptions below copied from http apache spark developers list n nabble com Make ML Developer APIs public post td html We plan to make these APIs public in Spark However they will be marked DeveloperApi and are very likely to be broken in the future VectorUDT To define a relation with a vector field VectorUDT must be instantiated Identifiable trait The trait generates a unique identifier for the associated pipeline component Nice to have a consistent format by reusing the trait ProbabilisticClassifier Third party components should leverage the complex logic around computing only selected columns We will not yet make these public SchemaUtils Third party pipeline components have a need for checking column types and appending columns This will probably be moved into Spark SQL Users can copy the methods into their own as needed Make some ML APIs public VectorUDT Identifiable ProbabilisticClassifier
Behavioral decision,Consider SortMergeJoin which requires a sorted clustered distribution of its input rows Say that both of SMJ s children produce unsorted output but are both single partition In this case we will need to inject sort operators but should not need to inject exchanges Unfortunately it looks like the Exchange unnecessarily repartitions using a hash partitioning We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied I d like to fix this for Spark since it makes certain types of unit tests easier to write EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied
Structural decision,Previously we use MB as the default page size which was way too big for a lot of Spark applications especially for single node This patch changes it so that the default page size if unset by the user is determined by the number of cores available and the total execution memory available Pick default page size more intelligently
Behavioral decision,Add feature interaction as a transformer which takes a list of vector double columns and generate a single vector column that contains the interactions multiplication among them with proper handling of feature names Add feature interaction as a transformer
Behavioral decision,E g currently we can do up to sorts within a task During the aggregation During a sort on the same key During the shuffle In environments with tight memory restrictions the first operator may acquire so much memory such that the subsequent ones in the same task are starved A simple fix is to reserve at least a page in advance in each of these places The reserved page size need not be the same as the normal page size This is a sister problem to SPARK in Spark Core Reserve a page in all unsafe operators to avoid starving an operator
Non-existence - ban decision,A small performance optimization we don t need to generate a Tuple and then immediately discard the key We also don t need an extra wrapper Remove SqlNewHadoopRDD s generated Tuple and InterruptibleIterator
Non-existence - ban decision,It is subsumed by the new aggregate implementation Remove GeneratedAggregate
Non-existence - ban decision,GenerateUnsafeProjection can be used directly as a code generated serializer We no longer need SparkSqlSerializer Remove SparkSqlSerializer in favor of Unsafe exchange
Behavioral decision,Currently we don t support using DecimalType with precision in new unsafe aggregation it s good to support it Support update DecimalType with precision in UnsafeRow
Behavioral decision,In many modeling application data points are not necessarily sampled with equal probabilities Linear regression should support weighting which account the over or under sampling LinearRegression should supported weighted data
Behavioral decision, update InternalRow toSeq to make it accept data type info
Non-existence - ban decision,Spark s style checker should ban the use of Scala s JavaConversions which provides implicit conversions between Java and Scala collections types Instead we should be performing these conversions explicitly using JavaConverters or forgoing the conversions altogether if they re occurring inside of performance critical code Ban use of JavaConversions and migrate all existing uses to JavaConverters
Behavioral decision, Add StreamingContext getActiveOrCreate to python API
Structural decision,See http apache spark developers list n nabble com Re Should spark ec get its own repo td html for more details Move spark ec from mesos to amplab
Non-existence - ban decision, remove the createCode and createStructCode and replace the usage of them by createStructCode
Non-existence - ban decision,While reviewing yhuai s patch for SPARK I noticed that Exchange s compatible check may be incorrectly returning false in many cases As far as I know this is not actually a problem because the compatible meetsRequirements and needsAnySort checks are serving only as short circuit performance optimizations that are not necessary for correctness In order to reduce code complexity I think that we should remove these checks and unconditionally rewrite the operator s children This should be safe because we rewrite the tree in a single bottom up pass Remove compatibleWith meetsRequirements and needsAnySort checks from Exchange
Non-existence - ban decision,They were added to improve performance so JIT can inline the JoinedRow calls However we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators Remove all extra JoinedRows
Non-existence - ban decision, Remove UnsafeRowConverter in favor of UnsafeProjection
Structural decision,We should consolidate LocalScheduler and ClusterScheduler given most of the functionalities are duplicated in both This can be done by removing the LocalScheduler and create a LocalSchedulerBackend that connects directly to an Executor Consolidate local scheduler and cluster scheduler
Non-existence - ban decision,It is a big change but it lets us use the type information to prevent accidentally passing internal types to external types Remove InternalRow s inheritance from Row
Non-existence - ban decision,See SPARK We added varargs again Though it is technically correct it often requires that developers do clean assembly rather than not clean assembly which is a nuisance during development This JIRA will remove it for now pending a fix to the Scala compiler Params setDefault should not keep varargs annotation
Non-existence - ban decision,Spark has an option called spark localExecution enabled according to the docs quote Enables Spark to run certain jobs such as first or take on the driver without sending tasks to the cluster This can make certain jobs execute very quickly but may require shipping a whole partition of data to the driver quote This feature ends up adding quite a bit of complexity to DAGScheduler especially in the runLocallyWithinThread method but as far as I know nobody uses this feature I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method As a step towards scheduler complexity reduction I propose that we remove this feature and all code related to it for Spark Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled
Non-existence - ban decision,A small change based on code review and offline discussion with dragos Removing unnecessary self types in Catalyst
Non-existence - ban decision,As the new Parquet external data source matures we should remove the old Parquet support now Removes old Parquet support code
Non-existence - ban decision,They are not very useful and cause problems with toString due to the order they are mixed in Remove LeafNode UnaryNode BinaryNode from TreeNode
Non-existence - ban decision,The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark Also the feature in SPARK is strictly better than a correct implementation of that feature We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work Remove references to preferredNodeLocalityData in javadoc and print warning when used
Non-existence - ban decision,It is unnecessary and makes the type hierarchy slightly more complicated than needed Remove ExtractValueWithOrdinal abstract class
Non-existence - ban decision,Right now InternalRow is megamorphic because it has many different implementations We should work towards having only one or at most two InternalRow implementations Remove EmptyRow class
Non-existence - ban decision,The type alias was there because initially when I moved Row around I didn t want to do massive changes to the expression code But now it should be pretty easy to just remove it One less concept to worry about Remove InternalRow type alias in expressions package
Non-existence - ban decision,Based on discussion offline with marmbrus we should remove GenerateProjection Remove GenerateProjection
Structural decision,This will make it convenient for R users to use SparkR from their browsers Install and configure RStudio server on Spark EC
Non-existence - ban decision,We should remove the existing ExpressionOptimizationSuite and update checkEvaluation to also run the optimizer version ExpressionEvalHelper checkEvaluation should also run the optimizer version
Non-existence - ban decision,From my perspective as a code reviewer I find them more confusing than using String directly Remove Term Code type aliases in code generation
Non-existence - ban decision,It s not a very useful type to use We can just remove it to simplify expressions slightly Remove EvaluatedType from SQL Expression
Structural decision,ExternalSorter contains a bunch of to move this functionality out of ExternalSorter and into a separate class which shares a common interface insertAll writePartitionedFile This is a stepping stone towards eventually removing this bypass path see SPARK Move hash style shuffle code out of ExternalSorter and into own file
Non-existence - ban decision,Learnt a lesson from SPARK Spark should avoid to use scala concurrent ExecutionContext Implicits global because the user may submit blocking actions to scala concurrent ExecutionContext Implicits global and exhaust all threads in it This could crash Spark So Spark should always use its own thread pools for safety Remove import scala concurrent ExecutionContext Implicits global
Non-existence - ban decision, Removed calling size length in while condition to avoid extra JVM call
Non-existence - ban decision,We can just rewrite distinct using groupby i e aggregate operator Remove physical Distinct operator in favor of Aggregate
Non-existence - ban decision, Removed diffSum which is theoretical zero in LinearRegression and coding formating
Structural decision,We want to change and improve the spark ml API for trees and ensembles but we cannot change the old API in spark mllib To support the changes we want to make we should move the implementation from spark mllib to spark ml We will generalize and modify it but will also ensure that we do not change the behavior of the old API There are several steps to this Copy the implementation over to spark ml and change the spark ml classes to use that implementation rather than calling the spark mllib implementation The current spark ml tests will ensure that the implementations learn exactly the same models Note This should include performance testing to make sure the updated code does not have any regressions UPDATE I have run tests using spark perf and there were no regressions Remove the spark mllib implementation and make the spark mllib APIs wrappers around the spark ml implementation The spark ml tests will again ensure that we do not change any behavior Move the unit tests to spark ml and change the spark mllib unit tests to verify model equivalence This JIRA is now for step only Steps and will be in separate JIRAs After these updates we can more safely generalize and improve the spark ml implementation Move tree forest implementation from spark mllib to spark ml
Non-existence - ban decision,This maven repository is blocked in China We should get rid of that dependency so people in China can compile Spark Remove dependency on Twitter J repository
Behavioral decision,This is not always possible but whenever possible we should remove or reduce the differences between Pandas and Spark DataFrames in Python Improve DataFrame API compatibility with Pandas
Behavioral decision,This depends on some internal interface of Spark SQL should be done after merging into Spark DataFrame UDFs in R
Behavioral decision,Would be great to create APIs for external block stores rather than doing a bunch of if statements everywhere Create external block store API
Structural decision, Upgrade Tachyon dependency to
Structural decision,Deprecated configs are currently all strewn across the code base It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere Centralize deprecated configs in SparkConf
Non-existence - ban decision,Continue the discussion from the LDA PR CheckpoingDir is a global Spark configuration which should not be altered by an ML algorithm We could check whether checkpointDir is set if checkpointInterval is positive Remove setCheckpointDir from LDA and tree Strategy
Non-existence - ban decision,This method survived the code review and it has been there since v It exposes jblas types Let s remove it from the public API I expect that no one calls it directly Hide ALS solveLeastSquares
Behavioral decision,toLocalIterator is available in Java and Scala If we add this functionality to Python then we can also be able to use PySpark to iterate over a dataset partition by partition Add toLocalIterator to pyspark rdd
Structural decision,Fix a todo in spark sql remove Command and use RunnableCommand instead Refactory command in spark sql
Non-existence - ban decision, Remove unneeded staging repositories from build
Structural decision,mqtt client was removed from the Eclipse Paho repository and hence is breaking Spark build Upgrade MQTT dependency to use mqtt client
Structural decision,In this refactoring the performance is slightly increased by removing the overhead from breeze vector The bottleneck is still in breeze norm which is implemented by activeIterator This inefficiency of breeze norm will be addressed in next PR At least this PR makes the base Refactorize Normalizer to make code cleaner
Non-existence - ban decision,In RDDSampler it try use numpy to gain better performance for possion but the number of call of random is only faction N in the pure python implementation of possion so there is no much performance gain from numpy numpy is not a dependent of pyspark so it maybe introduce some problem such as there is no numpy installed in slaves but only installed master as reported in xxxx It also complicate the code a lot so we may should remove numpy from RDDSampler remove numpy from RDDSampler of PySpark
Non-existence - ban decision,For example YarnRMClient and YarnRMClientImpl can be merged YarnAllocator and YarnAllocationHandler can be merged Remove layers of abstraction in YARN code no longer needed after dropping yarn alpha
Non-existence - ban decision,Due to vertex attribute caching EdgeRDD previously took two type parameters ED and VD However this is an implementation detail that should not be exposed in the interface so this PR drops the VD type parameter This requires removing the filter method from the EdgeRDD interface because it depends on vertex attribute caching Drop VD type parameter from EdgeRDD
Structural decision,We should upgrade snappy java to across all of our maintenance branches This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream this operation is always an error but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid corrupted streams see https github com xerial snappy java issues for more context This should be a major help in the Snappy debugging work that I ve been doing Upgrade snappy java to
Non-existence - ban decision,After Pyrolite release a new version with PR https github com irmen Pyrolite pull we should remove the workaround introduced in PR https github com apache spark pull remove workaround to pickle array of float for Pyrolite
Non-existence - ban decision,This will depend a bit on both user demand and the commitment level of maintainers but I d like to propose the following timeline for yarn alpha support Spark Deprecate YARN alpha Spark Remove YARN alpha i e require YARN stable Since YARN alpha is clearly identified as an alpha API it seems reasonable to drop support for it in a minor release However it does depend a bit whether anyone uses this outside of Yahoo and that I m not sure of In the past this API has been used and maintained by Yahoo but they ll be migrating soon to the stable API s Deprecate and later remove YARN alpha support
Non-existence - ban decision,HiveLocalContext is nearly completely redundant with HiveContext We should consider deprecating it and removing all uses Get rid of LocalHiveContext
Non-existence - ban decision,According to http doc akka io docs akka intro getting started html Akka is now published to Maven Central so our documentation and POM files don t need to use the old Akka repo It will be one less step for users to worry about Remove use of special Maven repo for Akka
Non-existence - ban decision,The removed codes are not reachable because InConversion already resolve the type coercion issues Remove IN type coercion from PromoteStrings
Non-existence - ban decision,There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown As we upgrade to Parquet which includes the fix for the pushdown of optional columns we don t need this metadata now Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown
Structural decision,Apache Parquet is released officially last week on Jan This issue aims to bump Parquet version to since it includes many fixes https lists apache org thread html af c f a d ec b bbeecaea aa ef f c Cdev parquet apache org E Upgrade Parquet to
Behavioral decision,Create a Python wrapper for spark ml classification LinearSVC LinearSVC Python API
Behavioral decision,To implement DDL commands we added several analyzer rules in sql hive module to analyze DDL related plans However our Analyzer currently only have one extending interface extendedResolutionRules which defines extra rules that will be run together with other rules in the resolution batch and doesn t fit DDL rules well because DDL rules may do some checking and normalization but we may do it many times as the resolution batch will run rules again and again until fixed point and it s hard to tell if a DDL rule has already done its checking and normalization It s fine because DDL rules are idempotent but it s bad for analysis performance some DDL rules may depend on others and it s pretty hard to write if conditions to guarantee the dependencies It will be good if we have a batch which run rules in one pass so that we can guarantee the dependencies by rules order add a new extending interface in Analyzer for post hoc resolution
Behavioral decision,When we use the jdbc in pyspark if we check the lowerBound and upperBound we can give a more friendly suggestion Check the lowerBound and upperBound whether equal None in jdbc API
Behavioral decision,Currently in SQL we implement overwrites by calling fs delete directly on the original data This is not ideal since we the original files end up deleted even if the job aborts We should extend the commit protocol to allow file overwrites to be managed as well Add deleteWithJob hook to internal commit protocol API
Behavioral decision,In SPARK support for AES encryption was added to the Spark network library But the authentication of different Spark processes is still performed using SASL s DIGEST MD mechanism That means the authentication part is the weakest link since the AES keys are currently encrypted using des strongest cipher supported by SASL Spark can t really claim to provide the full benefits of using AES for encryption We should add a new auth protocol that doesn t need these disclaimers AES based authentication mechanism for Spark
Non-existence - ban decision, remove the supportsPartial flag in AggregateFunction
Non-existence - ban decision,Remove useless databaseName from SimpleCatalogRelation Remove databaseName from SimpleCatalogRelation
Structural decision,Since spark sql hive thriftServer singleSession is a configuration of SQL component this conf can be moved from SparkConf to StaticSQLConf When we introduced spark sql hive thriftServer singleSession all the SQL configuration can be modified in different sessions Later static SQL configuration is added It is a perfect fit for spark sql hive thriftServer singleSession Previously we did the same move for spark sql warehouse dir from SparkConf to StaticSQLConf Move spark sql hive thriftServer singleSession to SQLConf
Structural decision,Right now ContextCleaner referenceBuffer is ConcurrentLinkedQueue and the time complexity of the remove action is O n It can be changed to use ConcurrentHashMap whose remove is O Change ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster
Non-existence - ban decision,SortPartitions and RedistributeData logical operators are not actually used and can be removed Note that we do have a Sort operator with global flag false that subsumed SortPartitions Remove SortPartitions and RedistributeData
Structural decision,I recently hit a bug of com thoughtworks paranamer paranamer which causes jackson fail to handle byte array defined in a case class Then I find https github com FasterXML jackson module scala issues which suggests that it is caused by a bug in paranamer Let s upgrade paranamer Since we are using jackson and jackson module paranamer use com thoughtworks paranamer paranamer I suggests that we upgrade paranamer to Upgrade com thoughtworks paranamer paranamer to
Behavioral decision,Currently we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog MSCK REPAIR TABLE or ALTER TABLE table RECOVER PARTITIONS Actually very hard for me to remember MSCK and have no clue what it means After the new Scalable Partition Handling the table repair becomes much more important for making visible the data in the created data source partitioned table It is desriable to add it into the Catalog interface so that users can repair the table by Add recoverPartitions API to Catalog
Structural decision,SparkR mllib R is getting bigger as we add more ML wrappers I d like to split it into multiple files to make us easy to maintain mllibClassification R mllibRegression R mllibClustering R mllibFeature R or mllib classification R mllib regression R mllib clustering R mllib features R For R convention it s more prefer the first way And I m not sure whether R supports the second organized way will check later Please let me know your preference I think the start of a new release cycle is a good opportunity to do this since it will involves less conflicts If this proposal was approved I can work on it cc felixcheung josephkb mengxr Split SparkR mllib R into multiple files
Structural decision,Make StreamExecution and progress classes serializable because it is too easy for it to get captured with normal usage Make StreamExecution and progress classes serializable
Behavioral decision, Expose event time time stats through StreamingQueryProgress
Behavioral decision,When starting a stream with a lot of backfill and maxFilesPerTrigger the user could often want to start with most recent files first This would let you keep low latency for recent data and slowly backfill historical data It s better to add an option to control this behavior Make FileStream be able to start with most recent files
Structural decision,Implement a wrapper in SparkR to support bisecting k means Bisecting k means wrapper in SparkR
Structural decision,spark logit is added in We need to update spark vignettes to reflect the changes This is part of SparkR QA work Update spark logit in sparkr vignettes
Behavioral decision,Currently when users use Python UDF in Filter BatchEvalPython is always generated below FilterExec However not all the predicates need to be evaluated after Python UDF execution Thus we can push down the predicates through BatchEvalPython Push Down Filter Through BatchEvalPython
Structural decision,An informal poll of a bunch of users found this name to be more clear Rename recentProgresses to recentProgress
Non-existence - ban decision,Otherwise other threads cannot query the content in MemorySink when DataFrame collect takes long time to finish MemorySink should not call DataFrame collect when holding a lock
Behavioral decision,Many Spark developers often want to test the runtime of some function in interactive debugging and testing It d be really useful to have a simple spark time method that can test the runtime SparkSession time a simple timer function
Behavioral decision,We currently have function input file name to get the path of the input file but don t have functions to get the block start offset and length This patch introduces two functions input file block start returns the file block start offset or if not available input file block length returns the file block length or if not available input file block start and input file block length function
Behavioral decision,AggregateFunction currently implements ImplicitCastInputTypes which enables implicit input type casting This can lead to unexpected results and should only be enabled when it is suitable for the function at hand AggregateFunction should not ImplicitCastInputTypes
Structural decision,Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly See discussion at https github com apache spark pull discussion r Move DT RF GBT Param setter methods to subclasses
Structural decision,We should include in Spark distribution the built source package for SparkR This will enable help and vignettes when the package is used Also this source package is what we would release to CRAN R Include package vignettes and help pages build source package in Spark distribution
Non-existence - ban decision, remove OverwriteOptions
Behavioral decision,trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming it let us no choice but to implement one for ourself this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only here is how you can use it endpoint root streaming api v Endpoint Meaning statistics Statistics information of stream receivers A list of all receiver streams receivers stream id Details of the given receiver stream batches A list of all retained batches batches batch id Details of the given batch batches batch id operations A list of all output operations of the given batch batches batch id operations operation id Details of the given operation given batch Add a REST api to spark streaming
Behavioral decision,These two methods were added to Scala Datasets but are not available in Python yet Add withWatermark and checkpoint to python dataframe
Structural decision,Refactor StaticInvoke Invoke and NewInstance as Introduce InvokeLike to extract common logic from StaticInvoke Invoke and NewInstance to prepare arguments Remove unneeded null checking and fix nullability of NewInstance Modify to short circuit if arguments have null when propageteNull true Refactor StaticInvoke Invoke and NewInstance
Structural decision,Spark s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing IO performance improvements made in Spark In order to fix this performance problem we should re implement those read paths in terms of TextFileFormat Use TextFileFormat in implementation of CSVFileFormat
Behavioral decision,As of today I could not access rdd localCheckpoint in pyspark This is an important issue for machine learning people as we often have to iterate algorithms and perform operations like joins in each iteration If the lineage is not truncated the memory usage the lineage and computation time explode rdd localCheckpoint seems like the most straightforward way of truncating the lineage but the python API does not expose it Expose RDD localCheckpoint in PySpark
Structural decision,I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite and the end to end tests FunctionsSuite It d be great to make all test suites consistent with this naming convention Use consistent naming for expression test suites
Structural decision,hash scala was getting pretty long and it s not obvious that hash expressions belong there Creating a hash scala to put all the hash expressions Move hash expressions from misc scala into hash scala
Structural decision,Column expr is private sql but it s an actually really useful field to have for debugging We should open it up similar to how we use QueryExecution Make Column expr public
Structural decision,Plan Mark it very explicit in Spark that support for the aforementioned environments are deprecated Remove support it Spark Also see mailing list discussion http apache spark developers list n nabble com Straw poll dropping support for things like Scala tp p html More officially deprecate support for Python Java and Scala
Non-existence - ban decision,Whenever we aggregate data by event time we want to consider data is late and out of order in terms of its event time Since we keep aggregate keyed by the time as state the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data Since the state is a store in memory we have to prevent building up of this unbounded state Hence we need a watermarking mechanism by which we will mark data that is older beyond a threshold as too late and stop updating the aggregates with them This would allow us to remove old aggregates that are never going to be updated thus bounding the size of the state Here is the design doc https docs google com document d z Pazs v rA azvmYhu I xwqaNQl ZLIS xhkfCQ edit usp sharing Observed delay based event time watermarks
Structural decision,We should upgrade to the latest release of MiMa in order to include my fix for a bug which led to flakiness in the MiMa checks https github com typesafehub migration manager issues Upgrade to MiMa
Non-existence - ban decision,There are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge Currently the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing refer the method mergeApplicationListing fileStatus FileStatus The process of replay involves each line in the event log being read as a string parsing the string to a Json structure converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive Tests show that majority of time spent in replay is in doing this work When the replay is performed for building the application listing the only two events that the code really cares for are SparkListenerApplicationStart and SparkListenerApplicationEnd since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener This means that when processing an event log file with a huge number hundreds of thousands can be more of events the work done to deserialize all of these event and then replay them is not needed Only two events are what we re interested in and this can be used to ensure that when replay is performed for the purpose of building the application list we only make the effort to replay these two events and not others My tests show that this drastically improves application list load time For a MB event log from a user with over events the load time local on my mac comes down from about secs to under second using this approach For customers that typically execute applications with large event logs and thus have multiple large event logs present this can speed up how soon the history server UI lists the apps considerably I will be updating a pull request with take at fixing this Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page
Behavioral decision,The new Tungsten execution engine has very robust memory management and speed for simple data types It does however suffer from the following For user defined aggregates Hive UDAFs Dataset typed operators it is fairly expensive to fit into the Tungsten internal format For aggregate functions that require complex intermediate data structures Unsafe on raw bytes is not a good programming abstraction due to the lack of structs The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases This operator however should limit its memory usage to avoid putting too much pressure on GC e g falling back to sort based aggregate as soon the number of objects exceeds a very low threshold Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark Introduce a JVM object based aggregate operator
Structural decision,In the existing code there are three layers of serialization involved in sending a task from the scheduler to an executor A Task object is serialized The Task object is copied to a byte buffer that also contains serialized information about any additional JARs files and Properties needed for the task to execute This byte buffer is stored as the member variable serializedTask in the TaskDescription class The TaskDescription is serialized in addition to the serialized task JARs the TaskDescription class contains the task ID and other metadata and sent in a LaunchTask message While it is necessary to have two layers of serialization so that the JAR file and Property info can be deserialized prior to deserializing the Task object the third layer of deserialization is unnecessary this is as a result of SPARK We should eliminate a layer of serialization by moving the JARs files and Properties into the TaskDescription class taskScheduler has some unneeded serialization
Structural decision,Code generation to get data from ColumnVector and ColumnarBatch is becoming pervasive The generation part as a trait for ease of reuse Refactor code generation to get data for ColumnVector ColumnarBatch
Non-existence - ban decision,We generate bitmasks for grouping sets during the parsing process and use these during analysis These bitmasks are difficult to work with in practice and have lead to numerous bugs I suggest that we remove these and use actual sets instead however we would need to generate these offsets for the grouping id Do not use bitmasks during parsing and analysis of CUBE ROLLUP GROUPING SETS
Structural decision,Classifier getNumClasses can not support Non Double types and classification algos relying on it do not support non double labelCol like NavieBayes As suggested by sethah it is not a reasonable way to do datatype cast everywhere And we can make cast only happen in Predictor yanboliang josephkb srowen Move LabelCol datatype cast into Predictor fit
Behavioral decision,ANSI SQL uses the following to specify the frame boundaries for window functions Improve window function frame boundary API in DataFrame
Behavioral decision,When I was creating the example code for SPARK I realized it was pretty convoluted to define the frame boundaries for window functions when there is no partition column or ordering column The reason is that we don t provide a way to create a WindowSpec directly with the frame boundaries We can trivially improve this by adding rowsBetween and rangeBetween to Window object DataFrame API should simplify defining frame boundaries without partitioning ordering
Non-existence - ban decision, Remove redundant Experimental annotations in sql streaming package
Structural decision,The current InternalRow hierarchy makes a difference between immutable and mutable rows In practice we cannot guarantee that an immutable internal row is immutable you can always pass a mutable object as an one of its elements Lets make all internal rows mutable and reduce the complexity Simplify InternalRow hierarchy
Non-existence - ban decision,Dataset always does eager analysis now Thus spark sql eagerAnalysis is not used any more Thus we need to remove it Remove spark sql eagerAnalysis
Structural decision,There are listLeafFiles related functions in Spark ListingFileCatalog listLeafFiles which calls HadoopFsRelation listLeafFilesInParallel if the number of paths passed in is greater than a threshold if it is lower then it has its own serial version implemented HadoopFsRelation listLeafFiles called only by HadoopFsRelation listLeafFilesInParallel HadoopFsRelation listLeafFilesInParallel called only by ListingFileCatalog listLeafFiles It is actually very confusing and error prone because there are effectively two distinct implementations for the serial version of listing leaf files This into ListingFileCatalog since it is the only class that needs this Keep only one function for listing files in serial Consolidate various listLeafFiles implementations
Non-existence - ban decision,Query Only one distinct should be necessary This makes a bunch of unions slower than a bunch of union alls followed by a distinct Optimizer should remove unnecessary distincts in multiple unions
Behavioral decision,We use multiple DStreams coming from different Kafka topics in a Streaming application Some settings like maxrate and backpressure enabled disabled would be better passed as config to KafkaUtils createStream and KafkaUtils createDirectStream instead of setting them in SparkConf Being able to set a different maxrate for different streams is an important requirement for us we currently work around the problem by using one receiver based stream and one direct stream We would like to be able to turn on backpressure for only one of the streams as well Set Streaming MaxRate Independently For Multiple Streams
Behavioral decision,Profiling a job we saw that patten matching in wrap function of HiveInspector is consuming around of the time which can be avoided A similar change in the unwrap function was made in SPARK When wrapping catalyst datatype to Hive data type avoid pattern matching
Behavioral decision,In logical plan SerializeFromObject for an array always use GenericArrayData as a destination UnsafeArrayData could be used for an primitive array This is a simple approach to solve issues that are addressed by SPARK Here is a motivating example Optimize SerializeFromObject for primitive array
Non-existence - ban decision,The TaskMetricsUIData updatedBlockStatuses field is assigned to but never read increasing the memory consumption of the web UI We should remove this field Remove unused TaskMetricsUIData updatedBlockStatuses field
Structural decision,Many users have requirements to use third party R packages in executors workers but SparkR can not satisfy this requirements elegantly For example you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios Users can install R packages from CRAN or custom CRAN like repository for each executors Users can load their local R packages and install them on each executors To achieve this goal the first thing is to make SparkR executors support virtualenv like Python conda I have investigated and found packrat http rstudio github io packrat is one of the candidates to support virtualenv for R Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space Then SparkR users can install third party packages in the application scope destroy after the application exit and don t need to bother IT administrators to install these packages manually I would like to know whether it make sense SparkR executors workers support virtualenv
Structural decision,For building SparkR vignettes on Jenkins machines we need the rmarkdown R The package is available at https cran r project org web packages rmarkdown index html I think running something like Rscript e install packages rmarkdown repos http cran stat ucla edu should work Install rmarkdown R package on Jenkins machines
Non-existence - ban decision,To regulate pending and running executors we determine the executors which are eligible to kill and kill them iteratively rather than a loop This does an RPC call and is synchronized leading to lock contention for SparkListenerBus Side effect listener bus is blocked while we iteratively remove executors Kill multiple executors together to reduce lock contention
Structural decision,It would be useful if more of JDBCRDD s JDBC Spark SQL functionality was usable from outside of JDBCRDD this would make it easier to write test harnesses comparing Spark output against other JDBC databases Refactor JDBCRDD to expose JDBC SparkSQL conversion functionality
Non-existence - ban decision,This is another step to get rid of HiveClient from HiveSessionState All the metastore interactions should be through ExternalCatalog interface However the existing implementation of InsertIntoHiveTable still requires Hive clients Thus we can remove HiveClient by moving the metastore interactions into ExternalCatalog Remove Direct Usage of HiveClient in InsertIntoHiveTable
Structural decision,In publishing SparkR to CRAN it would be nice to have a vignette as a user guide that describes the big picture introduces the use of various methods This is important for new users because they may not even know which method to look up Add package vignette to SparkR
Structural decision,Kolmogorov Smirnov Test is a popular nonparametric test of equality of distributions There is implementation in MLlib It will be nice if we can expose that in SparkR Add Kolmogorov Smirnov Test to SparkR
Structural decision, move CreateTables to HiveStrategies
Behavioral decision,Spark has configurable L regularization parameter for generalized linear regression It is very important to have them in SparkR so that users can run ridge regression SparkR spark glm should have configurable regularization parameter
Non-existence - ban decision,Method SQLContext parseDataType dataTypeString String could be removed we should use SparkSession parseDataType dataTypeString String instead This require updating PySpark Method SQLContext parseDataType dataTypeString String could be removed
Non-existence - ban decision,Since HiveClient is used to interact with the Hive metastore it should be hidden in HiveExternalCatalog After moving HiveClient into HiveExternalCatalog HiveSharedState becomes a wrapper of HiveExternalCatalog Thus removal of HiveSharedState becomes straightforward After removal of HiveSharedState the reflection logic is directly applied on the choice of ExternalCatalog types based on the configuration of CATALOG IMPLEMENTATION HiveClient is also used invoked by the other entities besides HiveExternalCatalog we defines the following two APIs Removal of HiveSharedState
Non-existence - ban decision, remove catalog table type INDEX
Structural decision, Refactor R mllib for easier ml implementations
Behavioral decision,Inline tables currently do not support SQL generation and as a result a view that depends on inline tables would fail Support SQL generation for inline tables
Non-existence - ban decision,CreateHiveTableAsSelectLogicalPlan is a dead code after refactoring Removal of useless CreateHiveTableAsSelectLogicalPlan
Behavioral decision, Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler
Non-existence - ban decision,After acquiring allocations from YARN and launching containers Spark currently waits for seconds for executors to connect to the driver On Spark standalone nothing like this happens I m wondering whether we can just remove this sleep entirely Is there a reason I m missing why YARN is different than standalone in this regard At the least we could do something smarter like wait until all executors have registered Remove second sleep before starting app on YARN
Structural decision,Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher The only option right now is to created a property file Add conf to mesos dispatcher process
Structural decision,CC mgummelt tnachen skonto I think this is fairly easy and would be beneficial as more work goes into Mesos It should separate into a module like YARN does just on principle really but because it also means anyone that doesn t need Mesos support can build without it I m entirely willing to take a shot at this Collect Mesos support code into a module profile
Non-existence - ban decision,The execution package is meant to be internal and as a result it does not make sense to mark things as private sql or private spark It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql and private spark from sql execution package
Structural decision,There could be same subquery within a single query we could reuse the result without running it multiple times Reuse subqueries within single query
Behavioral decision,Function related HiveExternalCatalog APIs do not have enough verification logics After the PR HiveExternalCatalog and InMemoryCatalog become consistent in the error handling For example below is the exception we got when calling renameFunction Verification of Function related ExternalCatalog APIs
Structural decision,Update LogisticCostAggregator serialization code to make it consistent with LinearRegression Update LogisticCostAggregator serialization code to make it consistent with LinearRegression
Non-existence - ban decision,Remove TestHiveSharedState Otherwise we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION Removal of TestHiveSharedState
Non-existence - ban decision, remove MaxOf and MinOf
Non-existence - ban decision,The catalyst package is meant to be internal and as a result it does not make sense to mark things as private sql or private spark It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql and private spark from catalyst package
Behavioral decision,SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added Open up SparkILoop getAddedJars
Structural decision,This JIRA is to upgrade the derby version from to Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark I now believe it is required based on comments for the pull request and so this is only a dependency upgrade The upgrade is due to an already disclosed vulnerability CVE in derby We used https www versioneye com search and will be checking for any other problems in a variety of libraries too investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this This was raised on the mailing list at http apache spark developers list n nabble com VOTE Release Apache Spark RC tp p html by Stephen Hellberg and replied to by Sean Owen I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version I checked up to the branch so ideally we d backport this for all impacted Spark releases I ve marked this as critical and ticked the important checkbox as it s going to impact every user there isn t a security component should we add one and hence the build tag Upgrade derby to from
Non-existence - ban decision,Some codes in subexpressionEliminationForWholeStageCodegen are never used actually Remove them using this jira Remove unused codes in subexpressionEliminationForWholeStageCodegen
Non-existence - ban decision, use StructType in CatalogTable and remove CatalogColumn
Structural decision, move BucketSpec to catalyst module and use it in CatalogTable
Behavioral decision,This is similar with https issues apache org jira browse SPARK Currently JdbcUtils savePartition is doing type based dispatch for each row to write appropriate values So appropriate writers can be created first according to the schema and then apply them to each row This approach is similar with CatalystWriteSupport Avoid per record type dispatch in JDBC when writing
Behavioral decision,Elt function doesn t support codegen execution It is better to provide the support Add codegen for Elt function
Structural decision, Move regexp unit tests to RegexpExpressionsSuite
Behavioral decision,Dataframe drop supported multi columns in spark api and should make python api also support it Dataframe drop supported multi columns in spark api and should make python api also support it
Behavioral decision,Currently filters for TimestampType and DecimalType are not being pushed down in ORC data source although ORC filters support both Support for pushing down filters for decimal and timestamp types in ORC
Structural decision, move hive hack for data source table into HiveExternalCatalog
Behavioral decision,It seems EqualNullSafe filter was missed for batch pruneing partitions in cached tables Supporting this improve the performance roughly it will vary Running the codes below Support partition batch pruning with EqualNullSafe predicate in InMemoryTableScanExec
Behavioral decision,ExternalShuffleService is essential for spark In order to better monitor shuffle service we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system Add metrics and source for external shuffle service
Non-existence - ban decision,When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider they will hit an error when resolving the relation Data Source APIs Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider
Non-existence - ban decision,Different from the other leaf nodes MetastoreRelation and SimpleCatalogRelation have a pre defined alias which is used to change the qualifier of the node However based on the existing alias handling alias should be put in SubqueryAlias This PR is to separate alias handling from MetastoreRelation and SimpleCatalogRelation to make it consistent with the other nodes For example below is an example query for MetastoreRelation which is converted to LogicalRelation Note the optimized plans are the same For SimpleCatalogRelation the existing code always generates two Subqueries Thus no change is needed Remove Alias from MetastoreRelation and SimpleCatalogRelation
Non-existence - ban decision,Currently there are a few reports about Spark query performance regression for large queries This issue speeds up SQL query processing performance by removing redundant consecutive executePlan call in Dataset ofRows function and Dataset instantiation Specifically this issue aims to reduce the overhead of SQL query execution plan generation not real query execution So we can not see the result in the Spark Web UI Please use the following query script Before Speed up SQL query performance by removing redundant executePlan call in Dataset
Non-existence - ban decision,LogicalPlan InsertIntoHiveTable is useless Thus we can remove it from the code base Remove InsertIntoHiveTable From Logical Plan
Behavioral decision,Currently our Optimizer may reorder the predicates to run them more efficient but in non deterministic condition change the order between deterministic parts and non deterministic parts may change the number of input rows For example may call rand for different times and therefore the output rows differ Improve the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition
Behavioral decision,Spark SQL currently falls back to Hive for xpath related functions Implement xpath user defined functions
Non-existence - ban decision,There are some duplicated code for options we should simplify them Cleanup options for DataFrame reader API in Python
Non-existence - ban decision,This patch removes the blind fallback into Hive for functions Instead it creates a whitelist and adds only a small number of functions to the whitelist i e the ones we intend to support in the long run in Spark Whitelist the list of Hive fallback functions
Behavioral decision,CollectSet cannot have map typed data because MapTypeData does not implement equals So if we find map type in CollectSet queries fail Improve the type check of CollectSet in CheckAnalysis
Structural decision,We embed partitioning logic in FileSourceStrategy apply making the function very long This is a small refactoring to move it into its own functions Eventually we would be able to move the partitioning functions into a physical operator rather than doing it in physical planning Move RDD creation logic from FileSourceStrategy apply
Non-existence - ban decision,In and earlier releases we have package grouping in the generated Java API docs See http spark apache org docs api java index html However this disappeared in http spark apache org docs api java index html Rather than fixing it I d suggest removing grouping Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala No one complained about missing groups since Remove package grouping in genjavadoc
Non-existence - ban decision,We need hashCode and euqals in UnsafeMapData because of the behaivour of UnsafeMapData is different from that of ArrayBasedMapData Remove hashCode and euqals in ArrayBasedMapData
Behavioral decision,Both VectorUDT and MatrixUDT are private APIs because UserDefinedType itself is private in Spark However in order to let developers implement their own transformers and estimators we should expose both types in a public API to simply the implementation of transformSchema transform etc Otherwise they need to get the data types using reflection Note that this doesn t mean to expose VectorUDT MatrixUDT classes We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type There are two ways to implement this following DataTypes java in SQL so Java users doesn t need the extra Define DataTypes in Scala Expose VectorUDT MatrixUDT in a public API
Behavioral decision,This issue adds read orc write orc to SparkR for API parity Add read orc write orc to SparkR
Structural decision,There is a ToDo of GenericArrayData class which is to eliminate boxing unboxing for a primitive array described here https github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance Prepare GenericArrayData implementation specialized for a primitive array
Behavioral decision,This is a debug only version of SPARK for tutorials and debugging of streaming apps it would be nice to have a text based socket source similar to the one in Spark Streaming It will clearly be marked as debug only so that users don t try to run it in production applications because this type of source cannot provide HA without storing a lot of state in Spark Add debug only socket source in Structured Streaming
Behavioral decision,Add a new API method called gapplyCollect for SparkDataFrame It does gapply on a SparkDataFrame and collect the result back to R Compared to gapply collect gapplyCollect offers performance optimization as well as programming convenience as no schema is needed to be provided This is similar to dapplyCollect add gapplyCollect for SparkDataFrame
Non-existence - ban decision,Interface method FileFormat prepareRead was added in PR https github com apache spark pull to handle a special case in the LibSVM data source However the semantics of this interface method isn t intuitive it returns a modified version of the data source options map Considering that the LibSVM case can be easily handled using schema metadata inside inferSchema we can remove this interface method to keep the FileFormat interface clean Remove FileFormat prepareRead
Behavioral decision,It would be good to have an additional implementation which uses dense format for UnsafeArrayData to reduce memory footprint Current UnsafeArrayData implementation uses only a sparse format It is useful for an UnsafeArrayData that is created by a method fromPrimitiveArray which have no null value Introduce additonal implementation with a dense format for UnsafeArrayData
Behavioral decision,This is for API parity of Scala API Refer to https issues apache org jira browse SPARK Add varargs type dropDuplicates function in SparkR
Structural decision,Right now Spark does not load hive site xml Based on users feedback it seems make sense to still load this conf file Originally this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances Let s avoid of using this way to load hive site xml Instead since hive site xml is a normal hadoop conf file we can first find its url using the classloader and then use Hadoop Configuration s addResource or add hive site xml as a default resource through Configuration addDefaultResource to load confs Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive Bring back the hive site xml support for Spark
Behavioral decision,Sometimes it doesn t make sense to specify partitioning parameters e g when we write data out from Datasets DataFrames into jdbc tables or streaming ForeachWriters We probably should add checks against this in DataFrameWriter Add assertNotPartitioned check in DataFrameWriter
Behavioral decision,We should expose codahale metrics for the codegen source text size and how long it takes to compile The size is particularly interesting since the JVM does have hard limits on how large methods can get Metrics for codegen size and perf
Structural decision,Spark s SBT build currently uses a fork of the sbt pom reader plugin but depends on that fork via a SBT subproject which is cloned from https github com scrapcodes sbt pom reader tree ignore artifact id This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted In order to address these issues I propose to publish a pre built binary of our forked sbt pom reader plugin to Maven Central under the org spark project namespace Publish Spark s forked sbt pom reader to Maven Central
Behavioral decision,There s no corresponding python api for KMeansSummary it would be nice to have it Add KMeanSummary in KMeans of PySpark
Structural decision,DataFrameWriter insertInto includes some Analyzer stuff We should move it to Analyzer Move some Analyzer stuff to Analyzer from DataFrameWriter
Behavioral decision,Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines Making them public should be safe even if we change internal formats Make DefaultParamsReadable Writable public APIs
Structural decision,Use the latest Sparksession to replace the existing SQLContext in MLlib Replace SQLContext with SparkSession in MLlib
Structural decision,See parent task SPARK python converage pyspark ml linalg
Structural decision,See parent task SPARK python converage ml regression module
Structural decision,See parent task SPARK python converage ml classification module
Structural decision,See parent task SPARK python converage ml recommendation module
Structural decision,See parent task SPARK bryanc did this component python coverage ml feature
Structural decision,This issue replaces all deprecated SQLContext occurrences with SparkSession in ML MLLib module except the following two classes These two classes use SQLContext as their function arguments ReadWrite scala TreeModels scala Replace SQLContext with SparkSession in ML MLLib
Behavioral decision,See containing JIRA for details SPARK ML QA Scala APIs audit for feature
Behavioral decision,See containing JIRA for details SPARK ML QA Scala APIs audit for evaluation tuning
Behavioral decision,Support for partitioned parquet format in FileStreamSink was added in Spark now let s add support for partitioned csv json text format Add support for writing partitioned csv json text formats in Structured Streaming
Structural decision,Several classes and methods have been deprecated and are creating lots of build warnings in branch This issue is to identify and fix those items WithSGD classes Change to make class not deprecated object deprecated and public class constructor deprecated Any public use will require a deprecated API We need to keep a non deprecated private API since we cannot eliminate certain uses Python API streaming algs and examples Use in PythonMLlibAPI Change to using private constructors Streaming algs No warnings after we un deprecate the classes Examples Deprecate or change ones which use deprecated APIs MulticlassMetrics fields precision etc Eliminate MLlib build warnings from deprecations
Behavioral decision,This is an umbrella ticket to list issues I found with APIs for the release Spark SQL API audit
Non-existence - ban decision,We removed some classes in Spark If the user uses an incompatible library he may see ClassNotFoundException It s better to give an instruction to ask people using a correct version Display a better message for not finding classes removed in Spark
Behavioral decision,We should open up the APIs for converting between new old linear algebra types in spark mllib linalg Vector asML Vectors fromML same for Sparse Dense and for Matrices I made these private originally but they will be useful for users transitioning workloads Make the mllib ml linalg type conversion APIs public
Behavioral decision,We re using asML to convert the mllib vector matrix to ml vector matrix now Using as is more correct given that this conversion actually shares the same underline data structure As a result in this PR toBreeze will be changed to asBreeze This is a private API as a result it will not affect any user s application Change toBreeze to asBreeze in Vector and Matrix
Structural decision,This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming Here is the design doc for an initial version of the Kafka Source https docs google com document d t rWe x tq e AOfrsM qb m BRuv fel i PqR edit usp sharing Old description Structured streaming doesn t have support for kafka yet I personally feel like time based indexing would make for a much better interface but it s been pushed back to kafka https cwiki apache org confluence display KAFKA KIP Add a time based log index Structured streaming support for consuming from Kafka
Behavioral decision,Since ml evaluation has supported save load at Scala side supporting it at Python side is very straightforward and easy PySpark ml evaluation should support save load
Behavioral decision,Now picklers for both new and old vectors are implemented under PythonMLlibAPI To separate spark mllib from spark ml we should implement them under spark ml python instead I set the target to since those are private APIs Implement Python picklers for ml Vector and ml Matrix under spark ml python
Behavioral decision,SPARK makes KMeansModel store the clusters one per row KMeansModel load method needs to be updated in order to load models saved with Spark Make spark ml KMeansModel load backwards compatible
Structural decision,Generate currently does not support code generation Lets add support for CG and for it and its most important generators explode and json tuple Implement code generation for Generate
Behavioral decision,We should add an interface to the GLR summaries in Python for feature parity Python API for Generalized Linear Regression Summary
Non-existence - ban decision,LazyFileRegion was created so we didn t create a file descriptor before having to send the file see https issues apache org jira browse SPARK The change has been pushed back into Netty to support the same things under the DefaultFileRegion https github com netty netty issues https github com netty netty commit a b d c f b f e b a be It looks like that went into Final I believe at the time we created LazyFileRegion we were on Final and we are now using Final so we should be able to use the netty class directly Remove LazyFileRegion
Structural decision,Our current dataset registerTempTable does not actually materialize data So it should be considered as creating a temp view We can deprecate it and create a new method called dataset createTempView replaceIfExists Boolean The default value of replaceIfExists should be false For registerTempTable it will call dataset createTempView replaceIfExists true Deprecate registerTempTable and add dataset createTempView
Behavioral decision,Implement repartitionByColumn on DataFrame This will allow us to run R functions on each partition identified by column groups with dapply method SparkR Implement repartitionByColumn on DataFrame
Behavioral decision,In SparkR spark kmeans take a DataFrame with double columns This is different from other ML methods we implemented which support R model formula We should add support for that as well Support formula in spark kmeans in SparkR
Behavioral decision,validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning PySpark TrainValidationSplitModel should support validationMetrics
Behavioral decision,JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up including eliminating unnecessary sorting and the use of inefficient collections Improve performance of JSON schema inference s inferField step
Behavioral decision,The YarnShuffleService currently just picks a directly in the yarn local dirs to store the leveldb file YARN added an interface in hadoop getRecoverPath to get the location where it should be storing this We should change to use getRecoveryPath This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop YarnShuffleService should use YARN getRecoveryPath for leveldb location
Behavioral decision,Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination We should support it Subexpression elimination in wholestage codegen version of TungstenAggregate
Structural decision,This issues aims to add new FoldablePropagation optimizer that propagates foldable expressions by replacing all attributes with the aliases of original foldable expression Other optimizations will take advantage of the propagated foldable expressions e g EliminateSorts optimizer now can handle the following Case and Case is the previous implementation Literals and foldable expression e g ORDER BY abc Now Foldable ordinals e g SELECT abc Now ORDER BY Foldable aliases e g SELECT x abc y Now z ORDER BY x y z Before Add FoldablePropagation optimizer
Behavioral decision,We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work For example there are various options in parquet mr that users might want to set but the data source API does not expose a per job way to set it This patch propagates the user specified options also into Hadoop Configuration Propagate data source options to Hadoop configurations
Non-existence - ban decision,Since SPARK breaks behavior of HashingTF we should try to enforce good practice by removing the native hashingAlg option in spark ml and pyspark ml We can leave spark mllib and pyspark mllib alone Remove spark ml HashingTF hashingAlg option
Structural decision,It looks like the head master branch of Spark uses quite an old version of Jetty v There have been some announcement of security vulnerabilities notably in and there are versions of both and that address those We recently left a web ui port open and had the server compromised within days Albeit this upgrade shouldn t be the only security improvement made the current version is clearly vulnerable as is Upgrade Jetty to latest version of
Structural decision, Break SQLQuerySuite out into smaller test suites
Behavioral decision,In SPARK we switched to use GenericArrayData to store indices and values in vector matrix UDTs However GenericArrayData is not specialized for primitive types This might hurt MLlib performance badly We should consider either specialize GenericArrayData or use a different container cc cloud fan yhuai VectorUDT MatrixUDT should take primitive arrays without boxing
Behavioral decision,In current master we have ML methods in SparkR If we make this change we might want to avoid name collisions because they have different signature We can use ml kmeans ml glm etc Sorry for discussing API changes in the last minute But I think it would be better to have consistent signatures in SparkR cc shivaram josephkb yanboliang Make ML APIs in SparkR consistent
Structural decision,This is an umbrella ticket to reduce the difference between sql core and sql hive Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs Merge functionality in Hive module into SQL core module
Structural decision,In Spark we shouldn t have two parsers anymore There should be only a single one Merge HiveSqlAstBuilder and SparkSqlAstBuilder
Behavioral decision,The current benchmark framework runs a code block for several iterations and reports statistics However there is no way to exclude per iteration setup time from the overall results Allow custom timing control in microbenchmarks
Structural decision, Rename upstreams inputRDDs in WholeStageCodegen
Behavioral decision,Provide API for SVM algorithm for DataFrames I would recommend using OWL QN rather than wrapping spark mllib s SGD based implementation The API should mimic existing spark ml classification APIs spark ml API for linear SVM
Behavioral decision,This issue aims to implement assert true function It s Hive Generic UDF Function https github com apache hive blob master ql src java org apache hadoop hive ql udf generic GenericUDFAssertTrue java since The following is function description of Hive Please note that Hive s false values are false null and empty string But in Spark we intentionally designed to use implicit typecasting to BooleanType Add assert true function
Behavioral decision,This issue aims to expose Scala bround function in Python R API bround function is implemented in SPARK by extending current round function We used the following semantics from Hive https github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java Add bround function in Python R
Behavioral decision,The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics They are unnecessarily convoluted and we should be able to simplify them quite a bit This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on At a high level I d would like to create better abstractions for internal implementations as well as creating a simplified accumulator v external interface that doesn t involve a complex type hierarchy Simplify accumulators and task metrics
Behavioral decision,This issue aims to add bound function aka Banker s round by extending current round implementation Hive supports bround since Language Manual https cwiki apache org confluence display Hive LanguageManual UDF Add bround function
Behavioral decision,When you have an error in your R code using the RDD API you always get as error message Error in if returnStatus argument is of length zero This is not very useful and I think it might be better to catch the R exception and show it instead Improve error messages for RDD API
Behavioral decision,Right now filter push down only works with Project Aggregate Generate and Join they can t be pushed through many other plans Improve filter push down
Behavioral decision,We currently disable codegen for CaseWhen if the number of branches is greater than in CaseWhen MAX NUM CASES FOR CODEGEN It would be better if this value is a non public config defined in SQLConf spark sql codegen maxCaseBranches config option
Structural decision,Currently Union only takes intersect of the constraints from it s children all others are dropped we should try to merge them together Improve constraints propagation in Union
Behavioral decision,It will be great to have these SQL functions IFNULL NULLIF NVL NVL The meaning of these functions could be found in oracle docs SQL function IFNULL NULLIF NVL and NVL
Behavioral decision,In Spark DataFrame is an alias of Dataset Row MLlib API actually works for other types of Dataset so we should accept Dataset instead It maps to Dataset in Java This is a source compatible change Accept Dataset instead of DataFrame in MLlib APIs
Structural decision, refactor object operator framework to make it easy to eliminate serializations
Structural decision,In order to upgrade to Kryo we need to shade Kryo in our custom Hive fork Shade Kryo in our custom Hive fork
Structural decision,We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process Those name and what they are used for is very similar so I think we can merge them Also the original unescapeSQLString method may have a fault When u style character literals are passed to the method it s not unescaped successfully Merge ParserUtils and ParseUtils
Behavioral decision,Currently many functions do now show usages like the followings The only exceptions are cube grouping grouping id rollup window All functions should show usages by command DESC FUNCTION
Behavioral decision,Right now operations for an existing functions in SessionCatalog do not really check if the function exists We should add this check and avoid of doing the check in command SessionCatalog needs to check function existence
Structural decision,There is no unit test for KMeansSummary in spark ml Other items which could be fixed here Add Since version to KMeansSummary class Modify clusterSizes method to match GMM method to be robust to empty clusters in case we support that sometime See PR for SPARK Unit test for spark ml KMeansSummary
Behavioral decision,The time windowing function window was added to Datasets This JIRA is to track the status for the R Python and SQL API Dateset Time Windowing API for Python R and SQL
Structural decision, Decouple deserializer expression resolution from ObjectOperator
Behavioral decision,The toLocalIterator of RDD is super slow we should have a optimized implementation for Dataset DataFrame Add toLocalIterator for Dataset
Structural decision,We use a single object SparkRWrappers https github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala to wrap method calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakr ml r instead of spark ml api r Refactor GLMs code in SparkRWrappers
Structural decision,We use a single object SparkRWrappers https github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala to wrap method calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakr ml r instead of spark ml api r Refactor k means code in SparkRWrappers
Behavioral decision,While running a Spark job which is spilling a lot of data in reduce phase we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method Please see the stack trace below Stack trace org xerial snappy SnappyNative YJP arrayCopy Native Method org xerial snappy SnappyNative arrayCopy SnappyNative java org xerial snappy Snappy arrayCopy Snappy java org xerial snappy SnappyInputStream rawRead SnappyInputStream java org xerial snappy SnappyInputStream read SnappyInputStream java java io DataInputStream readFully DataInputStream java java io DataInputStream readLong DataInputStream java org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java org apache spark util collection unsafe sort UnsafeSorterSpillMerger loadNext UnsafeSorterSpillMerger java org apache spark sql execution UnsafeExternalRowSorter next UnsafeExternalRowSorter java org apache spark sql execution UnsafeExternalRowSorter next UnsafeExternalRowSorter java The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data which is expensive We should fix Snappy java to use with non JNI based System arrayCopy method in this case Significant amount of CPU is being consumed in SnappyNative arrayCopy method
Structural decision, Reimplement TypedAggregateExpression to DeclarativeAggregate
Non-existence - ban decision,Currently a method submitStage for waiting stages is called on every iteration of the event loop in DAGScheduler to submit all waiting stages but most of them are not necessary because they are not related to Stage status The case we should try to submit waiting stages is only when their parent stages are successfully completed This elimination can improve DAGScheduler performance Eliminate unnecessary submitStage call
Behavioral decision, Execute multiple Python UDFs in single batch
Non-existence - ban decision, remove trait Queryable
Non-existence - ban decision,remove consumeChild always create code for UnsafeRow and variables Simplify whole stage codegen interface
Behavioral decision,We only parse create function command In order to support native drop function command we need to parse it too Parse Drop Function DDL command
Behavioral decision,In naive Bayes we expect inputs to be individual observations In practice people may have the frequency table instead It is useful for us to support instance weights to handle this case Support weighted instances in naive Bayes
Structural decision,It would be nice to refactor the MemoryStore so that it can be unit tested without constructing a full BlockManager or needing to mock tons of things Refactor MemoryStore to be testable independent of BlockManager
Behavioral decision, Improve SparkStatusTracker to also track executor information
Structural decision,They were kept in SQLContext implicits object for binary backward compatibility in the Spark x series It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits Move StringToColumn implicit class into SQLImplicits
Behavioral decision,Currently for the key that can not fit within a long we build a hash map for UnsafeHashedRelation it s converted to BytesToBytesMap after serialization and deserialization We should build a BytesToBytesMap directly to have better memory efficiency Build BytesToBytesMap in HashedRelation
Behavioral decision,Currently spark history server REST API provides functionality to query applications by application start time range based on minDate and maxDate query parameters but it lacks support to query applications by their end time In this Jira we are proposing optional minEndDate and maxEndDate query parameters and filtering capability based on these parameters to spark history server REST API This functionality can be used for following queries Applications finished in last x minutes Applications finished before y time Applications finished between x time to y time Applications started from x time and finished before y time For backward compatibility we can keep existing minDate and maxDate query parameters as they are and they can continue support filtering based on start time range Add functionality in spark history sever API to query applications by end time
Behavioral decision,Per our discussion on the mailing list please see here http mail archives apache org mod mbox spark dev mbox CCA g F aVRBH WyyK nvBSLCMPtSdUuL Ge WW DnmnvY SXg mail gmail com E it would be nice to specify a custom coalescing policy as the current coalesce method only allows the user to specify the number of partitions and we cannot really control much The need for this feature popped up when I wanted to merge small files by coalescing them by size Add support for custom coalescers
Structural decision, make SubqueryHolder an inner class
Non-existence - ban decision,When SortOrder does not contain any reference it has no effect on the sorting Remove the noop SortOrder in Optimizer Remove noop SortOrder in Sort
Non-existence - ban decision,Per nongli s suggestions We should do these things Remove the non vectorized parquet reader code Support the remaining types just big decimals Move the logic to determine if our parquet reader can be used to planning Only complex types should fall back to the parquet mr reader Cleanup Extend the Vectorized Parquet Reader
Behavioral decision,We should add support for caching serialized data off heap within the same process i e using direct buffers or sun misc unsafe I ll expand this JIRA later with more detail filing now as a placeholder Add support for off heap caching
Behavioral decision,When a block is persisted in the MemoryStore at a serialized storage level the current MemoryStore putIterator code will unroll the entire iterator as Java objects in memory then will turn around and serialize an iterator obtained from the unrolled array This is inefficient and doubles our peak memory requirements Instead I think that we should incrementally serialize blocks while unrolling them A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk However I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case Incrementally serialize blocks while unrolling them in MemoryStore
Behavioral decision,This is to support order by position in SQL e g This should be controlled by config option spark sql groupByOrdinal Support group by ordinal in SQL
Structural decision,Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment We can call the new module mllib local which might contain local models in the future The major issue is to remove dependencies on user defined types The package name will be changed from mllib to ml For example Vector will be changed from org apache spark mllib linalg Vector to org apache spark ml linalg Vector The return vector type in the new ML pipeline will be the one in ML package however the existing mllib code will not be touched As a result this will potentially break the API Also when the vector is loaded from mllib vector by Spark SQL the vector will automatically converted into the one in ml package Separate out local linear algebra as a standalone module without Spark dependency
Behavioral decision,Recently the fast serialization has been introduced to collecting DataFrame Dataset The same technology can be used on collect limit operator too Apply fast serialization on collect limit
Structural decision,Logging was made private in Spark If we move it then users would be able to create a Logging trait themselves to avoid changing their own code Alternatively we can also provide in a compatibility package that adds logging Move org apache spark Logging into org apache spark internal Logging
Behavioral decision,Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo e g RDDs whose key value and or combiner types are primitives arrays of primitives or strings This is likely to result in a large performance gain for many RDD API workloads Automatically use Kryo serializer when shuffling RDDs with simple types
Behavioral decision,This continues the work of SPARK SPARK and SPARK to expose R like model summary in more family and link functions Expose R like summary statistics in SparkR glm for more family and link functions
Behavioral decision,Instead of storing serialized blocks in individual ByteBuffers the BlockManager should be capable of storing a serialized block in multiple chunks each occupying a separate ByteBuffer This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks Our current serialization code uses a ByteBufferOutputStream which doubles and re allocates its backing byte array this increases the peak memory requirements during serialization since we need to hold extra memory while expanding the array In addition we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array so a megabyte serialized block may actually consume megabytes of memory After switching to storing blocks in multiple chunks we ll be able to efficiently trim the backing buffers so that no space is wasted This change is also a prerequisite to being able to cache blocks which are larger than GB although full support for that depends on several other changes which have not bee implemented yet Store serialized blocks as multiple chunks in MemoryStore
Structural decision,Currently Spark allows only a few cluster managers viz Yarn Mesos and Standalone But as Spark is now being used in newer and different use cases there is a need for allowing other cluster managers to manage spark components One such use case is embedding spark components like executor and driver inside another process which may be a datastore This allows colocation of data and processing Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again So this JIRA requests two functionalities Support for external cluster managers Allow a cluster manager to clean up the tasks without taking the parent process down Add support for pluggable cluster manager
Non-existence - ban decision,Our code can go through SessionState catalog This brings two small benefits Reduces internal dependency on SQLContext Removes another public method in Java Java does not obey package private visibility More importantly according to the design in SPARK we d need to claim this catalog function for the user facing public functions rather than having an internal field Remove SQLContext catalog internal method
Non-existence - ban decision,In general it is better for internal classes to not depend on the external class in this case SQLContext to reduce coupling between user facing APIs and the internal implementations Remove some internal classes dependency on SQLContext
Non-existence - ban decision,DescribeCommand should just take a TableIdentifier and ask the metadata catalog for table s information Remove DescribeCommand s dependency on LogicalPlan
Non-existence - ban decision,We introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these We still plan to implement a full local mode but it s probably going to be fairly different from what the current iterator based local mode would look like Let s just remove them for now and we can always re introduced them in the future by looking at branch Remove org apache spark sql execution local
Behavioral decision,When we generate code for join we copy the output row because there could be multiple output row from single input row We could avoid this copy when there is no join or the join will not generate multiple output rows from single input row Avoid the copy in whole stage codegen when there is no joins
Behavioral decision,When reading data from the DiskStore and attempting to cache it back into the memory store we should guard against race conditions where multiple readers are attempting to re cache the same block in memory Guard against race condition when re caching spilled bytes in memory
Behavioral decision,When generated code accesses a ColumnarBatch object it is possible to get values of each column from ColumnVector instead of calling getRow Direct consume ColumnVector in generated code when ColumnarBatch is used
Non-existence - ban decision,If the Window does not have any window expression it is useless It might happen after column pruning Eliminate Unnecessary Window
Non-existence - ban decision,We are using SELECT as a dummy table when the table is used for SQL statements in which a table reference is required but the contents of the table are not important For example In this case we will see a useless Project whose projectList is empty after executing ColumnPruning rule Remove Project when its projectList is Empty
Behavioral decision,RandomSampler sample currently accepts iterator as input and output another iterator This makes it inappropriate to use in wholestage codegen of Sampler operator We should add non iterator interface to RandomSampler Add non iterator interface to RandomSampler
Behavioral decision,Push down the predicate through the Window operator In this JIRA predicates are pushed through Window if and only if the following conditions are satisfied Predicate involves one and only one column that is part of window partitioning key Window partitioning key is just a sequence of attributeReferences i e none of them is an expression Predicate must be deterministic Predicate Push Down Through Window Operator
Non-existence - ban decision,projectList is useless Remove it from the class Window It simplifies the codes in Analyzer and Optimizer Remove projectList from Windows
Non-existence - ban decision,Today both the MemoryStore and DiskStore implement a common BlockStore API but I feel that this API is inappropriate because it abstracts away important distinctions between the behavior of these two stores For instance the disk store doesn t have a notion of storing deserialized objects so it s confusing for it to expose object based APIs like putIterator and getValues instead of only exposing binary APIs and pushing the responsibilities of serialization and deserialization to the client As part of a larger BlockManager interface cleanup I d like to remove the BlockStore API and refine the MemoryStore and DiskStore interfaces to reflect more narrow sets of responsibilities for those components Remove BlockStore interface to more cleanly reflect different memory and disk store responsibilities
Behavioral decision,When a cached block is spilled to disk and read back in serialized form i e as bytes the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch Therefore I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels There are two places where we request serialized bytes from the BlockStore getLocalBytes which is only called when reading local copies of TorrentBroadcast pieces Broadcast pieces are always cached using a serialized storage level so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store the non shuffle block branch in getBlockData which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms since those blocks seem more likely to be read in local computation Therefore I think this is a safe change Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills
Behavioral decision, QueryPlan expressions should always include all expressions
Structural decision,Right now we use PhysicalRDD for both existing RDD and data sources they are becoming much different we should use different physical plans for them Use different physical plan for existing RDD and data sources
Structural decision,A majority of Spark SQL queries likely run though HadoopFSRelation however there are currently several complexity and performance problems with this path Simplify and Speedup HadoopFSRelation
Non-existence - ban decision,In preparation for larger refactorings I think that we should remove the confusing returnValues option from the BlockStore put APIs returning the value is only useful in one place caching and in other situations such as block replication it s simpler to put and then get Remove returnValues from BlockStore APIs
Behavioral decision,We need to submit another PR against Spark to call the task failure callbacks before Spark calls the close function on various output streams For example we need to intercept an exception and call TaskContext markTaskFailed before calling close in the following Changes to Spark should include unit tests to make sure this always work in the future Invoke task failure callbacks before calling outputstream close
Non-existence - ban decision,Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin it has the same implementation as LeftSemiJoinBNL we should remove that Remove LeftSemiJoinBNL
Behavioral decision,After SPARK we should add Python API for generalized linear regression Python API for GeneralizedLinearRegression
Structural decision,We already have internal APIs for Hive to do this We should do it for SQLContext too so we can merge these code paths one day Track current database in SQL HiveContext
Non-existence - ban decision,Remove all the deterministic conditions in a Filter that are contained in the Child Prune Filters based on Constraints
Structural decision,This is just a clean up task Today there are all these fields in SQLContext that are not organized in any particular way However since each SQLContext is a session many of these fields are actually isolated per session To minimize the size of these context files and provide a logical grouping that makes more sense I propose that we move these fields into its own class called SessionState Refactor Move SQLContext HiveContext per session state to separate class
Behavioral decision,After SPARK we should add Python API for MaxAbsScaler Python API for MaxAbsScaler
Behavioral decision,As part of Spark we want to create a stable API foundation for Dataset to become the main user facing API in Spark This ticket tracks various tasks related to that The main high level changes are Merge Dataset DataFrame Create a more natural entry point for Dataset SQLContext HiveContext are not ideal because of the name SQL Hive and SparkContext is not ideal because of its heavy dependency on RDDs First class support for sessions First class support for some system catalog See the design doc for more details Dataset oriented API evolution in Spark
Non-existence - ban decision,With column pruning rule in optimizer we will introduce redundant project for some cases We should prevent it Remove redundant project in colum pruning rule
Structural decision,TaskContext supports task completion callback which gets called regardless of task failures However there is no way for the listener to know if there is an error This ticket proposes adding a new listener that gets called when a task fails Add a task failure listener to TaskContext
Structural decision,Following SPARK we can add a wrapper for naive Bayes in SparkR R s naive Bayes implementation is from package e with signature It should be easy for us to match the parameters Naive Bayes wrapper in SparkR
Behavioral decision,I think model summary interface which is available in Spark s scala Java and R interfaces should also be available in the python interface Similar to SPARK https issues apache org jira browse SPARK Expose ml summary function in PySpark for classification and regression models
Behavioral decision,Support queries that JOIN tables with USING clause SELECT from table JOIN table USING Support USING clause in JOIN
Behavioral decision,Currently the sbin start stop mesos dispatcher scripts only assume there is one mesos dispatcher launched but potentially users that like to run multi tenant dispatcher might want to launch multiples It also helps local development to have the ability to launch multiple ones Add support for launching multiple Mesos dispatchers
Behavioral decision,An broadcasted table could be used multiple times in a query we should cache them Avoid duplicated broadcasts
Behavioral decision,The current implementation of statistics of UnaryNode does not considering output for example Project we should considering it to have a better guess Considering output for statistics of logical plan
Behavioral decision,This bug is reported by Stuti Awasthi https www mail archive com user spark apache org msg html The lossSum has possibility of infinity because we do not standardize the feature before fitting model we should support feature standardization Another benefit is that standardization will improve the convergence rate AFTSurvivalRegression should support feature standardization
Non-existence - ban decision,Union Distinct has two Distinct that generate two Aggregation in the plan Remove an Extra Distinct in Union
Non-existence - ban decision,For lots of SQL operators we have metrics for both of input and output the number of input rows should be exactly the number of output rows of child we could only have metrics for output rows After we improve the performance using whole stage codegen the overhead of SQL metrics are not trivial anymore we should avoid that if it s not necessary Some of the operator does not have SQL metrics we should add that for them For those operators that have the same number of rows from input and output for example Projection we may don t need that Remove duplicated SQL metrics
Non-existence - ban decision,in newMutableProjection it will fallback to InterpretedMutableProjection if failed to compile Since we remove the configuration for codegen we are heavily reply on codegen also TungstenAggregate require the generated MutableProjection to update UnsafeRow should remove the fallback which could make user confusing see the discussion in SPARK Remove fallback in codegen
Behavioral decision,Spark SQL should collapse adjacent Repartition operators and only keep the last one Collapse adjacent Repartition operations
Behavioral decision,We currently delegate most DDLs directly to Hive through NativePlaceholder in HiveQl scala In Spark we want to provide native implementations for DDLs for both SQLContext and HiveContext The first step is to properly parse these DDLs and then create logical commands that encapsulate them The actual implementation can still delegate to HiveNativeCommand As an example we should define a command for RenameTable with the proper fields and just delegate the implementation to HiveNativeCommand we might need to track the original sql query in order to run HiveNativeCommand but we can remove the sql query in the future once we do the next step Once we flush out the internal persistent catalog API we can then switch the implementation of these newly added commands to use the catalog API Create native DDL commands
Structural decision,When you define a class inside of a package object the name ends up being something like org mycompany project package MyClass However when reflect on this we try and load org mycompany project MyClass Support for classes defined in package objects
Non-existence - ban decision, remove GenericInternalRowWithSchema
Behavioral decision,As of Spark Spark SQL internally has only a limited catalog and does not support any of the DDLs This is an umbrella ticket to introduce an internal API for a system catalog and the associated DDL implementations using this API Native database table system catalog
Behavioral decision,The StateDStream currently does not provide the batch time as input to the state update function This is required in cases where the behavior depends on the batch start time We Conviva have been patching it manually for the past several Spark versions but we thought it might be useful for others as well Add API for updateStateByKey to provide batch time as input
Behavioral decision,Implement a simple wrapper in SparkR to support k means K means wrapper in SparkR
Behavioral decision,Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis Survival analysis in SparkR
Behavioral decision,Parquet files benefit from vectorized decoding ColumnarBatches have been designed to support this This means that a single encoded parquet column is decoded to a single ColumnVector Vectorize parquet decoding using ColumnarBatch
Structural decision,As benchmarked and discussed here https github com apache spark pull files r Benefits from codegen the declarative aggregate function could be much faster than imperative one we should re implement all the builtin aggregate functions as declarative one For skewness and kurtosis we need to benchmark it to make sure that the declarative one is actually faster than imperative one Reimplement stat functions as declarative function
Structural decision,I was investingating progress in SPARK and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module Java Scala s examples SPARK use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK Does the class have different name in PySpark maybe Also I couldn t find any JIRA task to saying it need to be implemented Is it by design that the TrainValidationSplit estimator is not ported to PySpark If not that is if the estimator needs porting then I would like to contribute TrainValidationSplit is missing in pyspark ml tuning
Non-existence - ban decision,CacheManager directly calls MemoryStore unrollSafely and has its own logic for handling graceful fallback to disk when cached data does not fit in memory However this logic also exists inside of the MemoryStore itself so this appears to be unnecessary duplication Thanks to the addition of block level read write locks we can refactor the code to remove the CacheManager and replace it with an atomic getOrElseUpdate BlockManager method Remove CacheManager and replace it with new BlockManager getOrElseUpdate method
Behavioral decision,In Spark MLlib provides logistic regression and linear regression with L L elastic net regularization We want to expand the support of generalized linear models GLMs in e g Poisson Gamma families and more link functions SPARK implements a GLM solver for the case when the number of features is small We also need to design an interface for GLMs In SparkR we can simply follow glm or glmnet On the Python Scala Java side the interface should be consistent with LinearRegression and LogisticRegression e g from GeneralizedLinearModel Estimator interface for generalized linear models GLMs
Behavioral decision,As a pre requisite to off heap caching of blocks we need a mechanism to prevent pages blocks from being evicted while they are being read With on heap objects evicting a block while it is being read merely leads to memory accounting problems because we assume that an evicted block is a candidate for garbage collection which will not be true during a read but with off heap memory this will lead to either data corruption or segmentation faults To address this we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely I propose to do this in two phases first add a safe conservative approach in which all BlockManager get calls implicitly increment the reference count of blocks and where tasks references are automatically freed upon task completion This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions In phase two we should incrementally add release calls in order to fix the eviction of unreferenced blocks The latter change may need to touch many different components which is why I propose to do it separately in order to make the changes easier to reason about and review Use reference counting to prevent blocks from being evicted during reads
Structural decision,It would be easier to fix bugs and maintain the ec script separately from Spark releases For more information see https issues apache org jira browse SPARK Move spark ec scripts to AMPLab
Behavioral decision,Right now We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions the result projection of join could be very expensive if they generate lots of rows could be reduce mostly by condition SortMergeJoin and BroadcastHashJoin should support condition
Structural decision,Our current Intersect physical operator simply delegates to RDD intersect We should remove the Intersect physical operator and simply transform a logical intersect into a semi join This way we can take advantage of all the benefits of join implementations e g managed memory code generation broadcast joins Rewrite Intersect phyiscal plan using semi join
Behavioral decision,Add hash function for SparkR SparkR support hash function
Behavioral decision,This is beneficial just from a code testability point of view to be able to exercise individual components Also makes it easy to benchmark it It would be able to read data without need to create al the associate hadoop input split etc components Expose API on UnsafeRowRecordReader to just run on files
Non-existence - ban decision,Before https github com apache spark pull submitJob would create a separate thread to wait for the job result submitJobThreadPool was a workaround in ReceiverTracker to run these waiting job result threads Now https github com apache spark pull has been merged to master and resolved this blocking issue submitJobThreadPool can be removed now Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result
Non-existence - ban decision,MLlib s Transformer uses the deprecated callUDF API Remove the use of the deprecated callUDF in MLlib
Structural decision,Right now numFields will be passed in by pointTo then bitSetWidthInBytes is calculated making pointTo a little bit heavy It should be part of constructor of UnsafeRow The numFields of UnsafeRow should not changed by pointTo
Behavioral decision, Support window functions in SQLContext
Behavioral decision, Support intersect except in Hive SQL
Behavioral decision,cc nongli please attach the design doc bucketed table support
Behavioral decision,We can provides the option to choose JSON parser can be enabled to accept quoting of all character or not For example if JSON file that includes not listed by JSON backslash quoting specification it returns corrupt record This issue similar to HIVE HIVE Add option to accept quoting of all character backslash quoting mechanism
Structural decision,Right now the Java users cannot use ActorHelper because it uses special Scala syntax This patch just refactored the codes to provide Java API and add an example Refactor ActorReceiver to support Java
Non-existence - ban decision,Input SELECT FROM jdbcTable WHERE col xxx Current plan Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter
Structural decision,We should update to the latest version of Zinc in order to match our SBT version Upgrade Zinc from to
Non-existence - ban decision,Remove spark deploy mesos zookeeper dir and use existing configuration spark deploy zookeeper dir for Mesos cluster mode Remove spark deploy mesos zookeeper dir and use spark deploy zookeeper dir
Non-existence - ban decision,Remove spark deploy mesos zookeeper url and use existing configuration spark deploy zookeeper url for Mesos cluster mode Remove spark deploy mesos zookeeper url and use spark deploy zookeeper url
Non-existence - ban decision,Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode configuration for cluster mode Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode
Behavioral decision,We should add SQLUserDefinedType support for encoder Add SQLUserDefinedType support for encoder
Behavioral decision,CSV is the most common data format in the small data world It is often the first format people want to try when they see Spark on a single node Making this built in for the most common source can provide a better experience for first time users We should consider inlining https github com databricks spark csv Have a built in CSV data source implementation
Behavioral decision,Creating an actual logical physical operator for range for matching the performance of RDD Range APIs Compared with the old Range API the new version is times faster than the old version Improve performance of Range APIs via adding logical physical operators
Non-existence - ban decision,As discussed here https github com apache spark pull it might need to implement unhandledFilter to remove duplicated Spark side filtering Implement unhandledFilter interface
Behavioral decision,distinct and unique drop duplicated rows on all columns While dropDuplicates can drop duplicated rows on selected columns Implement dropDuplicates method of DataFrame in SparkR
Behavioral decision,Like shuffle file encryption in SPARK spills data should also be encrypted Support shuffle spill encryption in Spark
Behavioral decision,IsNotNull filter is not being pushed down for JDBC datasource It looks it is SQL standard according to SQL SQL SQL and SQL x and I believe most databases support this isnotnull operator not pushed down for JDBC datasource
Structural decision,Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases Use sqlContext from MLlibTestSparkContext for spark ml test suites
Non-existence - ban decision,There have been continuing requests e g SPARK for allowing users to extend and modify MLlib models and algorithms If you are a user who needs these changes please comment here about what specifically needs to be modified for your use case Remove final from classes in spark ml trees and ensembles where possible
Behavioral decision, Support UnsafeRow in LocalTableScan
Behavioral decision, Support UnsafeRow in Coalesce Except Intersect
Behavioral decision, Support UnsafeRow in MapPartitions MapGroups CoGroup
Behavioral decision,There are still some SparkPlan does not support UnsafeRow or does not support well Support UnsafeRow in all SparkPlan if possible
Behavioral decision,Similar to Dataset transform DataFrame transform function
Behavioral decision,mutate in the dplyr package supports adding new columns and replacing existing columns But currently the implementation of mutate in SparkR supports adding new columns only Also make the behavior of mutate more consistent with that in dplyr Throw error message when there are duplicated column names in the DataFrame being mutated when there are duplicated column names in specified columns by arguments the last column of the same name takes effect Enhance mutate to support replace existing columns
Behavioral decision, Implement drop method for DataFrame in SparkR
Behavioral decision,Created a new private variable boundTEncoder that can be shared by multiple functions RDD select and collect Replaced all the queryExecution analyzed by the function call logicalPlan A few API comments are using wrong class names e g DataFrame or parameter names e g n A few API descriptions are wrong e g mapPartitions SQL Code refactoring and comment correction in Dataset APIs
Structural decision,Kafka already released and it introduce new consumer API that not compatible with old one So I added new consumer api I made separate classes in package org apache spark streaming kafka v with changed API I didn t remove old classes for more backward compatibility User will not need to change his old spark applications when he uprgade to new Spark version Please rewiew my changes Update KafkaDStreams to new Kafka Consumer API
Behavioral decision,Currently we support read df write df jsonFile parquetFile in SQLContext we should support more external data source API such as read json read parquet read orc read jdbc read csv and so on Some of the exist API is deprecated and will remove at Spark we should also deprecate them at SparkR Note we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style DataFrameReader API http spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API http spark apache org docs latest api scala index html org apache spark sql DataFrameWriter Support more external data source API in SparkR
Structural decision, Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver
Structural decision, Change numPartitions in RDD to be getNumPartitions to be consistent with pyspark scala
Structural decision,I think that we should upgrade from Tachyon to in order to get the fix for https tachyon atlassian net browse TACHYON Upgrade Tachyon dependency to
Behavioral decision,Add isNaN to Column for SparkR Column should has three related variable functions isNaN isNull isNotNull Replace DataFrame isNaN with DataFrame isnan at SparkR side Because DataFrame isNaN has been deprecated and will be removed at Spark Add isnull to DataFrame for SparkR DataFrame should has two related functions isnan isnull Fix usage of isnan isNaN
Structural decision,Change cumeDist cume dist denseRank dense rank percentRank percent rank rowNumber row number There are two reasons that we should make this change We should follow the naming convention rule of R http www inside r org node Spark DataFrame has deprecated the old convention such as cumeDist and will remove it in Spark It s better to fix this issue before release otherwise we will make breaking API change Rename some window rank function names for SparkR
Behavioral decision,Spark SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum Stddev Variance etc should support columnName as arguments
Structural decision, unify GetStructField and GetInternalRowField
Behavioral decision,repartition Returns a new Dataset that has exactly numPartitions partitions coalesce Returns a new Dataset that has exactly numPartitions partitions Similar to coalesce defined on an RDD this operation results in a narrow dependency e g if you go from partitions to partitions there will not be a shuffle instead each of the new partitions will claim of the current partitions SQL Support coalesce and repartition in Dataset APIs
Behavioral decision,Implement struct encode decode in SparkR as documented at http spark apache org docs latest api scala index html org apache spark sql functions Implement struct encode decode in SparkR
Behavioral decision,Collection functions documented at http spark apache org docs latest api scala index html org apache spark sql functions are size explode array contains and sort array size explode are already implemented array contains and sort array are to be implemented Implement collection functions in SparkR
Behavioral decision,We want to support JSON serialization of vectors in order to support SPARK JSON serialization of Vectors
Structural decision, consolidate ExpressionEncoder tuple and Encoders tuple
Behavioral decision,RowEncoder doesn t support UserDefinedType now We should add the support for it. Database application is important to focus in this issue Add UserDefinedType support to RowEncoder
Behavioral decision,Random Forests have feature importance but GBT do not It would be great if we can add feature importance to GBT as well Perhaps the code in Random Forests can be refactored to apply to both types of ensembles See https issues apache org jira browse SPARK Feature Importance for GBT
Behavioral decision, Add Java API for trackStateByKey
Behavioral decision,If it returns Text we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF String without extra string decoding and encoding WholeTextFileRDD should return Text rather than String
Non-existence - ban decision, Remove OpenHashSet for the old aggregate
Non-existence - ban decision,We don t sufficiently test the path work well Remove the option to turn off unsafe and codegen
Behavioral decision,Implement Python API for bisecting k means Python API for bisecting k means
Behavioral decision,We deprecated runs in Spark SPARK In we can either remove runs or make it no effect with warning messages So we can simplify the implementation I prefer the latter for better binary compatibility Make runs no effect in k means
Non-existence - ban decision, Remove the internal implicit conversion from Expression to Column in functions scala
Non-existence - ban decision, Remove implicit conversion from Expression to Column
Non-existence - ban decision,DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame This has been fairly confusing to a few new contributors Since it doesn t buy us much we should just remove that implicit conversion Remove the internal implicit conversion from LogicalPlan to DataFrame
Behavioral decision,Umbrella ticket to walk through all newly introduced APIs to make sure they are consistent SQL API audit for Spark
Non-existence - ban decision,Since we have bytes as number of records in the beginning of a page then the address can not be zero so we do not need the bitset Remove Bitset in BytesToBytesMap
Non-existence - ban decision,We should remove methods for variance stddev skewness GroupedData should only keep common first order statistics
Structural decision,These two classes should be public since they are used in public code Make DataFrameHolder and DatasetHolder public
Behavioral decision, Add R API for stddev variance
Behavioral decision,Add Java friendly API for StreamingListener Add JavaStreamingListener
Non-existence - ban decision,Since SPARK is resolved MapPartitionWithPrepare is not needed anymore Remove PrepareRDD
Behavioral decision,DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications Add a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY
Behavioral decision,In order to lay the groundwork for proper off heap memory support in SQL Tungsten we need to extend our MemoryManager to perform bookkeeping for off heap memory Add support for off heap memory to MemoryManager
Behavioral decision,We use scala collection mutable BitSet in BroadcastNestedLoopJoin now We should use Spark s BitSet Use Spark BitSet in BroadcastNestedLoopJoin
Structural decision,runs introduces extra complexity and overhead in MLlib s k means implementation I haven t seen much usage with runs not equal to We can deprecate this method in and remove or void it in It helps us simplify the implementation Deprecate runs in k means
Behavioral decision,Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made S does not support flushing of data data is written once the stream is actually closed In case of failure the data for the last minute default rolling interval will not be properly written Therefore we need a flag to close the stream after the write so that we achieve read after write consistency Flag to close Write Ahead Log after writing
Behavioral decision,We should add text to DataFrameReader and DataFrameWriter Python API for text data source
Behavioral decision,This is the first cut implementation of trackStateByKey new improvement state management method in Spark Streaming See the epic jira for more details https issues apache org jira browse SPARK Implement trackStateByKey for improved state management
Structural decision, Mark all Stage ResultStage ShuffleMapStage internal state as private
Structural decision,Update the tachyon client dependency from to There are no new dependencies added or Spark facing APIs changed Upgrade Tachyon dependency to
Behavioral decision,As part of the work to implement SPARK it would be nice to have the network library efficiently stream data over a connection Currently all it has is the shuffle data protocol which is not very efficient for large files it requires the whole file to be buffered on the receiver side before the receiver can do anything For large files that comes at a huge cost in memory You can chunk large files but that requires the client to ask for each chunk separately Instead a similar approach but allowing the data to be processed as it arrives would be a lot more efficient and make it easier to implement the file server in the referenced bug Support streaming data using network library
Structural decision,Right now we stack a new URLClassLoader when a user add a jar through SQL s add jar command This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar For example In this case when we lookup class B we will not be able to find class A because Jar is the parent of Jar Use a single URLClassLoader for jars added through SQL s ADD JAR command
Structural decision,We use KCL in the current master KCL added integration with Kinesis Producer Library KPL and support auto de aggregation It would be great to upgrade KCL to the latest stable version Note that the latest version is and restored compatibility with dynamodb streams kinesis adapter which was broken in See https github com awslabs amazon kinesis client release notes tdas brkyvz Please recommend a version for upgrade Upgrade Kinesis Client Library to the latest stable version
Behavioral decision,Also SQLContext newSession Add getOrCreate for SparkContext SQLContext for Python
Non-existence - ban decision,For a variety of reasons we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi Remove DeveloperApi annotation from private classes
Non-existence - ban decision,In the spirit of SPARK we should clean up the use of SPARK HOME and if possible remove it entirely We need to look through what this is used for One use was allowing applications to run different versions of Spark in standalone mode For instance someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark This use case is not widely used and maybe should just be removed The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose If there are other legitimate reasons for SPARK HOME we can keep it around we need to audit the uses of it Clean up and clarify use of SPARK HOME
Non-existence - ban decision,For some use cases it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts At here root SQLContext means the first SQLContext that gets created Introduce a mechanism to ban creating new root SQLContexts in a JVM
Structural decision,We should share the SQLTab across sessions SQLTab should be shared by across sessions
Behavioral decision,We should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel This will be a temp fix until we have proper evaluators defined for clustering Add computeCost to KMeansModel in spark ml
Behavioral decision,The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier Add computeCost and clusterCenters to KMeansModel in spark ml package
Structural decision,The new netty RPC still behaves too much like akka it requires both client e g an executor and server e g the driver to listen for incoming connections That is not necessary since sockets are full duplex and RPCs should be able to flow either way on any connection Also because the semantics of the netty based RPC don t exactly match akka you get weird issues like SPARK Supporting a client only mode also reduces the number of ports Spark apps need to use Netty based RPC env should support a client only mode
Behavioral decision,The serialize will be called by actualSize and append we should use UnsafeProjection before unrolling Avoid the serialization multiple times during unrolling of complex types
Structural decision,Matches more closely with ImperativeAggregate Rename ExpressionAggregate DeclarativeAggregate
Non-existence - ban decision,typeId is not needed in columnar cache it s confusing to having them Remove typeId in columnar cache
Behavioral decision,There is support for message handler in Direct Kafka Stream which allows arbitrary T to be the output of the stream instead of Array Byte This is a very useful function therefore should exist in Kinesis as well Add MessageHandler to KinesisUtils createStream similar to Direct Kafka
Behavioral decision,Currently we try to support multiple sessions in SQL within a Spark Context but it s broken and not complete We should isolate these for each session current database of Hive SQLConf UDF UDAF UDTF temporary table For added jar and cached tables they should be accessible for all sessions Improve session management for SQL
Structural decision,SPARK uses network module to implement RPC However there are some configurations named with spark shuffle prefix in the network module We should refactor them and make sure the user can control them in shuffle and RPC separately Separate configs between shuffle and RPC
Structural decision,Refactoring Instance case class out from LOR and LIR and also cleaning up some code Refactoring Instance out from LOR and LIR and also cleaning up some code
Structural decision,The idea is that most of the logic of calling Python actually has nothing to do with RDD it is really just communicating with a socket there is nothing distributed about it and it is only currently depending on RDD because it was written this way If we extract that functionality out we can apply it to area of the code that doesn t depend on RDDs and also make it easier to test Refactor PythonRDD to decouple iterator computation from PythonRDD
Behavioral decision,The following method in LogisticRegressionModel is marked as private which prevents users from creating a summary on any given data set Check here https github com feynmanliang spark blob d fa c e f b a d cd mllib src main scala org apache spark ml regression LinearRegression scala L This method is definitely necessary to test model performance By the way the name evaluate is already pretty good for me mengxr Could you check this Thx Make Logistic Linear Regression Model evaluate method public
Behavioral decision,After SPARK we should add Python API for AFTSurvivalRegression Python API for AFTSurvivalRegression
Non-existence - ban decision,Bagel has been deprecated and we haven t done any changes to it There is no need to run those tests Remove Bagel test suites
Non-existence - ban decision,As of https issues apache org jira browse SPARK we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them We should remove the legacy log syncing code since this is a blocker to disabling Worker Master SSH on Jenkins Remove legacy SCP based Jenkins log archiving code
Behavioral decision,Hive already supports this according to https issues apache org jira browse HIVE Currently Spark sql still supports only primitive types collect list and collect set should accept struct types as argument
Behavioral decision,The BlockMatrix multiply sends each block to all the corresponding columns of the right BlockMatrix even though there might not be any corresponding block to multiply with Some optimizations we can perform are Simulate the multiplication on the driver and figure out which blocks actually need to be shuffled Send the block once to a partition and join inside the partition rather than sending multiple copies to the same partition Decrease communication in BlockMatrix multiply and increase performance
Structural decision,The name weights becomes confusing as we are supporting weighted instanced As discussed in https github com apache spark pull we want to deprecate weights and use coefficients instead Deprecate but do not remove weights Only make changes under spark ml deprecate weights and use coefficients instead in ML models
Structural decision,Really simple request to upgrade fastutil to x The current version x has some minor API s in the Object xxOpenHashMap structures which is used in many places in Spark and has been marked deprecated Plus there is a conflict with another library we are using saddle http saddle github io which uses a newer version of fastutil I d be happy to send a PR I guess a bigger question is do you want to keep using fastutil SPARK but Spark uses more than just hashmaps so that probably requires another discussion Remove Fastutil
Structural decision,We implemented dspr with sparse vector support in RowMatrix This method is also used in WeightedLeastSquares and other places It would be useful to move it to linalg BLAS move RowMatrix dspr to BLAS
Structural decision,SPARK need to generate random data which follow Weibull distribution Add WeibullGenerator for RandomDataGenerator
Non-existence - ban decision,Currently There will be ConvertToSafe for PythonUDF that s not needed actually PythonUDF could process UnsafeRow
Structural decision,This was recently released and it has many improvements especially the following quote Python side IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes objects and members This makes Py J an ideal tool to explore complex Java APIs e g the Eclipse API Thanks to jonahkichwacoders quote Normally we wrap all the APIs in spark but for the ones that aren t this would make it easier to offroad by using the java proxy objects Upgrade pyspark to use py j
Behavioral decision,Currently the method join right DataFrame usingColumns Seq String only supports inner join It is more convenient to have it support other join types Support to specify join type when calling join with usingColumns
Non-existence - ban decision,They don t bring much value since we now have better unit test coverage for hash joins This will also help reduce the test time Remove HashJoinCompatibilitySuite
Structural decision,Python s since is defined under pyspark sql It would be nice to move it under pyspark to be shared by all components Move since annotator to pyspark to be shared by all components
Behavioral decision,In ML pipelines each transformer estimator appends new columns to the input DataFrame For example it might produce DataFrames like the following columns a b c d where a is from raw input b udf b a c udf c b and d udf d c Some UDFs could be expensive However if we materialize c and d udf b and udf c are triggered twice i e value c is not re used It would be nice to detect this pattern and re use intermediate values Optimize sequential projections
Behavioral decision,In codegen we didn t consider nullability of expressions Once considering this we can avoid lots of null check reduce the size of generated code also improve performance Before that we should double check the correctness of nullablity of all expressions and schema or we will hit NPE or wrong results Consider nullability of expression in codegen
Behavioral decision,It is convenient to implement data source API for LIBSVM format to have a better integration with DataFrames and ML pipeline API This JIRA covers the following Read LIBSVM data as a DataFrame with two columns label Double and features Vector Accept numFeatures as an option The implementation should live under org apache spark ml source libsvm Implement SQL data source API for reading LIBSVM data
Behavioral decision,Add a column function on a DataFrame like ifelse in R to SparkR I guess we could implement it with a combination with when and otherwise h Example If df x is TRUE then return otherwise return Add ifelse Column function to SparkR
Structural decision,Right now we have QualifiedTableName TableIdentifier and Seq String to represent table identifiers We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name database name return unquoted string and return quoted string There will be TODOs having SPARK in it Those places need to be updated Consolidate different forms of table identifiers
Behavioral decision,ML Evaluator currently requires that metrics be maximized bigger is better That is counterintuitive for some metrics Currently we hackily negate some metrics in RegressionEvaluator which is weird Instead we should Return the metric as expected e g rmse should return RMSE not its negation Provide an indicator of whether the metric should be maximized or minimized Model selection algorithms can use the indicator as needed ML Evaluator should indicate if metric should be maximized or minimized
Structural decision,make MultilayerPerceptronClassifier layers and weights public make MultilayerPerceptronClassifier layers and weights public
Behavioral decision,currently in SparkR collect on a DataFrame collects the data within the DataFrame into a local data frame R users are used to using data frame However collect currently can t collect data of nested types from a DataFrame because The serializer in JVM backend does not support nested types collect in R side assumes each column is of simple atomic type that can be combinded into a atomic vector Improve the implementation of collect on DataFrame in SparkR
Behavioral decision,The stat functions are defined in http spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions Currently only crosstab is supported Functions to be supported include corr cov freqItems Add support for DataFrameStatFunctions in SparkR
Behavioral decision,Add Python API for mllib fpm PrefixSpan Add Python API for PrefixSpan
Behavioral decision,This JIRA is to define commands for Hadoop token The scope of this task is highlighted as following Token init authenticate and request an identity token then persist the token in token cache for later reuse Token display show the existing token with its info and attributes in the token cache Token revoke revoke a token so that the token will no longer be valid and cannot be used later Token renew extend the lifecycle of a token before it s expired Hadoop Token Command
Behavioral decision,Once JarFinder getJar is invoked by a client app it would be really useful to destroy the generated JAR after the JVM is destroyed by setting tempJar deleteOnExit In order to preserve backwards compatibility a configuration setting could be implemented e g test build dir purge on exit JarFinder getJar should delete the jar file upon destruction of the JVM
Behavioral decision,Currently different IPC servers in Hadoop use the same config variables names starting with ipc server This makes it difficult and confusing to maintain configuration for different IPC servers Support per server IPC configuration
Behavioral decision,The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided I have defined multiple local disks defined for a datanode dfs data dir data dfs dn data dfs dn data dfs dn data dfs dn data dfs dn data dfs dn true When one of those disks breaks and is unmounted then the mountpoint such as data in this example becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting When this situation happens the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed The only way around this is to alter the configuration and omit that specific disk configuration To my opinion It would be more practical to let Hadoop daemons start when at least disks partition in the provided list is in a usable state This prevents having to roll out custom configurations for systems which have temporarily a disk and therefor directory layout missing This might also be configurable that at least X partitions out of he available ones are in OK state Allow daemon startup when at least or configurable disk is in an OK state
Behavioral decision,This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous e g reject file c Windows Valid file URI syntax explained at http blogs msdn com b ie archive file uris in windows aspx Also see https issues apache org jira browse HADOOP Reject invalid Windows URIs
Behavioral decision,Quantcast has released QFS http quantcast github com qfs a C distributed filesystem based on Kosmos File System KFS QFS comes with various feature performance and stability improvements over KFS A hadoop fs shim needs be added to support QFS through qfs URIs Need to add fs shim to use QFS
Behavioral decision,Currently FileUtil unTar spawns tar utility to do the work Tar may not be present on all platforms by default eg Windows So changing this to use JAVA API s would help make it more cross platform FileUtil unZip uses the same approach Change untar to use Java API on Windows instead of spawning tar process
Behavioral decision,Because of reasons listed here http findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE comparators should be serializable To make deserialization work it is required that all superclasses have no arg constructor http findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR Simply add no arg constructor to WritableComparator WritableComparator must implement no arg constructor
Behavioral decision,As of now hadoop streaming uses old Hadoop M R API This JIRA ports it to the new M R API Port StreamInputFormat to new Map Reduce API
Behavioral decision,This is to track changes for restoring security in branch Restore security in Hadoop branch
Behavioral decision,HADOOP introduces a configure flag to prevent potential status inconsistency between zkfc and namenode by making auto and manual failover mutually exclusive However as described in section of design doc at HDFS we should allow manual and auto failover co exist by adding some rpc interfaces at zkfc manual failover shall be triggered by haadmin and handled by zkfc if auto failover is enabled Auto HA Allow manual failover to be invoked from zkfc
Behavioral decision,To keep the initial patches manageable kerberos security is not currently supported in the ZKFC implementation This JIRA is to support the following important pieces for security integrate with ZK authentication kerberos or password based allow the user to configure ACLs for the relevant znodes add keytab configuration and login to the ZKFC daemons ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons Security support for ZK Failover controller
Behavioral decision,Currently when the ZK session expires it results in a fatal error being sent to the application callback This is not the best behavior for example in the case of HA if ZK goes down we would like the current state to be maintained rather than causing either NN to abort When the ZK clients are able to reconnect they should sort out the correct leader based on the normal locking schemes Improve ActiveStandbyElector s behavior when session expires
Behavioral decision,If there are known failures test patch will bail out as soon as it sees them This causes the precommit builds to potentially not find real issues with a patch because the tests that would fail might come after a known failure We should add fn to just the mvn test command in test patch to get the full list of failures test patch should run tests with fn to avoid masking test failures
Structural decision,One thing that the original patch for HADOOP didn t address is the need for those curated jars to be visible in the final tarball make hadoop client set of curated jars available in a distribution tarball
Non-existence - ban decision,The Syncable sync was deprecated in We should remove it Remove the deprecated Syncable sync method
Behavioral decision, Access Control support for Non secure deployment of Hadoop on Windows
Behavioral decision,We should be able to start a kdc server for unit tests so that security could be turned on This will greatly improve the coverage of unit tests Add capability to turn on security in unit tests
Structural decision,Given that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves unless a tool that manages for you is used it would be good to also validate the provided config XML site xml files with a tool like xmllint or maybe Xerces somehow when running a command or at least when starting up daemons We should use this only if a relevant tool is available and optionally be silent if the env requests Validate XMLs if a relevant tool is available when using scripts
Behavioral decision,HDFS added a new public API SequenceFile syncFs we need to forward port this for compatibility Looks like it might have introduced other APIs that need forward porting as well eg LocaltedBlocks setFileLength and DataNode getBlockInfo Forward port SequenceFile syncFs and friends from Hadoop x
Behavioral decision,Currently the check done in the hasSufficientTimeElapsed method is hardcoded to mins wait The wait time should be driven by configuration and its default value for clients should be min Kerberos relogin interval in UserGroupInformation should be configurable
Behavioral decision, RPC Layer improvements to support protocol compatibility
Structural decision, create a script to setup application in order to create root directories for application such hbase hcat hive etc
Structural decision,See this thread http markmail org thread cxtz i lvztfgfxn We need to get things up and running for a top level hadoop tools module DistCpV will be the first resident of this new home Things we need The module itself and a top level pom with appropriate dependencies Integration with the patch builds for the new module Integration with the post commit and nightly builds for the new module Set things up for a top level hadoop tools module
Behavioral decision,hadoop common src main bin hadoop config sh needs to be updated post mavenization eg it still refers to build classes etc hadoop config sh needs to be updated post mavenization
Behavioral decision,hadoop common src main bin hadoop config sh needs to be updated post MR eg the layout of mapred home has changed hadoop config sh needs to be updated post MR
Behavioral decision,In a nutshell ls needs the ability to list a directory but not its contents W o d it is impossible to list the root directory s owner permissions etc See the original hdfs bug for details hadoop dfs ls Do not expand directories was HDFS
Behavioral decision,In HADOOP and HDFS it was agreed that FileSystem listStatus should throw FileNotFoundException instead of returning null when the target directory did not exist However in LocalFileSystem implementation today FileSystem listStatus still may return null when the target directory exists but does not grant read permission This causes NPE in many callers for all the reasons cited in HADOOP and HDFS See HADOOP and its linked issues for examples FileSystem listStatus should throw IOE upon access error
Behavioral decision, IPC Wire Compatibility
Behavioral decision,When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump the client currently just gets a non useful error message like EOFException Instead the IPC server code can speak just enough of prior IPC protocols to send back a fatal message indicating the version mismatch Send back nicer error to clients using outdated IPC version
Behavioral decision,When you have a key value class that s non Writable and you forget to attach io serializers for the same an NPE is thrown by the tasks with no information on why or what s missing and what led to it I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones When a serializer class is missing return null not throw an NPE
Behavioral decision,When setting up a compression codec in an MR job the full class name of the codec must be used To ease usability compression codecs should be resolved by their codec name ie gzip deflate zlib bzip instead their full codec class name Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name it could simplify how HBase resolves loads the codecs Add capability to resolve compression codec based on codec name
Structural decision,We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson The script would execute the following and take just the password as an argument Create a test patch script for Hudson
Behavioral decision, Deprecate metrics v
Structural decision,The FsShell has many chains if then else chains for instantiating and running commands A dynamic mechanism is needed for registering commands such that FsShell requires no changes when adding new commands Add command factory to FsShell
Behavioral decision,Our project Pig exposes FsShell functionality to our end users through a shell command We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign for instance removing a non existent directory We have asks related to this issue Meaningful error code returned from FsShell we use java class so that we can take different actions on different errors Unix like ways to tell the command to ignore certain behavior Here are the commands that we would like to be expanded implemented rm f rmdir ignore fail on non empty mkdir p Extensions to FsShell
Structural decision,DaemonFactory class is defined in hdfs util common would be a better place for this class DaemonFactory should be moved from HDFS to common
Behavioral decision,New file system API HADOOP should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations MAPREDUCE security implementation for new FileSystem FileContext API
Structural decision,We should allow users to use the more compact form of xml elements For example we could allow The old format would also be supported Allow compact property description in xml
Structural decision,While working HADOOP I noticed that our metrics naming style is all over the place Capitalized camel case e g FilesCreated in namenode metrics and some rpc metrics uncapitalized camel case e g threadsBlocked in jvm metrics and some rpc metrics lowercased underscored e g bytes written in datanode metrics and mapreduce metrics Let s make them consistent How about uncapitalized camel case My main reason for the camel case some backends have limits on the name length and underscore is wasteful Once we have a consistent naming style we can do Metric Number of INodes created MutableCounterLong filesCreated instead of the more redundant Metric FilesCreated Number of INodes created MutableCounterLong filesCreated Make metrics naming consistent
Structural decision,As Hadoop widespreads and matures the number of tools and utilities for users keeps growing Some of them are bundled with Hadoop core some with Hadoop contrib some on their own some are full fledged servers on their own For example just to name a few distcp streaming pipes har pig hive oozie Today there is no standard mechanism for making these tools available to users Neither there is a standard mechanism for these tools to integrate and distributed them with each other The lack of a common foundation creates issues for developers and users Common foundation for Hadoop client tools
Behavioral decision,Whenever a new patch is submitted for verification test patch process has to make sure that none of Herriot bindings were broken test patch needs to verify Herriot integrity
Behavioral decision,Per discussions with Arun Chris Hong and Rajiv et al we concluded that the current metrics framework needs an overhaul to Allow multiple plugins for different monitoring systems simultaneously see also HADOOP Refresh metrics plugin config without server restart Including filtering of metrics per plugin Support metrics schema for plugins The jira will be resolved when core hadoop components hdfs mapreduce are updated to use the new framework Updates to external components that use the existing metrics framework will be tracked by different issues The current design wiki http wiki apache org hadoop HADOOP MetricsV Overhaul metrics framework
Structural decision,Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on The versions of these libraries are also present in ivy libraries properties so that when a library is updated it must be updated in two places which is error prone We should instead only specify library versions in a single place versions of dependencies should be specified in a single place
Behavioral decision,Now that we are adding the serialized form of delegation tokens into the http interfaces we should include some version information Should add version to the serialization of DelegationToken
Behavioral decision,When token is used for authentication over RPC information other than username may be needed for access authorization This information is typically specified in TokenIdentifier This is especially true for block tokens used for client to datanode accesses where authorization is based on access permissions specified in TokenIdentifier and not on username Block tokens used to be called access tokens and one can think of them as capability tokens See HADOOP for more info Add authenticated TokenIdentifiers to UGI so that they can be used for authorization
Behavioral decision,The UserGroupInformation should contain authentication method in its subject This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients UGI should contain authentication method
Behavioral decision,We need a configurable mapping from full user names eg omalley APACHE ORG to local user names eg omalley For many organizations it is sufficient to just use the prefix however in the case of shared clusters there may be duplicated prefixes A configurable mapping will let administrators resolve the issue Need mapping from long principal names to local OS user names
Behavioral decision,The way metrics are currently exposed to the JMX in the NameNode is not helpful since only the current counters in the record can be fetched and without any context those number mean little For example the number of files created equal to only means that in the last period there were files created but when the new period will end is unknown so fetching again will either mean another files or we are fetching the same time period One of the solutions for this problem will be to have a JMX context that will accumulate the data being child class of AbstractMetricsContext and expose different records to the JMX through custom MBeans This way the information fetched from the JMX will represent the state of things in a more meaningful way JMX Context for Metrics
Structural decision,Common tests are functional tests or end to end It makes sense to have Mockito framework for the convenience of true unit tests development Add unit tests framework Mockito
Behavioral decision,FileSystem should have mkdir and create file apis which do not create parent path The usecase is illustrated at this https issues apache org jira browse HDFS focusedCommentId page com atlassian jira plugin system issuetabpanels Acomment tabpanel action FileSystem should have mkdir and create file apis which do not create parent path
Behavioral decision,Configuration objects send a DEBUG level log message every time they re instantiated which include a full stack trace This is more appropriate for TRACE level logging as it renders other debug logs very hard to read Configuration sends too much data to log j
Structural decision,For HDFS we need to use unix domain sockets This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android apache license Add support for unix domain sockets to JNI libs
Behavioral decision,There s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson test patch script has to verify if the version of FindBugs is correct test patch should verify that FindBugs version used for verification is correct one
Behavioral decision,Currently with quota turned on user cannot call rmr on large directory that causes over quota Besides from error message being unfriendly how should this be handled Handling of Trash with quota
Structural decision,We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances In particular we often end up with calls to rpcs being wrapped with retry loops for timeouts We should be able to make a retrying proxy that will call the rpc and retry in some circumstances we need some rpc retry framework
Behavioral decision,Adding an option to FsShell stat to get a file s block location information will be very useful we can print the block location information in this format blockID XXXXX byte range YYYY ZZZZ location dn dn blockID XXXXX byte range YYYY ZZZZ location dn dn Add a command to FsShell stat to get a file s block location information
Structural decision,The following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP Some of them are not related to hdfs e g TestFTPFileSystem These files should be moved out from hdfs and should not use hdfs codes Some of them are testing hdfs features e g TestStickyBit They should be defined under org apache hadoop hdfs package fs tests should not be placed in hdfs
Non-existence - ban decision,In an effort to simplify the in the capacity scheduler We would reintroduce this possibly with some revisions to the original design after a while This will be an incompatible change Any objections Remove pre emption from the capacity scheduler code base
Behavioral decision,A specific sub case of the general priority inversion problem noted in HADOOP is when many lower priority jobs are submitted and are waiting for mappers to free up Even though they haven t actually done any work they will be assigned any free reducers If a higher priority job is submitted priority inversion results not just due to the lower priority tasks that are in the midst of completing but also due to the ones that haven t yet started but have claimed all the free reducers A simple workaround is to require a job to complete some useful work before assigning it a reducer This can be done in a tunable and backwards compatible manner by adding a minimum map progress percentage before assigning a reducer option to the JobConf Setting this to would eliminate the common case above and setting it to would technically eliminate the inversion of HADOOP though likely at an unacceptably high cost JobConf option for minimum progress threshold before reducers are assigned
Structural decision,to increase productivity in our current project which makes a heavy use of Hadoop we wrote a small Eclipse based GUI application which basically consists in views a HDFS explorer adapted from Eclipse filesystem explorer example For now it includes the following features o classical tree based browsing interface with directory content being detailed in a columns table file name file size file type o refresh button o delete file or directory with confirm dialog select files in the tree or table and click the Delete button o rename file or directory simple click on the file in the table type the new name and validate o open file with system editor select the file in the table and click Open button works on Windows not on Linux o internal drag drop o external drag drop from the local filesystem to the HDFS the opposite doesn t work a MapReduce very simple job launcher o select the job XML configuration file o run the job o kill the job o visualize map and reduce progress with progress bars o open a browser on the Hadoop job tracker web interface INSTALLATION NOTES Eclipse JDK import the archive in Eclipse copy your hadoop conf file hadoop default xml in src folder this step should be moved in the GUI later right click on the project and Run As Eclipse Application enjoy Eclipse based GUI DFS explorer and basic Map Reduce job launcher
Structural decision,already has quotas for HDFS namespace HADOOP HADOOP implements similar quotas for disk space on HDFS in This jira proposes to port HADOOP to Port HDFS space quotas to
Behavioral decision,Watchdog is watching for ChukwaAgent only once every minutes so there s no point in retrying more than once every mins In practice if the watchdog is not able to automatically restart the agent it will take more than minutes to get Ops to restart it Also Ops want us to limit the number of communications between Hadoop and Chukwa that s why minutes ChukwaAgent controller should retry to register for a longer period but not as frequent as now
Behavioral decision, to optimize hudsonBuildHadoopNightly sh script
Structural decision, Split build script for building core hdfs and mapred separately
Behavioral decision,When data need to be reprocessed in the database there is currently no manual method to reload the chukwa sequence files into database A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this Add utilities to load chukwa sequence file to database
Behavioral decision,It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime This would allow us to stop relying on exec ing bash to get access to information such as user groups process limits etc and for features such as chown chgrp org apache hadoop util Shell Implement a native OS runtime for Hadoop
Structural decision,Amongst other things JUnit has better support for class wide set up and tear down via BeforeClass and AfterClass annotations and more flexible assertions http junit sourceforge net doc ReleaseNotes html It would be nice to be able to take advantage of these features in tests we write JUnit can run tests written for JUnit without any changes Upgrade to JUnit
Behavioral decision,In the FileTailingAdaptor if when trying to read a file a File does not exist Permission denied exception is throws then that file should be log and removed When the FileTailingAdaptor is unable to read a file it should take action instead of trying times
Structural decision, Add Hadoop native library to java library path compression
Structural decision,Ability to test endToEnd chukwa pipeline From log file to dataSink From dataSink to demux output Chukwa test framework
Behavioral decision,An external application an Ops script or some CLI based tool can change the configuration of the Capacity Scheduler change the capacities of various queues for example by updating its config file This application then needs to tell the Capacity Scheduler that its config has changed which causes the Scheduler to re read its configuration It s possible that the Capacity Scheduler may need to interact with external applications in other similar ways Capacity Scheduler needs to re read its configuration
Structural decision,Create some utility classes to dump both Archive and ChukwaRecords files Utility classes for Archive and ChukwaRecord files
Behavioral decision,In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output avoid immediate merging if there s already file for the same time range create a spill file instead merge all raw files every hours merge all hourly files every days Rolling mechanism for demux output
Structural decision,Update Chukwa parsers Update Chukwa parsers
Behavioral decision,simplify Parsers implementation add map and reduce side to the demux add dynamic link between RecordType and Parsers using configuration file and alias encapsulate data files creation location and naming convention inside the core demux classes sort all data by TimePartition Machine Timestamp by default Update Chukwa Demux process
Behavioral decision,Problem We need to have Object Serialization Deserialization support for TFile ObjectFile on top of TFile
Behavioral decision,Besides the default JT scheduling algorithm there is work going on with at least two more schedulers HADOOP HADOOP HADOOP makes it easier to plug in new schedulers into the JT Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment and easy for developers to add in more schedulers into the framework without inundating it Hadoop Core should support source filesfor multiple schedulers
Behavioral decision,SequenceFile s block compression format is too complex and requires codecs to compress or decompress It would be good to have a file format that only needs New binary file format
Behavioral decision,Should we change the hash function for Text to something that handles non ascii characters better http bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup Replace Text hashcode with a better hash function for non ascii strings
Structural decision,Utility will collapse the contents of a directory into a small number of files compaction utility for directories
Behavioral decision,Currently only way to find files with all replica being corrupt is when we read those files Instead can we have fsck report those Using the corrupted blocks found by the periodic verification fsck to show checksum corrupted files
Behavioral decision,Checkpoint to verify the fsimage each time it creates the new one Keep two generations of fsimage
Behavioral decision,this is something that we have implemented in the application layer may be useful to have in hadoop itself long term log storage systems often keep data sorted by some sort key future computations on such files can often benefit from this sort order if the job requires grouping by the sort key then it should be possible to do reduction in the map stage itself this is not natively supported by hadoop except in the degenerate case of map file per task since splits can span the sort key however aligning the data read by the map task to sort key boundaries is straightforward and this would be a useful capability to have in hadoop the definition of the sort key should be left up to the application it s not necessarily the key field in a Sequencefile through a generic interface but otherwise the sequencefile and text file readers can use the extracted sort key to align map task data with key boundaries align map splits on sorted files with key boundaries
Structural decision,This jira is intended to enhance IPC s scalability and robustness Currently an IPC server can easily hung due to a disk failure or garbage collection during which it cannot respond to the clients promptly This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously the server may be swarmed by requests and cannot work responsively The proposed changes aim to provide a better client server coordination Server should be able to throttle client during burst of requests A slow client should not affect server from serving other clients A temporary hanging server should not cause catastrophic failures to clients Client server should detect remote side failures Examples of failures include the remote host is crashed the remote host is crashed and then rebooted the remote process is crashed or shut down by an operator Fairness Each client should be able to make progress Improve the Scalability and Robustness of IPC
Structural decision,Move hbase out of hadoop core Move its JIRA issues and move it in svn from https svn apache org repos asf hadoop core trunk src contrib hbase to https svn apache org repos asf hadoop hbase trunk Move hbase out of hadoop core
Behavioral decision,Currently when a block is transfered to a data node the client interleaves data chunks with the respective checksums This requires creating an extra copy of the original data in a new buffer interleaved with the crcs We can avoid extra copying if the data and the crc are fed to the socket one after another Non interleaved checksums would optimize block transfers
Behavioral decision,A user was moving from to and was invoking randomwriter with a config on the command line like bin hadoop jar hadoop examples jar randomwriter output conf xml which worked in but in it ignores the conf xml without complaining The equivalent is bin hadoop jar hadoop examples jar randomwriter conf conf xml output randomwriter should complain if there are too many arguments
Behavioral decision,The metrics system in the JobTracker is defaulting to every seconds computing all of the counters for all of the jobs This work is a substantial amount of work showing up as running in of the snapshots that I ve seen I d like to lower the default interval to once every seconds and make it a low priority thread the metrics system in the job tracker is running too often
Behavioral decision,Currently max queue size for IPC server is set to handlers Usually when RPC failures are observed e g HADOOP we increase number of handlers and the problem goes away I think a big part of such a fix is increase in max queue size I think we should make maxQsize per handler configurable with a bigger default than There are other improvements also HADOOP Server keeps reading RPC requests from clients When the number in flight RPCs is larger than maxQsize the earliest RPCs are deleted This is the main feedback Server has for the client I have often heard from users that Hadoop doesn t handle bursty traffic Say handler count is default and Server can handle RPCs a sec quite conservative low for a typical server it implies that an RPC can wait for only for sec before it is dropped If there clients and all of them send RPCs around the same time not very rare with heartbeats etc will be dropped In stead of dropping the earliest RPCs if the server delays reading new RPCs the feedback to clients would be much smoother I will file another jira regd queue management For this jira I propose to make queue size per handler configurable with a larger default may be IPC server max queue size should be configurable
Behavioral decision,Hudson should kill long running tests I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up It would be nice if when the timer goes off Hudson did a See the section Killing a hung test at http wiki apache org lucene hadoop HudsonBuildServer Hudson should kill long running tests
Behavioral decision,Unlike gzip the bzip file format supports splitting Compression is by blocks k by default and blocks are separated by a synchronization marker a bit approximation of Pi This would permit very large compressed files to be split into multiple map tasks which is not currently possible unless using a Hadoop specific file format want InputFormat for bzip files
Behavioral decision,It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings It could then be hooked up to bin hadoop fs cat and the web ui to textify sequence and compressed files Create a utility to convert binary sequence and compressed files to strings
Structural decision,is missing in pom xml That way mvn versions set does not work for the project Missing hadoop cloud storage project module in pom xml
Behavioral decision,Doing some Tomcat performance tuning on a loaded cluster we found that acceptCount acceptorThreadCount and protocol can be useful Let s make these configurable in the kms startup script Since the KMS is Jetty in x this is targeted at just branch Make additional KMS tomcat settings configurable
Structural decision,Other people aren t seeing this yet but unless you explicitly exclude v of commons lang from the azure build which HADOOP does then the dependency declaration of commons lang v is creating a resolution conflict That s a dependency only needed for the local dynamodb tests I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one excluding that you get for free It doesn t impact anything shipped in production but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common explicitly declare the commons lang dependency as
Non-existence - ban decision,the share hadoop component template directories are from when RPM and such were built as part of the build system that no longer happens and now those files cause more harm than good since they are in the classpath let s remove them Remove vestigal templates directories creation
Structural decision,ADLS has multiple upgrades since the version we are using and Change list https github com Azure azure data lake store java blob master CHANGES md Update ADLS SDK to
Behavioral decision,Read ADLS credentials using Hadoop CredentialProvider API See https hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html Read ADLS credentials from Credential Provider
Behavioral decision,The FTP transfer mode used by FTPFileSystem is BLOCK TRANSFER MODE FTP Data connection mode used by FTPFileSystem is ACTIVE LOCAL DATA CONNECTION MODE This jira makes them configurable Make FTPFileSystem s data connection mode and transfer mode configurable
Behavioral decision,Currently we have one command to get state of namenode It will be good to have command which will give state of all the namenodes Add haadmin getAllServiceState option to get the HA state of all the NameNodes ResourceManagers
Non-existence - ban decision,Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency This JIRA removes the SDK snapshot dependency to released SDK candidate There is not functional change in the SDK and no impact to live contract test Remove snapshot version of SDK dependency from Azure Data Lake Store File System
Behavioral decision,ViewFileSystem doesn t override FileSystem getLinkTarget So when view filesystem is used to resolve the symbolic links the default FileSystem implementation throws UnsupportedOperationException The proposal is to define getLinkTarget for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links Path thus returned is preferred to be a viewfs qualified path so that it can be used again on the ViewFileSystem handle Implement getLinkTarget for ViewFileSystem
Behavioral decision,Current implementation of WASB only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers Added to the fact that WASB does not inherently support ACL s WASB is its current implementation cannot be securely used for environments like secure hadoop cluster This JIRA is created to add a new mode in WASB which operates on Azure Storage SAS keys which can provide fine grained timed access to containers and blobs providing a segway into supporting WASB for secure hadoop cluster More details about the issue and the proposal are provided in the design proposal document Azure Add a new SAS key mode for WASB
Behavioral decision,This is the KMS part Please refer to HDFS for the design doc Add reencryptEncryptedKey interface to KMS
Structural decision,KMS and HttpFS currently uses Tomcat propose to upgrade to the latest version is Upgrade Tomcat to
Behavioral decision,Currently SequenceFiles put in sync blocks every bytes It would be much better if it was configurable with a much higher default mb or so The distance between sync blocks in SequenceFiles should be configurable
Non-existence - ban decision,Shell java has a hardcoded path to bin ls which is not correct on all platforms eg not on NixOS see HADOOP for a similar issue Remove hardcoded absolute path for ls
Behavioral decision,Currently the MutableRates metrics class serializes all writes to metrics it contains because of its use of MetricsRegistry add i e even two increments of unrelated metrics contained within the same MutableRates object will serialize w r t each other This class is used by RpcDetailedMetrics which may have many hundreds of threads contending to modify these metrics Instead we should allow updates to unrelated metrics objects to happen concurrently To do so we can let each thread locally collect metrics and on a snapshot aggregate the metrics from all of the threads I have collected some benchmark performance numbers in HADOOP https issues apache org jira secure attachment benchmark results which indicate that this can bring significantly higher performance in high contention situations Make MutableRates metrics thread local write aggregate on read
Behavioral decision,To track user level connections How many connections for each user in busy cluster where so many connections to server Expose NumOpenConnectionsPerUser as a metric
Behavioral decision,DiskChecker can fail to detect total disk controller failures indefinitely We have seen this in real clusters DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted A simple improvement is to write some data and flush it to the disk DiskChecker should perform some disk IO
Non-existence - ban decision,The DiskChecker class has a few unused public methods We can remove them Cleanup DiskChecker interface
Structural decision,To make our tests robust against timing problems and eventual consistent stores we need to do more spin wait for state We have some we ve examples to follow Some of that work has been reimplemented slightly in S ATestUtils eventually I propose adding a class in the test tree Eventually to be a successor replacement for these has an eventually waitfor operation taking a predicate that throws an exception has an evaluate exception which tries to evaluate an answer until the operation stops raising an exception again from scalatest plugin backoff strategies from Scalatest lets you do exponential as well as linear option of adding a special handler to generate the failure exception e g run more detailed diagnostics for the exception text etc be Java lambda expression friendly be testable and tested itself Add LambdaTestUtils class for tests fix eventual consistency problem in contract test setup
Behavioral decision,The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned We need to allow for the subprocess to be interrupted and killed when the shell process gets killed Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed Ability to clean up subprocesses spawned by Shell when the process exits
Non-existence - ban decision,Per discussion on HADOOP I d like to revert HADOOP It removes a deprecated API but the x line does not have a release with the new replacement API This places a burden on downstream applications Revert HADOOP Remove unused TrashPolicy getInstance and initialize code
Behavioral decision,Add a new instrumented read write lock in hadoop common so that the HDFS can use this to improve the locking in FsDatasetImpl Add a new instrumented read write lock
Non-existence - ban decision,We generate source code with line numbers for inclusion in the javadoc JARs Given that there s github and other online viewers this doesn t seem so useful these days Disabling the linkSource option saves us MB for the hadoop common javadoc jar Stop bundling HTML source code in javadoc JARs
Structural decision,Currently downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S A need to list dependencies on each one This creates an ongoing maintenance burden for those projects because they need to update their build whenever a new Hadoop compatible file system is introduced This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems Similar to hadoop client this new artifact will consist of just a pom xml listing the individual dependencies Downstream users can depend on this artifact to sweep in everything and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop
Structural decision,The newly added Kafka module defines the Kafka dependency as Reduce Kafka dependencies in hadoop kafka module
Behavioral decision,As work continues on HADOOP it s become evident that we need better hooks to start daemons as specifically configured users Via the command subcommand USER environment variables in x we actually have a standardized way to do that This in turn means we can make the sbin scripts super functional with a bit of updating Consolidate start dfs sh and start secure dns sh into one script Make start sh and stop sh know how to switch users when run as root Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users Update scripts to be smarter when running with privilege
Structural decision,We re currently pulling in version incubating I think we should upgrade to the latest incubating Upgrade HTrace version
Structural decision,We re currently pulling in version I think we should upgrade to the latest Upgrade commons configuration version to
Structural decision,add a metadata file giving the FS impl of swift remove the entry from core default xml swift FS to add a service load metadata file
Behavioral decision,The UGI has a background thread to renew the tgt On exception it terminates itself https github com apache hadoop blob bee f f ca f ade c fd b dad a hadoop common project hadoop common src main java org apache hadoop security UserGroupInformation java L L If something temporarily goes wrong that results in an IOE even if it recovered no renewal will be done and client will eventually fail to authenticate We should retry with our best effort until tgt expires in the hope that the error recovers before that Retry until TGT expires even if the UGI renewal thread encountered exception
Structural decision,Based on discussion at YETUS this code can t go there but it s still very useful for release managers A similar variant of this script has been used for a while by Apache HBase and Apache Kudu and IMO JACC output is easier to understand than JDiff Incorporate checkcompatibility script which runs Java API Compliance Checker
Structural decision,ZStandard https github com facebook zstd has been used in production for months by facebook now v was recently released Create a codec for this library Add Codec for ZStandard Compression
Behavioral decision,Leveraging HADOOP will allow non rpc calls to be added to the call queue This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS Support external calls in the RPC call queue
Behavioral decision,This patch adds to fs shell Stat java the missing options of a and A FileStatus already contains the getPermission method required for returning symbolic permissions FsPermission contains the method to return the binary short but nothing to present in standard Octal format Most UNIX admins base their work on such standard octal permissions Hence this patch also introduces one tiny method to translate the toShort return into octal Build has already passed unit tests and javadoc Add A and a formats for fs stat command to print permissions
Structural decision,Introduce an AutoCloseableLock class that is a thin wrapper over RentrantLock It allows using RentrantLock with try with resources syntax The wrapper functions perform no expensive operations in the lock acquire release path Add an AutoCloseableLock class
Behavioral decision,The RPC layer supports QoS but other protocols ex webhdfs are completely unconstrained Generalizing Server Call to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols Design Server Call to be extensible for unified call queue
Non-existence - ban decision,Shell java has a hardcoded path to bin bash which is not correct on all platforms Pointed out by aw while reviewing HADOOP Remove hardcoded absolute path for shell executable
Non-existence - ban decision,We re cleaning up Hive and Spark s use of FileSystem exists because it is often the case we see code of exists open exists delete when the exists probe is needless Against object stores expensive needless Hadoop can set an example here by stripping them out It will also show where there are opportunities to optimise things better and or improve reporting Eliminate needless uses of FileSystem exists isFile isDirectory
Behavioral decision,Umbrella jira for y optimizations to reduce object allocations more efficiently use protobuf APIs unified ipc and webhdfs callq to enable QoS etc IPC layer optimizations
Behavioral decision,Currently KMS audit log is using log j to write a text format log We should refactor this so that people can easily add new format audit logs The current text format log should be the default and all of its behavior should remain compatible Allow pluggable audit loggers in KMS
Non-existence - ban decision,In branch and later the patches for various child and related bugs listed in HADOOP most recently including HADOOP HADOOP HADOOP HADOOP and HDFS eliminate all use of commons httpclient from Hadoop and its sub projects except for hadoop tools hadoop openstack see HADOOP However after incorporating these patches commons httpclient is still listed as a dependency in these POM files hadoop project pom xml hadoop yarn project hadoop yarn hadoop yarn registry pom xml We wish to remove these but since commons httpclient is still used in many files in hadoop tools hadoop openstack we ll need to add the dependency to hadoop tools hadoop openstack pom xml We ll add a note to HADOOP to undo this when commons httpclient is removed from hadoop openstack In this was mostly done by HADOOP but the version info formerly inherited from hadoop project pom xml also needs to be added so that is in the branch version of the patch Other projects with undeclared transitive dependencies on commons httpclient previously provided via hadoop common or hadoop client may find this to be an incompatible change Of course that also means such project is exposed to the commons httpclient CVE and needs to be fixed for that reason as well remove unneeded commons httpclient dependencies from POM files in Hadoop and sub projects
Structural decision,Update WASB driver to use the latest version of SDK for Microsoft Azure Storage Clients We are currently using version of the SDK Version brings some breaking changes Need to fix code to resolve all these breaking changes and certify that everything works properly Update WASB driver to use the latest version of SDK for Microsoft Azure Storage Clients
Non-existence - ban decision,Right now the git repo has a branch named master in addition to our trunk branch Since master is the common place name of the most recent branch in git repositories this is misleading to new folks It looks like the branch is from months ago We should remove it delete spurious master branch
Behavioral decision,Big features like YARN demonstrate that even senior level Hadoop developers forget that daemons need a custom OPTS env var We can replace all of the custom vars with generic handling just like we do for the username check For example with generic handling in place Old Var New Var HADOOP NAMENODE OPTS HDFS NAMENODE OPTS YARN RESOURCEMANAGER OPTS YARN RESOURCEMANAGER OPTS n a YARN TIMELINEREADER OPTS n a HADOOP DISTCP OPTS n a MAPRED DISTCP OPTS HADOOP DN SECURE EXTRA OPTS HDFS DATANODE SECURE EXTRA OPTS HADOOP NFS SECURE EXTRA OPTS HDFS NFS SECURE EXTRA OPTS HADOOP JOB HISTORYSERVER OPTS MAPRED HISTORYSERVER OPTS This makes it a consistent across the entire project b consistent for every subcommand c eliminates almost all of the custom appending in the case statements It s worth pointing out that subcommands like distcp that sometimes need a higher than normal client side heapsize or custom options are a huge win Combined with hadooprc and or dynamic subcommands it means users can easily do customizations based upon their needs without a lot of weirdo shell aliasing or one line shell scripts off to the side Deprecate HADOOP SERVERNAME OPTS replace with command subcommand OPTS
Structural decision, Update maven enforcer plugin version to
Behavioral decision,Current ViewFileSystem does not support storage policy related API it will throw UnsupportedOperationException ViewFileSystem should support storage policy related API
Behavioral decision,In HADOOP the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background avoiding many slow group lookups Even with this change I have seen quite a few clusters with issues due to slow group lookups The problem is most prevalent in HA clusters where a slow group lookup on the hdfs user can fail to return for over seconds causing the Failover Controller to kill it The way the current Guava cache implementation works is approximately On initial load the first thread to request groups for a given user blocks until it returns Any subsequent threads requesting that user block until that first thread populates the cache When the key expires the first thread to hit the cache after expiry blocks While it is blocked other threads will return the old value I feel it is this blocking thread that still gives the Namenode issues on slow group lookups If the call from the FC is the one that blocks and lookups are slow if can cause the NN to be killed Guava has the ability to refresh expired keys completely in the background where the first thread that hits an expired key schedules a background cache reload but still returns the old value Then the cache is eventually updated This patch introduces this background reload feature There are two new parameters hadoop security groups cache background reload default false to keep the current behaviour Set to true to enable a small thread pool and background refresh for expired keys hadoop security groups cache background reload threads only relevant if the above is set to true Controls how many threads are in the background refresh pool Default is which is likely to be enough Reload cached groups in background after expiry
Structural decision,This JIRA is to address Jing s comments https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment in HADOOP AsyncCallHandler should use an event driven architecture to handle async calls
Behavioral decision,In current Async DFS implementation file system calls are invoked and returns Future immediately to clients Clients call Future get to retrieve final results Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB ProtobufRpcEngine and ipc Client The callback path bypasses the original retry layer logic designed for synchronous DFS This proposes refactoring to make retry also works for Async DFS Support async call retry and failover
Structural decision,This is a follow up jira from HADOOP Now with the findbug warning As discussed in HADOOP bq Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away we will add the findbugsExcludeFile xml and will get rid of this given kerby rc release Add the kerby version hadoop project pom xml bq hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop under dependencyManagement Only here version will be mentioned All other Hadoop Modules will inherit hadoop project so all submodules will use the same version In submodule version need not be mentioned in pom xml This will make version management easier Follow on fixups after upgraded mini kdc using Kerby
Structural decision,slaves sh and the slaves file should get replace with workers sh and a workers file replace slaves with workers
Structural decision,Upgrade yetus wrapper to be now that it has passed vote Upgrade to Apache Yetus
Non-existence - ban decision,The hadoop ant code is an ancient kludge unlikely to have any users still We can delete it from trunk as a scream test for x Remove hadoop ant from hadoop tools
Behavioral decision,Currently the Future returned by ipc async call only support Future get but not Future get timeout unit We should support the latter as well Support Future get with timeout in ipc async calls
Non-existence - ban decision,We should slim down the Docker image by removing JDK now that trunk no longer supports it remove JDK from Dockerfile
Behavioral decision,After DistCp copies a file it calls getFileStatus to get the FileStatus from the destination so that it can compare to the source and update metadata if necessary If the DistCp command was run without the option to preserve metadata attributes then this additional getFileStatus call is wasteful In DistCp prevent unnecessary getFileStatus call when not preserving metadata
Structural decision,We want to rename to alpha for the first alpha release However the version number is also encoded outside of the pom xml s so we need to update these too Change project version from to alpha
Behavioral decision,LdapGroupsMapping currently does not set timeouts on the LDAP queries This can create a risk of a very long infinite wait on a connection Support timeouts in LDAP queries in LdapGroupsMapping
Behavioral decision,Currently FileSystem Statistics exposes the following statistics BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks There is logic within DfsClient to map operations to these counters that can be confusing for instance mkdirs counts as a writeOp Proposed enhancement Add a statistic for each DfsClient operation including create append createSymlink delete exists mkdirs rename and expose them as new properties on the Statistics object The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS For example we can use them to identify jobs that end up creating a large number of files Once this information is available in the Statistics object the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary Add a new interface for retrieving FS and FC Statistics
Structural decision,The jira proposes an improvement over HADOOP to remove webhdfs dependencies from the ADL file system client and build out a standalone client At a high level this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake The scheme used for accessing the file system will continue to be adl azuredatalake net path to file The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store Refactor Azure Data Lake Store as an independent FileSystem
Behavioral decision,Cross Frame Scripting XFS prevention for UIs can be provided through a common servlet filter This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs if appropriate Add XFS Filter for UIs to Hadoop Common
Behavioral decision,This allows metrics collector such as AMS to collect it with MetricsSink The per user RPC call counts schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues Support MetricsSource interface for DecayRpcScheduler Metrics
Non-existence - ban decision,As per comment https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment from wheat in HADOOP Need to remove FileUtil copyMerge CC to wheat Remove FileUtil copyMerge
Behavioral decision,In async RPC if the callers don t read replies fast enough the buffer storing replies could be used up This is to propose limiting the number of outstanding async calls to eliminate the issue Limit the number of outstanding async calls
Behavioral decision,HADOOP added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook For each of the shutdown hook registered we currently don t have an upper bound for its execution time We have seen namenode failed to shutdown completely waiting for shutdown hook to finish after failover for a long period of time which breaks the namenode high availability scenarios This ticket is opened to allow specifying a timeout value for the registered shutdown hook ShutdownHookManager should have a timeout for each of the Registered shutdown hook
Behavioral decision,Currently the dfs test command only supports d e f s z options It would be helpful if we add w r to verify permission of r w before actual read or write This will help script programming Add w r options in dfs test command
Behavioral decision,Umbrella for converting hadoop hdfs mapred and yarn to allow for dynamic subcommands See first comment for more details Umbrella Dynamic subcommands for hadoop shell scripts
Behavioral decision,Currently back off policy from HADOOP is hard coded to base on whether call queue is full This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities Allow RPC scheduler callqueue backoff using response times
Structural decision,As discussed in the mailing list we d like to introduce Apache Kerby into Hadoop Initially it s good to start with upgrading Hadoop MiniKDC with Kerby offerings Apache Kerby https github com apache directory kerby as an Apache Directory sub project is a Java Kerberos binding It provides a SimpleKDC server that borrowed ideas from MiniKDC and implemented all the facilities existing in MiniKDC Currently MiniKDC depends on the old Kerberos implementation in Directory Server project but the implementation is stopped being maintained Directory community has a plan to replace the implementation using Kerby MiniKDC can use Kerby SimpleKDC directly to avoid depending on the full of Directory project Kerby also provides nice identity backends such as the lightweight memory based one and the very simple json one for easy development and test environments Upgrade Hadoop MiniKDC with Kerby
Behavioral decision,In ipc Client the underlying mechanism is already supporting asynchronous calls the calls shares a connection the call requests are sent using a thread pool and the responses can be out of order Indeed synchronous call is implemented by invoking wait in the caller thread in order to wait for the server response In this JIRA we change ipc Client to support asynchronous mode In asynchronous mode it return once the request has been sent out but not wait for the response from the server Change ipc Client to support asynchronous calls
Structural decision,Update Yetus to Update Yetus to
Non-existence - ban decision,We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker Remove MRv terms from HttpAuthentication md
Behavioral decision,gridmix shouldn t require a raw java command line to run add a subcommand for gridmix
Non-existence - ban decision,When o a h record was moved bin rcc was never updated to pull those classes from the streaming jar Remove bin rcc script
Behavioral decision,As discussed in mailing list this will disable style checks in class setters like the following Disable hiding field style checks in class setters
Structural decision,As hadoop tools grows bigger and bigger it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows Let s rework this to be smarter Rework hadoop tools
Structural decision,Let s pull the shell code out of the hadoop dist pom xml pull shell code out of hadoop dist
Behavioral decision,Java supports TLSv and TLSv which are more secure than TLSv which was all that was supported in Java so we should add those to the default list for hadoop ssl enabled protocols Enable TLS v and
Structural decision,The HBase s HMaster port number conflicts with Hadoop kms port number Both uses There might be use cases user need kms and HBase present on the same cluster The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories Users would have to manually override the default port of either application on their cluster It would be nice to have different default ports so kms and HBase could naturally coexist Change kms server port number which conflicts with HMaster port number
Behavioral decision,The typical LDAP group name resolution works well under typical scenarios However we have seen cases where a user is mapped to many groups in an extreme case a user is mapped to more than groups The way it s being implemented now makes this case super slow resolving groups from ActiveDirectory The current LDAP group resolution implementation sends two queries to a ActiveDirectory server The first query returns a user object which contains DN distinguished name The second query looks for groups where the user DN is a member If a user is mapped to many groups the second query returns all group objects associated with the user and is thus very slow After studying a user object in ActiveDirectory I found a user object actually contains a memberOf field which is the DN of all group objects where the user belongs to Assuming that an organization has no recursive group relation that is a user A is a member of group G and group G is a member of group G we can use this properties to avoid the second query which can potentially run very slow I propose that we add a configuration to only enable this feature for users who want to reduce group resolution time and who does not have recursive groups so that existing behavior will not be broken Faster LDAP group name resolution with ActiveDirectory
Non-existence - ban decision,Remove getaclstatus call for non acl commands in getfacl Remove getaclstatus call for non acl commands in getfacl
Structural decision,Various SSL security fixes are needed See CVE CVE CVE CVE update apache httpclient version to httpcore to
Behavioral decision,The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in The issue is that HDFS does not update the file size until it s closed HDFS and if no new metrics record comes in then the file size will never be updated This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour RollingFileSystemSink should eagerly rotate directories
Behavioral decision,To protect against CSRF attacks HADOOP introduces a CSRF filter that will require a specific HTTP header to be sent with every REST API call This will affect all API consumers from web apps to CLIs and curl Since CSRF is primarily a browser based attack we can try and minimize the impact on non browser clients This enhancement will provide additional configuration for identifying non browser useragents and skipping the enforcement of the header requirement for anything identified as a non browser This will largely limit the impact to browser based PUT and POST calls when configured appropriately Extend CSRF Filter with UserAgent Checks
Structural decision,Aliyun OSS is widely used among China s cloud users but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application because of no original support for OSS in Hadoop This work aims to integrate Aliyun OSS with Hadoop By simple configuration Spark Hadoop applications can read write data from OSS without any code change Narrowing the gap between user s APP and data storage like what have been done for S in Hadoop Incorporate Aliyun OSS file system implementation
Behavioral decision,There is a problem when a user job adds too many dependency jars in their command line The HADOOP CLASSPATH part can be addressed including using wildcards But the same cannot be done with the libjars argument Today it takes only fully specified file paths We may want to consider supporting wildcards as a way to help users in this situation The idea is to handle it the same way the JVM does it expands to the list of jars in that directory It does not traverse into any child directory Also it probably would be a good idea to do it only for libjars i e don t do it for files and archives support wildcard in libjars argument
Behavioral decision,Some of the checkstyle checks are not realistic like the line length leading to spurious in precommit Let s disable Disable spurious checkstyle checks
Behavioral decision,We need a metrics sink that can write metrics to HDFS The sink should accept as configuration a directory prefix and do the following in putMetrics Get yyyyMMddHH from current timestamp If HDFS dir dir prefix yyyyMMddHH doesn t exist create it Close any currently open file and create a new file called log in the new directory Write metrics to the current log file Add an HDFS metrics sink
Behavioral decision,CSRF prevention for REST APIs can be provided through a common servlet filter This filter would check for the existence of an expected configurable HTTP header such as X XSRF Header The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin Add CSRF Filter for REST APIs to Hadoop Common
Behavioral decision,Currently if the value of ipc client rpc timeout ms is greater than the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed RPC timeout should work without effectively disabling IPC ping RPC timeout should not override IPC ping interval
Behavioral decision,Currently Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section However the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites This so it can exclude the ciphers supplied through this key Support excluding weak Ciphers in HttpServer through ssl server xml
Structural decision,h Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store ADL from within Hadoop This would enable existing Hadoop applications such has MR HIVE Hbase etc to use ADL store as input or output ADL is ultra high capacity Optimized for massive throughput with rich management and security features More details available at https azure microsoft com en us services data lake store Support Microsoft Azure Data Lake as a file system in Hadoop
Structural decision,Now that Yetus has had a release we should rip out the components that make it up from dev support and replace them with wrappers The wrappers should default to a sane version allow for version overrides via an env var download into patchprocess execute with the given parameters Marking this as an incompatible change since we should also remove the filename extensions and move these into a bin directory for better maintenance towards the future Replace dev support with wrappers to Yetus
Behavioral decision,Currently the WASB implementation of the HDFS interface does not support Append API This JIRA is added to design and implement the Append API support to WASB The intended support for Append would only support a single writer Adding Append API support for WASB
Structural decision,HADOOP added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment start build env sh This needs to be fixed Intel ISA L libraries should be added to the Dockerfile
Behavioral decision,We should add a config to disable the logs endpoint in HttpServer Listing a directory like this can be dangerous from a security perspective We can keep it enabled by default for compatibility though Add a config to disable the logs endpoints
Behavioral decision,The FileContext class currently is annotated as Evolving However at this point we really need to treat it as a Stable interface FileContext and AbstractFileSystem should be annotated as a Stable interface
Behavioral decision,The WriteableRPCEninge depends on Java s serialization mechanisms for RPC requests Without proper checks it has be shown that it can lead to security vulnerabilities such as remote code execution e g COLLECTIONS HADOOP The current implementation has migrated from WriteableRPCEngine to ProtobufRPCEngine now This jira proposes to deprecate WriteableRPCEngine in branch and to remove it in trunk Deprecate WriteableRPCEngine
Behavioral decision,hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations Additionally the token files that are created use Java serializations which are hard impossible to deal with in other languages It should be replaced with a better utility in common that can read write protobuf based token files has enough flexibility to be used with other services and offers key functionality such as append and rename The old version file format should still be supported for backward compatibility but will be effectively deprecated A follow on JIRA will deprecrate fetchdt Updated utility to create modify token files
Behavioral decision,It would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project Make hadoop dockerfile usable by Yetus
Behavioral decision,It would be good if we could read s creds from a source other than via a java property Hadoop configuration option Read s a creds from a Credential Provider
Behavioral decision,Make the re j dependency consistent with other parts of Hadoop Seeing some weird rare failures with older versions of maven that appear to be related to this make re j dependency consistent
Behavioral decision,The hadoop azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account The configuration works by overwriting the src test resources azure test xml file This can be an error prone process The azure test xml file is checked into revision control to show an example There is a risk that the tester could overwrite azure test xml containing the keys and then accidentally commit the keys to revision control This would leak the keys to the world for potential use by an attacker This issue proposes to use XInclude to isolate the keys into a separate file ignored by git which will never be committed to revision control This is very similar to the setup already used by hadoop aws for integration testing Use XInclude in hadoop azure test configuration to isolate Azure Storage account keys for service integration tests
Non-existence - ban decision,After HADOOP we should remove metrics v from trunk Remove metrics v
Behavioral decision,kms dt currently does not have its own token identifier class to de Support decoding KMS Delegation Token with its own Identifier
Behavioral decision,FileSystem createNonRecursive is deprecated However there is no DistributedFileSystem create implementation which throws exception if parent directory doesn t exist This limits clients migration away from the deprecated method For HBase IO fencing relies on the behavior of FileSystem createNonRecursive Variant of create method should be added which throws exception if parent directory doesn t exist Undeprecate createNonRecursive
Behavioral decision,Because CLI is using CommandWithDestination java which add COPYING to the tail of file name when it does the copy For blobstore like S and Swift to create COPYING file and rename it is expensive direct flag can allow user to avoiding the COPYING file Add direct flag option for fs copy so that user can choose not to create COPYING file
Behavioral decision,It would be useful for rd party apps to know the locations of things when hadoop is running without explicit path env vars set expose calculated paths
Behavioral decision,We have seen many cases with customers deleting data inadvertently with skipTrash The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though skipTrash is being used Add safely flag to rm to prompt when deleting many files
Behavioral decision,HADOOP mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival The way this was achieved was by applying an intensive exponential retry when throttling occurred As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries i e we will do a client side copy of the blob and then copy it back to destination This operation will not be subject to throttling and hence should provide a stronger mitigation However it is more expensive hence we do it only in the case we fail after all retries Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries
Behavioral decision,This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs Slow RPCs are RPCs that fall at th percentile This is useful to troubleshoot why certain services like name node freezes under heavy load RPC Metrics Add the ability track and log slow RPCs
Structural decision,The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and even after HADOOP is not thread safe both start and stop are potentially re entrant It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle which for all Yarn services is the YARN app lifecycle as implemented in Hadoop common Making the monitor a subclass of AbstractService and moving the init start stop operations in serviceInit serviceStart serviceStop methods will fix the concurrency and state model issues and make it trivial to add as a child to any YARN service which subclasses CompositeService most the NM and RM apps will be able to hook up the monitor simply by creating one in the ctor and adding it as a child Make JvmPauseMonitor an AbstractService
Behavioral decision,When using LdapGroupsMapping with Hadoop nested groups are not supported So for example if user jdoe is part of group A which is a member of group B the group mapping currently returns only group A Currently this facility is available with ShellBasedUnixGroupsMapping and SSSD or similar tools but would be good to have this feature as part of LdapGroupsMapping directly Add support for nested groups in LdapGroupsMapping
Behavioral decision,In order to enable significantly better unit testing as well as enhanced functionality large portions of config sh should be pulled into functions See first comment for more pull argument parsing into a function
Behavioral decision,The protoc maven plugin currently generates new Java classes every time which means Maven always picks up changed files in the build It would be better if the protoc plugin only generated new Java classes when the source protoc files change Support for incremental generation in the protoc plugin
Behavioral decision,NetworkToplogy uses nodes with a list of children The access to these children is slow as it s a linear search NetworkTopology is not efficient adding getting removing nodes
Structural decision,Some of the monitoring functions could be moved from YARN to Common for easier sharing Move ResourceCalculatorPlugin from YARN to Common
Structural decision,Given test patch s tendency to get forked into a variety of different projects it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base Umbrella Split test patch off into its own TLP
Behavioral decision,The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter HADOOP added a support to plug in custom authentication scheme in addition to Kerberos via AltKerberosAuthenticationHandler class But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics As per RFC http www w org Protocols rfc rfc html HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information This mechanism is initiated by server sending the Authenticate response with WWW Authenticate header which includes at least one challenge that indicates the authentication scheme s and parameters applicable to the Request URI In case server supports multiple authentication schemes it may return multiple challenges with a Authenticate response and each challenge may use a different auth scheme A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses Negotiate as the challenge as part of WWW Authenticate response header As per the following documentation Negotiate challenge scheme is only applicable to Kerberos and Windows NTLM authentication schemes SPNEGO based Kerberos and NTLM HTTP Authentication http tools ietf org html rfc Understanding HTTP Authentication https msdn microsoft com en us library ms v vs aspx On the other hand for LDAP authentication typically Basic authentication scheme is used Note TLS is mandatory with Basic authentication scheme http httpd apache org docs trunk mod mod authnz ldap html Hence for this feature the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes Kerberos via Negotiate auth challenge and LDAP via Basic auth challenge During the authentication phase it would send both the challenges and let client pick the appropriate one If client responds with an Authorization header tagged with Negotiate it will use Kerberos authentication If client responds with an Authorization header tagged with Basic it will use LDAP authentication Note some HTTP clients e g curl or Apache Http Java client need to be configured to use one scheme over the other e g curl tool supports option to use either Kerberos via negotiate flag or username password based authentication via basic and u flags Apache HttpClient library can be configured to use specific authentication scheme http hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of strength of security e g take a look at the design of Chrome browser for HTTP authentication https www chromium org developers design documents http authentication Support multiple authentication schemes via AuthenticationFilter
Structural decision,guice doesn t work with lambda statement https github com google guice issues We should upgrade it to which includes the fix JDK Update guice version to
Behavioral decision,Right now S Credentials only works with cleartext passwords in configs as a secret access key or the URI The non URI version should use credential providers with a fallback to the clear text option S Credentials should support use of CredentialProvider
Behavioral decision,Since our min version is now JDK there s hardlink support via Files This means we can deprecate the JNI implementation and discontinue usage Deprecate usage of NativeIO link
Behavioral decision,During http authentication a cookie which contains the authentication token is dropped The expiry time of the authentication token can be configured via hadoop http authentication token validity The default value is hours For clusters which require enhanced security it is desirable to have a configurable MaxInActiveInterval for the authentication token If there is no activity during MaxInActiveInterval the authentication token will be invalidated The MaxInActiveInterval will be less than hadoop http authentication token validity The default value will be minutes Enable MaxInactiveInterval for hadoop http auth token
Behavioral decision,ThreadLocalRandom should be used when available in place of ThreadLocal For JDK the difference is minimal but JDK starts including optimizations for ThreadLocalRandom Replace uses of ThreadLocal with JDK ThreadLocalRandom
Behavioral decision,AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client This JIRA is created to enable Azure Storage Client Side logging at the Job submission level User should be able to configure Client Side logging on a Per Job bases Enable Azure Storage Client Side logging
Behavioral decision,we have the issue matching regex configurable via altering the default add in a cli arg to update it on invocation test patch s issue matching regex should be configurable
Behavioral decision,deprecate DistCpV and Logalyzer which are no longer used Deprecate DistCpV and Logalyzer
Structural decision,Set minimum version of trunk to JDK JDK Set minimum version of Hadoop to JDK
Behavioral decision,As discussed with aw In AVRO a docker based solution was created to setup all the tools for doing a full build This enables much easier reproduction of any issues and getting up and running for new developers This issue is to copy port that setup into the hadoop project in preparation for the bug squash Make setting up the build environment easier
Behavioral decision,For very large source trees on s distcp is taking long time to build file listing client code before starting mappers For a dataset I used M files K dirs it was taking minutes before my fix in HADOOP and minutes after the fix Speed up distcp buildListing using threadpool
Non-existence - ban decision,With the commit of HADOOP the CHANGES txt files are now EOLed We should remove them Remove all of the CHANGES txt files
Structural decision,The current way we generate these build artifacts is awful Plus they are ugly and in the case of release notes very hard to pick out what is important Rework the changelog and releasenotes
Behavioral decision,Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request Using JWT provides a number of benefits It is not tied to any specific authentication mechanism so buys us many SSO integrations It is cryptographically verifiable for determining whether it can be trusted Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing validating and parsing JWT tokens Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth
Behavioral decision,Currently ViewFileSystem does not dispatch snapshot methods through the mount table All snapshot methods throw UnsupportedOperationException even though the underlying mount points could be HDFS instances that support snapshots We need to update ViewFileSystem to implement the snapshot methods ViewFileSystem should support snapshot methods
Non-existence - ban decision,distcpv is pretty much unsupported we should just remove it Remove DistCpV and Logalyzer
Non-existence - ban decision,The EMC ViPR ECS object storage platform uses proprietary headers starting by x emc like Amazon does with x amz Headers starting by x emc should be included in the signature computation but it s not done by the Amazon S Java SDK it s done by the EMC S SDK When s a copy an object it copies all the headers but when the object includes x emc headers it generates a signature mismatch Removing the x emc headers from the copy would allow s a to be compatible with the EMC ViPR ECS object storage platform Removing the x which aren t x amz headers from the copy would allow s a to be compatible with any object storage platform which is using proprietary headers Ignore x and response headers when copying an Amazon S object
Behavioral decision,FileUtil copyMerge is currently unused in the Hadoop source tree In branch it had been part of the implementation of the hadoop fs getmerge shell command In branch the code for that shell command was rewritten in a way that no longer requires this method Please check more details here https issues apache org jira browse HADOOP focusedCommentId page com atlassian jira plugin system issuetabpanels comment tabpanel comment Deprecate FileUtil copyMerge
Non-existence - ban decision,According to the discussion in HADOOP we should remove io native lib available from trunk and always use native libraries if they exist Remove io native lib available
Behavioral decision,When a new file is added the source is dev null rather than the root of the tree which would mean a a b prefix Allow for this Allow smart apply patch sh to add new files in binary git patches
Non-existence - ban decision,With HADOOP now committed we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts converting them to use the new slaves option Additionally the documentation should be updated to reflect these new command options Update sbin commands and documentation to use new slaves option
Behavioral decision,Add a slaves shell option to hadoop config sh to trigger the given command on slave nodes This is required to deprecate hadoop daemons sh and yarn daemons sh Add slaves shell option
Behavioral decision,HadoopKerberosName has been around as a secret hack for quite a while We should clean up the output and make it official by exposing it via the hadoop command Expose HadoopKerberosName as a hadoop subcommand
Behavioral decision,An RPC server handler thread is tied up for each incoming RPC request This isn t ideal since this essentially implies that RPC operations should be short lived and most operations which could take time end up falling back to a polling mechanism Some use cases where this is useful YARN submitApplication which currently submits followed by a poll to check if the application is accepted while the submit operation is written out to storage This can be collapsed into a single call YARN allocate requests and allocations use the same protocol New allocations are received via polling The allocate protocol could be split into a request heartbeat along with a awaitResponse The request heartbeat is sent only when there s a request or on a much longer heartbeat interval awaitResponse is always left active with the RM and returns the moment something is available MapReduce Tez task to AM communication is another example of this pattern The same pattern of splitting calls can be used for other protocols as well This should serve to improve latency as well as reduce network traffic since the keep alive heartbeat can be sent less frequently I believe there s some cases in HDFS as well where the DN gets told to perform some operations when they heartbeat into the NN Allow handoff on the server side for RPC requests
Structural decision,We had an application sitting on top of Hadoop and got problems using jsch once we switched to java Got this exception Upgrading to jsch from jsch fixed the issue for us but then it got in conflict with hadoop s jsch version we fixed this for us by jarjar ing our jsch version So i think jsch got introduce by namenode HA HDFS So you guys should check if the ssh part is properly working for java or preventively upgrade the jsch lib to jsch Some references to problems reported http sourceforge net p jsch mailman jsch users thread loom T post gmane org https issues apache org bugzilla show bug cgi id Upgrade jsch lib to jsch to avoid problems running on java
Behavioral decision,This should be hdfs dfsadmin and yarn rmadmin ServiceLevelAuth still references hadoop dfsadmin mradmin
Structural decision,It would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure This would allow us to pull the HDFS MapReduce and YARN shell functions out of hadoop functions sh Additionally it should let rd parties such as HBase influence things like classpaths at runtime Pluggable shell integration
Non-existence - ban decision,Along the same vein as HADOOP there are now several remaining usages of guava APIs that are now incompatible with a more recent version e g This JIRA proposes eliminating those usages With this the hadoop base compatible with guava Remove some uses of obsolete guava APIs from the hadoop codebase
Behavioral decision,It is a very common shell pattern in x to effectively replace sub project specific vars with generics We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated Additionally we should use this shell function to deprecate the shell vars that are holdovers already Deprecate shell vars
Non-existence - ban decision,The permission AccessControlException has been deprecated for last major releases and it should be removed Removed deprecated o a h fs permission AccessControlException
Behavioral decision,The system should be able to read in user defined env vars from hadooprc Add support for hadooprc
Structural decision, Rewrite sls rumen to use new shell framework
Structural decision,As hadoop will drop the support of Java the jenkins slaves should be compiling code using Java Move jenkins to Java
Behavioral decision,During heavy shuffle packet loss for IPC packets was observed from a machine Avoid packet loss and speed up transfer by using x QOS bits for the packets Add a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY
Behavioral decision,DistCp uses ThrottleInputStream which provides a bandwidth throttling on a specified stream Currently Distcp allows the max bandwidth value in Mega Bytes which does not accept fractional values It would be better if it accepts the Max Bandwitdh in fractional MegaBytes Due to this we are not able to throttle the bandwidth in KBs in our prod setup Allow ditscp to accept bandwitdh in fraction MegaBytes
Structural decision,Hadoop currently supports one JVM defined through JAVA HOME Since multiple JVMs Java are active it will be helpful if there is an user configuration to choose the custom but supported JVM for her job In other words user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM Allow user to choose JVM for container execution
Structural decision,Java is coming quickly to various clusters Making sure Hadoop seamlessly works with Java is important for the Apache community This JIRA is to track the issues experiences encountered during Java migration If you find a potential bug please create a separate JIRA either as a sub task or linked into this JIRA If you find a Hadoop or JVM configuration tuning you can create a JIRA as well Or you can add a comment here Umbrella Support Java in Hadoop
Behavioral decision,There is little to no reason for it to call hadoop daemon sh anymore hadoop daemons sh should just call hdfs directly
Behavioral decision,As part of HADOOP java execution across many different shell bits were consolidated down to effectively two routines Prior to calling those two routines the CLASSPATH is exported This export should really be getting handled in the exec function and not in the individual shell bits Additionally it would be good if there was so that bash x would show the content of the classpath or even a debug classpath option that would echo the classpath to the screen prior to java exec to help with debugging CLASSPATH handling should be consolidated debuggable
Structural decision,Post HADOOP we need to rework how heap is configured for small footprint machines deprecate some options introduce new ones for greater flexibility rework heap management vars
Structural decision,Write a metrics sink plugin for Hadoop to send metrics directly to Apache Kafka in addition to the current Graphite Hadoop https issues apache org jira browse HADOOP Ganglia and File sinks metrics sink plugin for Apache Kafka
Structural decision,KMS and HttpFS are using Tomcat we should move it to to get bug fixes and security fixes We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS Update Tomcat version used by HttpFS and KMS to latest x version
Structural decision,kms was not rewritten to use the new shell framework It should be reworked to take advantage of it Rewrite kms to use new shell framework
Non-existence - ban decision,We should make an effort to clean up the shell env var name space by removing unsafe variables See comments for list Rename remove non HADOOP etc from the shell scripts
Behavioral decision,Storm would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS STORM But to do so we need to open up access to some of APIs Most notably FileSystem addDelegationTokens Token renew Credentials getAllTokens and UserGroupInformation but there may be others At a minimum adding in storm to the list of allowed API users But ideally making them public Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR or tools that reuse MR input formats Open up already widely used APIs for delegation token fetching renewal to ecosystem projects
Non-existence - ban decision,Hadoop streaming no longer requires many classes in o a h record This jira removes the dead code Remove dead classes in hadoop streaming
Structural decision,The classes in o a h record have been deprecated for more than a year and a half They should be removed As the first step the jira moves all these classes into the hadoop streaming project which is the only user of these classes Move o a h record to hadoop streaming
Structural decision,Jetty is no longer maintained Update the dependency to jetty Update jetty dependency to version
Behavioral decision,As noted in MAPREDUCE and HADOOP LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations LocalDirAllocator should avoid holding locks while accessing the filesystem
Behavioral decision,Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods Cleanup TestFilterFileSystem
Behavioral decision,Our current dataset.registerTempTable does not actually materialize data. So, it should be considered as creating a temp view. We can deprecate it and create a new method called dataset.createTempView(replaceIfExists: Boolean). The default value of replaceIfExists should be false. For registerTempTable, it will call dataset.createTempView(replaceIfExists = true). Deprecate registerTempTable and add dataset.createTempView
Structural decision,This is necessary for s3 reads and writes to work correctly with some hadoop versions. Add jets3t dependency to Spark Build