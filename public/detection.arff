@relation designdecisions

@attribute class {1, 0}
@attribute DecisionText string

@data
1,'Update sparkml and sparkmllib package Since we decided to switch sparkmllib package into maintenance mode in 20'
1,'20 python converage mltuning module See parent task SPARK14813'
1,'consolidate ExpressionEncodertuple and Encoderstuple'
1,'Move hash expressions from miscscala into hashscala hashscala was getting pretty long and its not obvious that hash expressions belong there Creating a hashscala to put all the hash expressions'
1,'Deprecate NIO ConnectionManager We should deprecate ConnectionManager in 15 before removing it in 16'
1,'Upgrade Tachyon dependency to 082 I think that we should upgrade from Tachyon 081 to 082 in order to get the fix for'
1,'Upgrade parquetmr to 181 parquetmr 181 fixed several issues that affect Spark For example PARQUET201 SPARK9407'
1,'Refactor MemoryStore to be testable independent of BlockManager It would be nice to refactor the MemoryStore so that it can be unittested without constructing a full BlockManager or needing to mock tons of things'
1,'Reimplement stat functions as declarative function Benefits from codegen the declarative aggregate function could be much faster than imperative one we should reimplement all the builtin aggregate functions as declarative one For skewness and kurtosis we need to benchmark it to make sure that the declarative one is actually faster than imperative one'
1,'Upgrade to MiMa 0111 We should upgrade to the latest release of MiMa 0111 in order to include MiMa checks'
1,'20 python converage mlrecommendation module See parent task SPARK14813'
1,'Merge HiveSqlAstBuilder and SparkSqlAstBuilder In Spark 20 we shouldnt have two parsers anymore There should be only a single one'
1,'TrainValidationSplit  is missing in pysparkmltuning I was investingating progress in SPARK10759 and I noticed that there is no TrainValidationSplit  in pysparkmltuning module Java Scalas examples SPARK10759 use orgapachesparkmltuningTrainValidationSplit that is not available from Python and this blocks SPARK10759 Does the  have different name in PySpark maybe Also I couldnt find any JIRA task to saying it need to be implemented Is it by design that the TrainValidationSplit estimator is not ported to PySpark If not that is if the estimator needs porting then I would like to contribute'
1,'Umbrella Split testpatch off into its own TLP Given testpatchs tendency to get forked into a variety of different projects it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base'
1,'Upgrade Hadoop MiniKDC with Kerby As discussed in the mailing list wed like to introduce Apache Kerby into Hadoop Initially its good to start with upgrading Hadoop MiniKDC with Kerby offerings Apache Kerby as an Apache Directory sub project is a Java Kerberos binding It provides a SimpleKDC server that borrowed ideas from MiniKDC and implemented all the facilities existing in MiniKDC Currently MiniKDC depends on the old Kerberos implementation in Directory Server project but the implementation is stopped being maintained Directory community has a plan to replace the implementation using Kerby MiniKDC can use Kerby SimpleKDC directly to avoid depending on the full of Directory project Kerby also provides nice identity backends such as the lightweight memory based one and the very simple json one for easy development and test environments'
1,'Upgrade Jetty to latest version of 8 9 It looks like the head master branch of Spark uses quite an old version of Jetty 8114v20131031 There have been some announcement of security vulnerabilities notably in 2015 and there are versions of both 8 and 9 that address those We recently left a webui port open and had the server compromised within days Albeit this upgrade shouldnt be the only security improvement made the current version is clearly vulnerable asis'
1,'Rewrite Intersect phyiscal plan using semijoin Our current Intersect physical operator simply delegates to RDD intersect We should remove the Intersect physical operator and simply transform a logical intersect into a semijoin This way we can take advantage of all the benefits of join implementations eg managed memory code generation broadcast joins'
1,'Move sparkec2 from mesos to amplab'
1,'20 python converage mlregression module See parent task SPARK14813'
1,'Upgrade Parquet to 182 Apache Parquet 182 is released officially last week on 26 Jan This issue aims to bump Parquet version to 182 since it includes many fixes'
1,'Add support for pluggable cluster manager Currently Spark allows only a few cluster managers viz Yarn Mesos and Standalone But as Spark is now being used in newer and different use cases there is a need for allowing other cluster managers to manage spark components One such use case is embedding spark components like executor and driver inside another process which may be a datastore This allows colocation of data and processing Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again So this JIRA requests two functionalities Support for external cluster managers Allow a cluster manager to clean up the tasks without taking the parent process down'
1,'Add conf to mesos dispatcher process Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher The only option right now is to created a property file'
1,'Support for classes defined in package objects When you define a  inside of a package object the name ends up being something like orgmycompanyprojectpackage$MyClass However when reflect on this we try and load orgmycompanyprojectMyClass'
1,'Naive Bayes wrapper in SparkR Following SPARK13011 we can add a wrapper for naive Bayes in SparkR Rs naive Bayes implementation is from package e1071 with signature It should be easy for us to match the parameters'
1,'Reimplement TypedAggregateExpression to DeclarativeAggregate'
1,'Refactor StaticInvoke Invoke and NewInstance Refactor StaticInvoke Invoke and NewInstance as Introduce InvokeLike to extract common logic from StaticInvoke Invoke and NewInstance to prepare arguments Remove unneeded null checking and fix nullability of NewInstance Modify to short circuit if arguments have null when propageteNull true'
1,'Add unit tests framework Mockito Common tests are functional tests or end to end It makes sense to have Mockito framework for the convenience of true unit tests development'
1,'Upgrade HTrace version Were currently pulling in version 401incubating I think we should upgrade to the latest 410incubating'
1,'Move regexp unit tests to RegexpExpressionsSuite'
1,'Rename some window rank function names for SparkR Change cumeDist cumedist denseRank denserank percentRank percentrank rowNumber rownumber There are two reasons that we should make this change We should follow the naming convention rule of R Spark DataFrame has deprecated the old convention such as cumeDist and will remove it in Spark 20 Its better to fix this issue before 16 release otherwise we will make breaking API change'
1,'Upgrade MQTT dependency to use mqttclient 101 mqtt client 040 is breaking Spark build'
1,'Add Codec for ZStandard Compression ZStandard has been used in production for 6 months by facebook now v10 was recently released Create a codec for this library'
1,'Centralize deprecated configs in SparkConf Deprecated configs are currently all strewn across the code base It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere'
1,'Update Yetus to 020 Update Yetus to 020'
1,'Move StringToColumn implicit  into SQLImplicits They were kept in SQLContextimplicits object for binary backward compatibility in the Spark 1x series It makes more sense for this API to be in SQLImplicits since thats the single  that defines all the SQL implicits'
1,'replace slaves with workers slavessh and the slaves file should get replace with workerssh and a workers file'
1,'Refactory command in spark sql Fix a todo in spark sql remove Command and use RunnableCommand instead'
1,'Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver'
1,'we need some rpc retry framework We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances In particular we often end up with calls to rpcs being wrapped with retry loops for timeouts We should be able to make a retrying proxy that will call the rpc and retry in some circumstances'
1,'Incorporate Aliyun OSS file system implementation Aliyun OSS is widely used among China cloud users but currently it is not easy to access data laid on OSS storage from users Hadoop Spark application because of no original support for OSS in Hadoop This work aims to integrate Aliyun OSS with Hadoop By simple configuration Spark Hadoop applications can read write data from OSS without any code change Narrowing the gap between users APP and data storage like what have been done for S3 in Hadoop'
1,'Upgrade to JUnit 4 Amongst other things JUnit 4 has better support for classwide set up and tear down via BeforeClass and AfterClass annotations and more flexible assertions It would be nice to be able to take advantage of these features in tests we write JUnit 4 can run tests written for JUnit 381 without any changes'
1,'Add WeibullGenerator for RandomDataGenerator components SPARK8518 need to generate random data which follow Weibull distribution'
1,'Rename StringIndexerInverse to IndexToString What StringIndexerInverse does is not strictly associated with StringIndexer and the name is not super clear'
1,'Structured streaming support for consuming from Kafka This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming Here is the design doc for an initial version of the Kafka Source Structured streaming doesnt have support for kafka yet I personally feel like time based indexing would make for a much better interface but its been pushed back to kafka 0101'
1,'move RowMatrixdspr to BLAS component We implemented dspr with sparse vector support in RowMatrix This method is also used in WeightedLeastSquares and other places It would be useful to move it to linalgBLAS modue'
1,'Rewrite kms to use new shell framework kms was not rewritten to use the new shell framework It should be reworked to take advantage of it'
1,'Replace SQLContext with SparkSession in MLlib Use the latest Sparksession to replace the existing SQLContext in MLlib'
1,'Update Fastutil Really simple request to upgrade fastutil to 65x The current version 64x has some minor APIs in the Object2xxOpenHashMap structures which is used in many places in Spark and has been marked deprecated Plus there is a conflict with another library we are using saddle which uses a newer version of fastutil'
1,'rework heap management vars PostHADOOP9902 we need to rework how heap is configured for small footprint machines deprecate some options introduce new ones for greater flexibility'
1,'Upgrade pyspark to use py4j 09'
1,'Update Tomcat version used by HttpFS and KMS to latest 6x version KMS and HttpFS are using Tomcat 6037 we should move it to 6041 to get bug fixes and security fixes We should add a property with the tomcat version in the hadoopproject POM and use that property from KMS and HttpFS'
1,'20 python converage mlclassification module See parent task SPARK14813'
1,'Collect Mesos support code into a module profile I think this is fairly easy and would be beneficial as more work goes into Mesos It should separate into a module like YARN does just on principle really but because it also means anyone that doesnt need Mesos support can build without it Im entirely willing to take a shot at this'
1,'Deprecate runs in kmeans runs introduces extra complexity and overhead in MLlibs kmeans implementation I havent seen much usage with runs not equal to 1 We can deprecate this method in 16 and remove or void it in 17 It helps us simplify the implementation'
1,'JDK8 Update guice version to 40 guice 30 doesnt work with lambda statement We should upgrade it to 40 which includes the fix'
1,'Pick default page size more intelligently Previously we use 64MB as the default page size which was way too big for a lot of Spark applications especially for single node This patch changes it so that the default page size if unset by the user is determined by the number of cores available and the total execution memory available'
1,'Move sparksqlhivethriftServersingleSession to SQLConf Since sparksqlhivethriftServersingleSession is a configuration of SQL component this conf can be moved from SparkConf to StaticSQLConf When we introduced sparksqlhivethriftServersingleSession all the SQL configuration can be modified in different sessions Later static SQL configuration is added It is a perfect fit for sparksqlhivethriftServersingleSession Previously we did the same move for sparksqlwarehousedir from SparkConf to StaticSQLConf'
1,'Change ContextCleanerreferenceBuffer to ConcurrentHashMap to make it faster Right now ContextCleanerreferenceBuffer is ConcurrentLinkedQueue and the time complexity of the remove action is O n It can be changed to use ConcurrentHashMap whose remove is O 1'
1,'20 python coverage mlfeature module See parent task SPARK14813 ~bryanc did this component'
1,'Upgrade Zinc from 0353 to 039 We should update to the latest version of Zinc in order to match our SBT version'
1,'make MultilayerPerceptronClassifier layers and weights public make MultilayerPerceptronClassifier layers and weights public'
1,'Update LogisticCostAggregator serialization code to make it consistent with LinearRegressionmodule Update LogisticCostAggregator serialization code to make it consistent with LinearRegression module'
1,'Add package vignette to SparkR'
1,'Replace SQLContext with SparkSession in ML MLLib This issue replaces all deprecated SQLContext occurrences with SparkSession in ML MLLib module except the following two classes These two classes use SQLContext as their function arguments'
1,'Update ADLS SDK to 214 ADLS has multiple upgrades since the version 2011 we are using 211 212 and 214'
1,'Refactor GLMs code in SparkRWrappers We use a single object SparkRWrappers to wrap calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakrmlr instead of sparkmlapir'
1,'Minor refactoring and cleanup in BlockManager block status reporting and block removal As a precursor to fixing a block fetch bug Id like to split a few small refactorings in BlockManager into their own patch hence this JIRA'
1,'Upgrade Tachyon dependency to 060'
1,'Move Google Test Framework code from mapreduce to hadoopcommon The mapreduce project has Google Test Framework code to allow testing of native libraries This should be moved to hadoopcommon so that other projects can use it as well'
1,'Consolidate local scheduler and cluster scheduler We should consolidate LocalScheduler and ClusterScheduler given most of the functionalities are duplicated in both This can be done by removing the LocalScheduler and create a LocalSchedulerBackend that connects directly to an Executor'
1,'SQLTab should be shared by across sessions We should share the SQLTab across sessions'
1,'Pluggable shell integration It would be useful to provide a way for core and noncore Hadoop components to plug into the shell infrastructure This would allow us to pull the HDFS MapReduce and YARN shell functions out of hadoopfunctionssh Additionally it should let 3rd parties such as HBase influence things like classpaths at runtime'
1,'Separate out local linear algebra as a standalone module without Spark dependency Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment We can call the new module mlliblocal which might contain local models in the future The major issue is to remove dependencies on userdefined types The package name will be changed from mllib to ml For example Vector will be changed from orgapachesparkmlliblinalgVector to orgapachesparkmllinalgVector The return vector type in the new ML pipeline will be the one in ML package however the existing mllib code will not be touched As a result this will potentially break the API Also when the vector is loaded from mllib vector by Spark SQL the vector will automatically converted into the one in ml package'
1,'Separate configs between shuffle and RPC SPARK6028 uses network module to implement RPC However there are some configurations named with sparkshuffle prefix in the network module We should refactor them and make sure the user can control them in shuffle and RPC separately'
1,'Update mavenenforcerplugin version to 141'
1,'Refactor kmeans code in SparkRWrappers We use a single object SparkRWrappers to wrap calls to glm and kmeans in SparkR This is quite hard to maintain We should refactor them into separate wrappers like AFTSurvivalRegressionWrapper and NaiveBayesWrapper The package name should be spakrmlr instead of sparkmlapir'
1,'Use sqlContext from MLlibTestSparkContext for sparkml test suites Use sqlContext from MLlibTestSparkContext rather than creating new one for sparkml test cases'
1,'Refactor code generation to get data for ColumnVector ColumnarBatch Code generation to get data from ColumnVector and ColumnarBatch is becoming pervasive The code generation part can be reused by multiple components eg parquet reader data cache and so on This JIRA refactors the code generation part as a trait for ease of reuse'
1,'Upgrade derby to 101211 from 101111 This JIRA is to upgrade the derby version from 101111 to 101211 Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark I now believe it is required based on comments for the pull request and so this is only a dependency upgrade The upgrade is due to an already disclosed vulnerability CVE20151832 in derby 101111 We used search and will be checking for any other problems in a variety of libraries too investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this'
1,'Update WASB driver to use the latest version 420 of SDK for Microsoft Azure Storage Clients Update WASB driver to use the latest version 420 of SDK for Microsoft Azure Storage Clients We are currently using version 220 of the SDK Version 420 brings some breaking changes Need to fix code to resolve all these breaking changes and certify that everything works properly'
1,'Refactor R mllib for easier ml implementations'
1,'Nettybased RPC env should support a clientonly mode The new netty RPC still behaves too much like akka it requires both client eg an executor and server eg the driver to listen for incoming connections That is not necessary since sockets are fullduplex and RPCs should be able to flow either way on any connection Also because the semantics of the nettybased RPC dont exactly match akka you get weird issues like SPARK10987 Supporting a clientonly mode also reduces the number of ports Spark apps need to use'
1,'Move orgapachesparkLogging into orgapachesparkinternalLogging Logging was made private in Spark 20 If we move it then users would be able to create a Logging trait themselves to avoid changing their own code Alternatively we can also provide in a compatibility package that adds logging'
1,'merge SortMergeJoin and SortMergeOuterJoin There are lots of duplicated code in SortMergeJoin and SortMergeOuterJoin should merge them and reduce the duplicated code'
1,'Prepare GenericArrayData implementation specialized for a primitive array There is a ToDo of GenericArrayData  which is to eliminate boxing unboxing for a primitive array It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance'
1,'Update Chukwa parsers Update Chukwa parsers'
1,'Add a task failure listener component to TaskContext TaskContext supports task completion callback which gets called regardless of task failures However there is no way for the listener to know if there is an error This ticket proposes adding a new listener that gets called when a task fails'
1,'Upgrade Tachyon dependency to 080 Update the tachyonclient dependency from 071 to 080'
1,'move CreateTables to HiveStrategies'
1,'Change kms server port number which conflicts with HMaster port number The HBases HMaster port number conflicts with Hadoop kms port number Both uses 16000 There might be use cases user need kms and HBase present on the same cluster The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories Users would have to manually override the default port of either application on their cluster It would be nice to have different default ports so kms and HBase could naturally coexist'
1,'Upgrade Tomcat to 6048 KMS and HttpFS currently uses Tomcat 6044 propose to upgrade to the latest version is 6048'
1,'deprecate weights and use coefficients instead in ML models The name weights becomes confusing as we are supporting weighted instanced As discussed in 7884 we want to deprecate weights and use coefficients instead Deprecate but do not remove weights Only make changes under sparkml'
1,'fs tests should not be placed in hdfs The following tests are under the orgapachehadoopfs package but were moved to hdfs subdirectory by HADOOP5135 Some of them are not related to hdfs eg TestFTPFileSystem These files should be moved out from hdfs and should not use hdfs codes Some of them are testing hdfs features eg TestStickyBit They should be defined under orgapachehadoophdfs package'
1,'AsyncCallHandler should use an event driven architecture to handle async calls'
1,'More officially deprecate support for Python 26 Java 7 and Scala 210 Plan Mark it very explicit in Spark 210 that support for the aforementioned environments are deprecated Remove support it Spark 220'
1,'Add an AutoCloseableLock  Introduce an AutoCloseableLock  that is a thin wrapper over RentrantLock It allows using RentrantLock with trywithresources syntax The wrapper functions perform no expensive operations in the lock acquire release path'
1,'Validate XMLs if a relevant tool is available when using scripts Given that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves unless a tool that manages for you is used it would be good to also validate the provided config XML sitexml files with a tool like xmllint or maybe Xerces somehow when running a command or at least when starting up daemons We should use this only if a relevant tool is available and optionally be silent if the env requests'
1,'compaction utility for directories Utility will collapse the contents of a directory into a small number of files'
1,'Intel ISAL libraries should be added to the Dockerfile HADOOP11887 added a compile and runtime dependence on the Intel ISAL library but didnt add it to the Dockerfile so that it could be part of the Dockerbased build environment'
1,'Rework hadooptools As hadooptools grows bigger and bigger its becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows Lets rework this to be smarter'
1,'Simplify and Speedup HadoopFSRelation A majority of Spark SQL queries likely run though HadoopFSRelation however there are currently several complexity and performance problems with this code path The  mixes the concerns of file management schema reconciliation scan building bucketing partitioning and writing data For very large tables we are broadcasting the entire list of files to every executor For partitioned tables we always do an extra projection This results not only in a copy but undoes much of the performance gains that we are going to get from vectorized reads This is an umbrella ticket to track a set of improvements to this codepath'
1,'Move RDD creation logic from FileSourceStrategyapply We embed partitioning logic in FileSourceStrategyapply making the function very long This is a small refactoring to move it into its own functions Eventually we would be able to move the partitioning functions into a physical operator rather than doing it in physical planning'
1,'Upgrade comthoughtworksparanamer paranamer to 26 Causes jackson fail to handle byte array defined in a case Lets upgrade paranamer'
1,'Refactoring Instance out from LOR and LIR and also cleaning up some code Refactoring Instance case  out from LOR and LIR and also cleaning up some code'
1,'Upgrade to Apache Yetus 030 Upgrade yetuswrapper to be 030 now that it has passed vote'
1,'Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop Currently downstream projects that want to integrate with different Hadoopcompatible file systems like WASB and S3A need to list dependencies on each one This creates an ongoing maintenance burden for those projects because they need to update their build whenever a new Hadoopcompatible file system is introduced This issue proposes adding a new artifact that transitively includes all Hadoopcompatible file systems Similar to hadoopclient this new artifact will consist of just a pomxml listing the individual dependencies Downstream users can depend on this artifact to sweep in everything and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version'
1,'create a script to setup application in order to create root directories for application such hbase hcat hive etc'
1,'Make JvmPauseMonitor an AbstractService The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and even after HADOOP12313 is not thread safe both start and stop are potentially reentrant It also requires every  which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle which for all Yarn services is the YARN app lifecycle as implemented in Hadoop common Making the monitor a subclass of AbstractService and moving the init start & stop operations in serviceInit serviceStart & serviceStop methods will fix the concurrency and state model issues and make it trivial to add as a child to any YARN service which subclasses CompositeService most the NM and RM apps will be able to hook up the monitor simply by creating one in the ctor and adding it as a child'
1,'pull shell code out of hadoopdist Lets pull the shell code out of the hadoopdist pomxml'
1,'move BucketSpec to catalyst module and use it in CatalogTable'
1,'Upgrade snappyjava to 1115 We should upgrade snappyjava to 1115 across all of our maintenance branches This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream this operation is always an error but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid corrupted streams This should be a major help in the Snappy debugging work that Ive been doing'
1,'Rename PlatformDependentUNSAFE to Platform PlatformDependentUNSAFE is way too verbose'
1,'Use consistent naming for expression test suites I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite and the endtoend tests FunctionsSuite Itd be great to make all test suites consistent with this naming convention'
1,'Move hbase out of hadoop core Move hbase out of hadoop core Move its JIRA issues and move it in svn'
1,'Simplify InternalRow hierarchy The current InternalRow hierarchy makes a difference between immutable and mutable rows In practice we cannot guarantee that an immutable internal row is immutable you can always pass a mutable object as an one of its elements Lets make all internal rows mutable and reduce the complexity'
1,'Upgrade Kinesis Client Library to the latest stable version We use KCL 130 in the current master KCL 140 added integration with Kinesis Producer Library KPL and support auto deaggregation It would be great to upgrade KCL to the latest stable version Note that the latest version is 161 and 160 restored compatibility with dynamodbstreamskinesisadapter which was broken in 140'
1,'Add KolmogorovSmirnov Test to SparkR KolmogorovSmirnov Test is a popular nonparametric test of equality of distributions There is implementation in MLlib It will be nice if we can expose that in SparkR'
1,'Move hashstyle shuffle code out of ExternalSorter and into own file ExternalSorter contains a bunch of code for handling the bypassMergeThreshold hashstyle shuffle path I think that it would significantly simplify the code to move this functionality out of ExternalSorter and into a separate  which shares a common interface insertAll writePartitionedFile'
1,'Rename upstreams to inputRDDs in WholeStageCodegen'
1,'Refactorize Normalizer to make code cleaner In this refactoring the performance is slightly increased by removing the overhead from breeze vector The bottleneck is still in breeze norm which is implemented by activeIterator This inefficiency of breeze norm will be addressed in next PR At least this PR makes the code more consistent in the codebase'
1,'Add Hadoop native library to javalibrarypath compression'
1,'Eclipsebased GUI DFS explorer and basic Map Reduce job launcher to increase productivity in our current project which makes a heavy use of Hadoop we wrote a small Eclipsebased GUI application which basically consists in 2 views A HDFS explorer adapted from Eclipse filesystem explorer example A MapReduce very simple job launcher'
1,'Shade Kryo in our custom Hive 121 fork In order to upgrade to Kryo 3 we need to shade Kryo in our custom Hive 121 fork'
1,'Add FoldablePropagation component optimizer This issues aims to add new FoldablePropagation optimizer component that propagates foldable expressions by replacing all attributes with the aliases of original foldable expression Other optimizations will take advantage of the propagated foldable expressions eg EliminateSorts optimizer now can handle the following Case 2 and 3
1,'make SubqueryHolder an inner '
1,'Utility classes for Archive and ChukwaRecord files Create some utility classes to dump both Archive and ChukwaRecords files'
1,'Create a testpatch script for Hudson We should create a script that Hudson uses to execute testpatch that is in source control so modifications to testpatchsh arguments can be done w o updating Hudson'
1,'Install and configure RStudio server on Spark EC2 This will make it convenient for R users to use SparkR from their browsers'
1,'Merge functionality in Hive module into SQL core module This is an umbrella ticket to reduce the difference between sql core and sql hive Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs'
1,'Update KafkaDStreams to new Kafka 010 Consumer API Kafka 09 already released and it introduce new consumer API that not compatible with old one So I added new consumer api I made separate classes in package orgapachesparkstreamingkafkav09 with changed API I didnt remove old classes for more backward compatibility User will not need to change his old spark applications when he uprgade to new Spark version'
1,'Bring back the hivesitexml support for Spark 20 Right now Spark 20 does not load hivesitexml Based on users feedback it seems make sense to still load this conf file Originally this file was loaded when we load HiveConf  and all settings can be retrieved after we create a HiveConf instances Lets avoid of using this way to load hivesitexml Instead since hivesitexml is a normal hadoop conf file we can first find its url using the classloader and then use Hadoop Configurations addResource or add hivesitexml as a default resource through ConfigurationaddDefaultResource to load confs Please note that hivesitexml needs to be loaded into the hadoop conf used to create metadataHive'
1,'Use different physical plan for existing RDD and data sources Right now we use PhysicalRDD for both existing RDD and data sources they are becoming much different we should use different physical plans for them'
1,'Merge HiveSqlAstBuilder and SparkSqlAstBuilder In Spark 20 we shouldnt have two parsers anymore There should be only a single one'
1,'Break SQLQuerySuite out into smaller test suites'
1,'Allow user to choose JVM for container execution Hadoop currently supports one JVM defined through JAVAHOME Since multiple JVMs Java 6789 are active it will be helpful if there is an user configuration to choose the custom but supported JVM for her job In other words user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM'
1,'WindowsGetSpaceUsed constructor should be public WindowsGetSpaceUsed constructor should be made public Otherwise building using builder will not work'
1,'Rename HiveTypeCoercion TypeCoercion We originally designed the type coercion rules to match Hive but over time we have diverged It does not make sense to call it HiveTypeCoercion anymore'
1,'Refactor SortMergeOuterJoin to reduce duplication As of the writing of this message LeftOuterIterator and RightOuterIterator are symmetrically identical If someone makes a change in one but forgets to do the same thing in the other well end up with inconsistent behavior'
1,'Consolidate the Cholesky solvers in WeightedLeastSquares and ALS There are two Cholesky solvers in WeightedLeastSquares and ALS we should merge them into one'
1,'Rename gen genCode to genCode doGenCode to better reflect the semantics'
1,'Reuse subqueries within single query There could be same subquery within a single query we could reuse the result without running it multiple times'
1,'Make labels public in StringIndexerModel  Necessary for creating inverse IndexToString'
1,'Make metrics naming consistent While working HADOOP6728 I noticed that our metrics naming style is all over the place Capitalized camel case eg FilesCreated in namenode metrics and some rpc metrics uncapitalized camel case eg threadsBlocked in jvm metrics and some rpc metrics lowercased underscored eg byteswritten in datanode metrics and mapreduce metrics'
1,'Rename ExpressionAggregate DeclarativeAggregate Matches more closely with ImperativeAggregate'
1,'Merge ParserUtils and ParseUtils We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process Those name and what they are used for is very similar so I think we can merge them Also the original unescapeSQLString method may have a fault When \u0061 style character literals are passed to the method its not unescaped successfully'
1,'Move DT RF GBT Param setter methods to sub classes Move DT RF GBT Param setter methods to sub classes and deprecate these methods in the Model classes to make them more Javafriendly'
1,'Move LabelCol datatype cast into Predictorfit ClassifiergetNumClasses can not support NonDouble types and classification algos relying on it do not support nondouble labelCol like NavieBayes It is not a reasonable way to do datatype cast everywhere And we can make cast only happen in Predictor'
1,'refactor object operator framework to make it easy to eliminate serializations'
1,'Move jenkins to Java 7 As hadoop 27 will drop the support of Java 6 the jenkins slaves should be compiling code using Java 7'
1,'Incorporate checkcompatibility script which runs Java API Compliance Checker Based on discussion at YETUS445 this code cant go there but its still very useful for release managers A similar variant of this script has been used for a while by Apache HBase and Apache Kudu and IMO JACC output is easier to understand than JDiff'
1,'Bisecting kmeans wrapper module in SparkR Implement a wrapper module in SparkR to support bisecting kmeans'
1,'Update sparklogit in sparkrvignettes sparklogit is added in 21 We need to update sparkvignettes to reflect the changes This is part of SparkR QA work'
1,'Improve constraints propagation in Union Currently Union only takes intersect of the constraints from its children all others are dropped we should try to merge them together'
1,'Chukwa test framework Ability to test endToEnd chukwa pipeline From log file to dataSink From dataSink to demux output'
1,'Eliminate MLlib 20 build warnings from deprecations Several classes and methods have been deprecated and are creating lots of build warnings in branch20 This issue is to identify and fix those items WithSGD classes Change to make  not deprecated object deprecated and public  constructor deprecated Any public use will require a deprecated API We need to keep a nondeprecated private API since we cannot eliminate certain uses Python API streaming algs and examples Use in PythonMLlibAPI Change to using private constructors Streaming algs No warnings after we undeprecate the classes Examples Deprecate or change ones which use deprecated APIs MulticlassMetrics fields precision etc'
1,'Rework the changelog and releasenotes The current way we generate these build artifacts is awful Plus they are ugly and in the case of release notes very hard to pick out what is important'
1,'taskScheduler has some unneeded serialization While it is necessary to have two layers of serialization so that the JAR file and Property info can be deserialized prior to deserializing the Task object the third layer of deserialization is unnecessary We should eliminate a layer of serialization by moving the JARs files and Properties into the TaskDescription '
1,'explicitly declare the commonslang3 dependency as 34 Other people arent seeing this yet but unless you explicitly exclude v 34 of commonslang3 from the azure build which HADOOP13660 does then the dependency declaration of commonslang3 v 332 is creating a resolution conflict Thats a dependency only needed for the local dynamodb & tests'
1,'Use TextFileFormat in implementation of CSVFileFormat Sparks CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing IO performance improvements made in Spark 20 In order to fix this performance problem we should reimplement those read paths in terms of TextFileFormat'
1,'Change project version from 300 to 300alpha1 We want to rename 300 to 300alpha1 for the first alpha release However the version number is also encoded outside of the pomxmls so we need to update these too
1,'move hive hack for data source table into HiveExternalCatalog'
1,'Track current database in SQL HiveContext We already have internal APIs for Hive to do this We should do it for SQLContext too so we can merge these code paths one day'
1,'Implement code generation module for Generate Generate currently does not support code generation Lets add support for CG and for it and its most important generators explode and jsontuple'
1,'swift FS to add a service load metadata file add a metadata file giving the FS impl of swift remove the entry from coredefaultxml'
1,'Port HDFS space quotas to 018 018 already has quotas for HDFS namespace HADOOP3187 HADOOP3938 implements similar quotas for disk space on HDFS in 019 This jira proposes to port HADOOP3938 to 0184'
1,'Refactor PythonRDD to decouple iterator computation from PythonRDD The idea is that most of the logic of calling Python actually has nothing to do with RDD it is really just communicating with a socket there is nothing distributed about it and it is only currently depending on RDD because it was written this way If we extract that functionality out we can apply it to area of the code that doesnt depend on RDDs and also make it easier to test'
1,'Move some Analyzer stuff to Analyzer from DataFrameWriter module DataFrameWriterinsertInto includes some Analyzer stuff We should move it to Analyzer module'
1,'Move treeforest implementation from sparkmllib to sparkml We want to change and improve the sparkml for trees and ensembles but we cannot change the old sparkmllib To support the changes we want to make we should move the implementation from sparkmllib to sparkml We will generalize and modify it but will also ensure that we do not change the behavior Move the unit tests to sparkml and change the sparkmllib unit tests to verify model equivalence'
1,'Support Microsoft Azure Data Lake as a file system in Hadoop h2 This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store ADL from within Hadoop This would enable existing Hadoop applications such has MR HIVE Hbase etc to use ADL store as input or output ADL is ultrahigh capacity Optimized for massive throughput with rich management and security features'
1,'Consolidate different forms of table identifiers Right now we have QualifiedTableName TableIdentifier and SeqString to represent table identifiers We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name database name return unquoted string and return quoted string'
1,'Install rmarkdown R package on Jenkins machines For building SparkR vignettes on Jenkins machines we need the rmarkdown R The package is available'
1,'Publish Sparks forked sbtpomreader to Maven Central Sparks SBT build currently uses a fork of the sbtpomreader plugin but depends on that fork via a SBT subproject which is cloned This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted In order to address these issues I propose to publish a prebuilt binary of our forked sbtpomreader plugin to Maven Central under the orgsparkproject namespace'
1,'Move since annotator to pyspark to be shared by all components Pythons since is defined under pysparksql It would be nice to move it under pyspark to be shared by all components
1,'Rewrite sls rumen to use new shell framework'
1,'Make Columnexpr public Columnexpr is privatesql but its an actually really useful field to have for debugging We should open it up similar to how we use QueryExecution'
1,'Umbrella Support Java 8 in Hadoop Java 8 is coming quickly to various clusters Making sure Hadoop seamlessly works with Java 8 is important for the Apache community This JIRA is to track the issues experiences encountered during Java 8 migration If you find a potential bug please create a separate JIRA either as a subtask or linked into this JIRA If you find a Hadoop or JVM configuration tuning you can create a JIRA as well Or you can add a comment here'
1,'DaemonFactory should be moved from HDFS to common DaemonFactory  is defined in hdfs util common would be a better place for this '
1,'Move userfacing streaming classes into sqlstreaming'
1,'Upgrade jsch lib to jsch0151 to avoid problems running on java7 We had an application sitting on top of Hadoop and got problems using jsch once we switched to java 7 Upgrading to jsch0151 from jsch0149 fixed the issue for us but then it got in conflict with hadoops jsch version So i think jsch got introduce by namenode HA HDFS1623 So you guys should check if the ssh part is properly working for java7 or preventively upgrade the jsch lib to jsch0151'
1,'R Include package vignettes and help pages build source package in Spark distribution We should include in Spark distribution the built source package for SparkR This will enable help and vignettes when the package is used Also this source package is what we would release to CRAN'
1,'Upgrade commonsconfiguration version to 21 Were currently pulling in version 16 I think we should upgrade to the latest 110'
1,'Refactor Move SQLContext HiveContext persession state to separate  This is just a clean up task Today there are all these fields in SQLContext that are not organized in any particular way However since each SQLContext is a session many of these fields are actually isolated persession To minimize the size of these context files and provide a logical grouping that makes more sense I propose that we move these fields into its own  called SessionState'
1,'Split build script for building core hdfs and mapred separately'
1,'The numFields of UnsafeRow should not be changed by pointTo Right now numFields will be passed in by pointTo then bitSetWidthInBytes is calculated making pointTo a little bit heavy It should be part of constructor of UnsafeRow'
1,'Refactor ActorReceiver to support Java Right now the Java users cannot use ActorHelper because it uses special Scala syntax This patch just refactored the codes to provide Java support'
1,'Make DataFrameHolder and DatasetHolder public These two classes should be public since they are used in public code'
1,'Move utilities for binary data into ByteArray The utilities such as Substring substringBinarySQL and BinaryPrefixComparator computePrefix for binary data are put together in ByteArray for easytoread'
1,'Consolidate various listLeafFiles implementations There are 4 listLeafFilesrelated functions in Spark ListingFileCataloglistLeafFiles which calls HadoopFsRelationlistLeafFilesInParallel if the number of paths passed in is greater than a threshold if it is lower then it has its own serial version implemented HadoopFsRelationlistLeafFiles called only by HadoopFsRelationlistLeafFilesInParallel HadoopFsRelationlistLeafFilesInParallel called only by ListingFileCataloglistLeafFiles It is actually very confusing and error prone because there are effectively two distinct implementations for the serial version of listing leaf files This code can be improved by Move all file listing code into ListingFileCatalog since it is the only  that needs this Keep only one function for listing files in serial'
1,'Rename recentProgresses to recentProgress An informal poll of a bunch of users found this name to be more clear'
1,'Use Travis CI for Java Linter and JDK78 compilation test'
1,'Make StreamExecution and progress classes serializable Make StreamExecution and progress classes serializable because it is too easy for it to get captured with normal usage'
1,'Replace devsupport with wrappers to Yetus Now that Yetus has had a release we should rip out the components that make it up from devsupport and replace them with wrappers The wrappers should default to a sane version allow for version overrides via an env var download into patchprocess execute with the given parameters Marking this as an incompatible change since we should also remove the filename extensions and move these into a bin directory for better maintenance towards the future
1,'Decouple deserializer expression resolution from ObjectOperator'
1,'Unit test for sparkml KMeansSummary There is no unit test for KMeansSummary in sparkml Other items which could be fixed here Add Since version to KMeansSummary  Modify clusterSizes method to match GMM method to be robust to empty clusters'
1,'Add LambdaTestUtils  for tests fix eventual consistency problem in contract test setup To make our tests robust against timing problems and eventual consistent stores we need to do more spin & wait for state We have some code in GenericTestUtilswaitFor to await a condition being met but the predicate it calls doesnt throw exceptions theres no way for a probe to throw an exception and all you get is the eventual timed out message We can do better and in closureready languages scala & scalatest groovy and some slider code weve examples to follow Some of that work has been reimplemented slightly in S3ATestUtilseventually I propose adding a  in the test tree Eventually to be a successor replacement for these has an eventually waitfor operation taking a predicate that throws an exception has an evaluate exception which tries to evaluate an answer until the operation stops raising an exception again from scalatest plugin backoff strategies from Scalatest lets you do exponential as well as linear option of adding a special handler to generate the failure exception eg run more detailed diagnostics for the exception text etc be Java 8 lambda expression friendly be testable and tested itself'
1,'20 python converage pysparkmllinalg See parent task SPARK14813'
1,'Use a single URLClassLoader for jars added through SQLs ADD JAR command Right now we stack a new URLClassLoader when a user add a jar through SQLs add jar command This approach can introduce issues caused by the ordering of added jars when a  of a jar depends on another  of another jar In this case when we lookup  B we will not be able to find  A because Jar2 is the parent of Jar1'
1,'Follow on fixups after upgraded minikdc using Kerby We will add the findbugsExcludeFilexml and will get rid of this given kerby100rc3 release Add the kerby version hadoopproject pomxml because hadoopproject pomxml contains the dependencies of all libraries used in all modules of hadoop under dependencyManagement Only here version will be mentioned All other Hadoop Modules will inherit hadoopproject so all submodules will use the same version In submodule version need not be mentioned in pomxml This will make version management easier'
1,'Add command factory to FsShell The FsShell has many chains if then else chains for instantiating and running commands A dynamic mechanism is needed for registering commands such that FsShell requires no changes when adding new commands'
1,'versions of dependencies should be specified in a single place Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on The versions of these libraries are also present in ivy librariesproperties so that when a library is updated it must be updated in two places which is errorprone We should instead only specify library versions in a single place'
1,'Allow compact property description in xml We should allow users to use the more compact form of xml elements The old format would also be supported'
1,'metrics2 sink plugin for Apache Kafka Write a metrics2 sink plugin for Hadoop to send metrics directly to Apache Kafka in addition to the current Graphite Hadoop9704 Ganglia and File sinks'
1,'Mark all Stage ResultStage ShuffleMapStage internal state as private'
1,'Set things up for a top level hadooptools module We need to get things up and running for a top level hadooptools module DistCpV2 will be the first resident of this new home Things we need The module itself and a top level pom with appropriate dependencies Integration with the patch builds for the new module Integration with the postcommit and nightly builds for the new module'
1,'Move sparkec2 scripts to AMPLab It would be easier to maintain the ec2 script separately from Spark releases'
1,'Rename numPartitions in RDD to getNumPartitions to be consistent with pyspark scala'
1,'Reduce Kafka dependencies in hadoopkafka module The newly added Kafka module defines the Kafka dependency as This is unfavorable because its using the server dependency which transitively has the client jars The server dependency includes all of the server code and some larger transitive dependencies like Scala and Zookeeper Instead the pom file should be changed to only depend on the clients jar which is a much smaller footprint'
1,'update apache httpclient version to 452 httpcore to 444 See CVE20126153 CVE20114461 CVE20143577 CVE20155262'
1,'SparkR executors workers support virtualenv Many users have requirements to use third party R packages in executors workers but SparkR can not satisfy this requirements elegantly For example you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios 1 Users can install R packages from CRAN or custom CRANlike repository for each executors 2 Users can load their local R packages and install them on each executors To achieve this goal the first thing is to make SparkR executors support virtualenv like Python conda I have investigated and found packrat is one of the candidates to support virtualenv for R Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space Then SparkR users can install third party packages in the application scope destroy after the application exit and dont need to bother IT administrators to install these packages manually'
1,'unify GetStructField and GetInternalRowField'
1,'Deprecate registerTempTable and add datasetcreateTempView Our current datasetregisterTempTable does not actually materialize data So it should be considered as creating a temp view We can deprecate it and create a new method called datasetcreateTempView replaceIfExists Boolean The default value of replaceIfExists should be false For registerTempTable it will call datasetcreateTempView replaceIfExists true'
1,'JDK8 Set minimum version of Hadoop 3 to JDK 8 Set minimum version of trunk to JDK 8'
1,'Split SparkR mllibR into multiple files SparkR mllibR is getting bigger as we add more ML wrappers Id like to split it into multiple files to make us easy to maintain For R convention its more prefer the first way And Im not sure whether R supports the second organized way will check later Please let me know your preference I think the start of a new release cycle is a good opportunity to do this since it will involves less conflicts If this proposal was approved I can work on it'
1,'Refactor JDBCRDD to expose JDBC to SparkSQL conversion It would be useful if more of JDBCRDDs JDBC to Spark SQL was usable from outside of JDBCRDD this would make it easier to write test harnesses comparing Spark output against other JDBC databases
1,'Update jetty dependency to version 9 Jetty6 is no longer maintained Update the dependency to jetty9'
1,'Add jets3t dependency to Spark Build This is necessary for s3 reads and writes to work correctly with some hadoop versions'
1,'Move Hadoop cloud scripts to Whirr Whirr is a new Apache Incubator project for running cloud services The cloud scripts in src contrib cloud should move to Whirr This should be done in 021 too so the scripts do not appear in that release since having them in Hadoop for a single release would be confusing'
1,'Move oahrecord to hadoopstreaming The classes in oahrecord have been deprecated for more than a year and a half They should be removed As the first step the jira moves all these classes into the hadoopstreaming project which is the only user of these classes'
1,'Improve the Scalability and Robustness of IPC This jira is intended to enhance IPCs scalability and robustness Currently an IPC server can easily hung due to a disk failure or garbage collection during which it cannot respond to the clients promptly This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously the server may be swarmed by requests and cannot work responsively The proposed changes aim to provide a better client server coordination Server should be able to throttle client during burst of requests A slow client should not affect server from serving other clients A temporary hanging server should not cause catastrophic failures to clients Client server should detect remote side failures Examples of failures include 1 the remote host is crashed 2 the remote host is crashed and then rebooted 3 the remote process is crashed or shut down by an operator Fairness Each client should be able to make progress'
1,'Move ResourceCalculatorPlugin from YARN to Common Some of the monitoring functions could be moved from YARN to Common modules for easier sharing'
1,'Common foundation for Hadoop client tools As Hadoop widespreads and matures the number of tools and utilities for users keeps growing Some of them are bundled with Hadoop core some with Hadoop contrib some on their own some are full fledged servers on their own For example just to name a few distcp streaming pipes har pig hive oozie Today there is no standard mechanism for making these tools available to users Neither there is a standard mechanism for these tools to integrate and distributed them with each other The lack of a common foundation creates issues for developers and users'
1,'make hadoopclient set of curated jars available in a distribution tarball One thing that the original patch for HADOOP8082 didnt address is the need for those curated jars to be visible in the final tarball'
1,'Upgrade jaxbapi version Were currently pulling in version 222 I think we should upgrade to the latest 2212'
1,'Refactor Azure Data Lake Store as an independent FileSystem The jira proposes an improvement over HADOOP12666 to remove webhdfs dependencies from the ADL file system client and build out a standalone client At a high level this approach would extend the Hadoop file system  to provide an implementation for accessing Azure Data Lake The scheme used for accessing the file system will continue to be adl The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store'
1,'Missing hadoopcloudstorageproject module in pomxml module hadoopcloudstorageproject is missing in pomxml'
1,'Add support for unix domain sockets to JNI libs For HDFS347 we need to use unix domain sockets This JIRA is to include a library in common which adds a oahnetunix package based on the code from Android apache 2 license'
1,'Removes old Parquet support code As the new Parquet external data source matures we should remove the old Parquet support now'
1,'use StructType in CatalogTable and remove CatalogColumn'
1,'Remove sparkml HashingTF hashingAlg option Since SPARK10574 breaks behavior of HashingTF we should try to enforce good practice by removing the native hashingAlg option in sparkml and pysparkml We can leave sparkmllib and pysparkmllib alone'
1,'Improve performance of max min Currently the generated code for max min is too complicated should be simplified remove unnecessary nullcheck'
1,'Remove ionativelibavailable According to the discussion in HADOOP8642 we should remove ionativelibavailable from trunk and always use native libraries if they exist'
1,'Remove unneeded staging repositories from build'
1,'Remove ConvertNaNs analyzer rule Cast already handles NaN when casting from string to double float I dont think this rule is necessary anymore'
1,'Remove the option to turn off unsafe and codegen We dont sufficiently test the code path with these settings off It is better to just consolidate and focus on making one code path work well'
1,'Implement unhandledFilter interface and remove duplicated Sparkside filtering As discussed here it might need to implement unhandledFilter to remove duplicated Sparkside filtering'
1,'Remove BlockStore interface to more cleanly reflect different memory and disk store responsibilities Today both the MemoryStore and DiskStore implement a common BlockStore API but I feel that this API is inappropriate because it abstracts away important distinctions between the behavior of these two stores For instance the disk store doesnt have a notion of storing deserialized objects so its confusing for it to expose objectbased APIs like putIterator and getValues instead of only exposing binary APIs and pushing the responsibilities of serialization and deserialization to the client As part of a larger BlockManager interface cleanup Id like to remove the BlockStore API and refine the MemoryStore and DiskStore interfaces to reflect more narrow sets of responsibilities for those components'
1,'Remove privatesql and privatespark from sqlexecution package The execution package is meant to be internal and as a result it does not make sense to mark things as privatesql or privatespark It simply makes debugging harder when Spark developers need to inspect the plans at runtime'
1,'Remove final from classes in sparkml trees and ensembles where possible There have been continuing requests eg SPARK7131 for allowing users to extend and modify MLlib models and algorithms If you are a user who needs these changes please comment here about what specifically needs to be modified for your use case'
1,'Remove redundant project in colum pruning rule With column pruning rule in optimizer we will introduce redundant project for some cases We should prevent it'
1,'Remove CacheManager and replace it with new BlockManagergetOrElseUpdate method CacheManager directly calls MemoryStoreunrollSafely and has its own logic for handling graceful fallback to disk when cached data does not fit in memory However this logic also exists inside of the MemoryStore itself so this appears to be unnecessary duplication Thanks to the addition of blocklevel read write locks we can refactor the code to remove the CacheManager and replace it with an atomic getOrElseUpdate BlockManager method'
1,'ExpressionEvalHelpercheckEvaluation should also run the optimizer version We should remove the existing ExpressionOptimizationSuite and update checkEvaluation to also run the optimizer version'
1,'Remove sparksqleagerAnalysis Dataset always does eager analysis now Thus sparksqleagerAnalysis is not used any more Thus we need to remove it'
1,'MemorySink should not call DataFramecollect when holding a lock Otherwise other threads cannot query the content in MemorySink when DataFramecollect takes long time to finish'
1,'Remove runs from KMeans under the pipeline API This requires some discussion Im not sure whether runs is a useful parameter It certainly complicates the implementation We might want to optimize the kmeans implementation with block matrix operations In this case having runs may not be worth the tradeoffs'
1,'remove OverwriteOptions'
1,'Eliminate needless uses of FileSystem exists isFile isDirectory Were cleaning up Hive and Sparks use of FileSystemexists because it is often the case we see code of exists+open exists+delete when the exists probe is needless Against object stores expensive needless Hadoop can set an example here by stripping them out It will also show where there are opportunities to optimise things better and or improve reporting'
1,'Remove sparkdeploymesoszookeeperurl and use sparkdeployzookeeperurl Remove sparkdeploymesoszookeeperurl and use existing configuration sparkdeployzookeeperurl for Mesos cluster mode'
1,'Remove redundant Experimental annotations in sqlstreaming package'
1,'GroupedData should only keep common first order statistics We should remove methods for variance stddev skewness'
1,'Remove EmptyRow  Right now InternalRow is megamorphic because it has many different implementations We should work towards having only one or at most two InternalRow implementations'
1,'Do not use bitmasks during parsing and analysis of CUBE ROLLUP GROUPING SETS We generate bitmasks for grouping sets during the parsing process and use these during analysis These bitmasks are difficult to work with in practice and have lead to numerous bugs I suggest that we remove these and use actual sets instead however we would need to generate these offsets for the groupingid'
1,'Removed deprecated oahfspermissionAccessControlException The oahfspermissionAccessControlException has been deprecated for last major releases and it should be removed'
1,'Remove the internal implicit conversion from Expression to Column in functionsscala'
1,'Clean up and remove SPARKHOME In the spirit of SPARK929 we should clean up the use of SPARKHOME and if possible remove it entirely'
1,'Get rid of LocalHiveContext HiveLocalContext is nearly completely redundant with HiveContext We should consider deprecating it and removing all uses'
1,'Remove fallback in codegen in newMutableProjection it will fallback to InterpretedMutableProjection if failed to compile Since we remove the configuration for codegen we are heavily reply on codegen also TungstenAggregate require the generated MutableProjection to update UnsafeRow should remove the fallback which could make user confusing see the discussion in SPARK13116'
1,'Method SQLContextparseDataType dataTypeString String could be removed Method SQLContextparseDataType dataTypeString String could be removed we should use SparkSessionparseDataType dataTypeString String instead This require updating PySpark'
1,'Remove the use of the deprecated callUDF in MLlib MLlibs Transformer uses the deprecated callUDF API'
1,'Remove ALSsolveLeastSquares This method survived the code review and it has been there since v110 It exposes jblas types Lets remove it from the public API I expect that no one calls it directly'
1,'Observed delay based event time watermarks Whenever we aggregate data by event time we want to consider data is late and outoforder in terms of its event time Since we keep aggregate keyed by the time as state the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data Since the state is a store inmemory we have to prevent building up of this unbounded state Hence we need a watermarking mechanism by which we will mark data that is older beyond a threshold as too late and stop updating the aggregates with them This would allow us to remove old aggregates that are never going to be updated thus bounding the size of the state Here is the design doc'
1,'Remove Project when its projectList is Empty We are using SELECT 1 as a dummy table when the table is used for SQL statements in which a table reference is required but the contents of the table are not important In this case we will see a useless Project whose projectList is empty after executing ColumnPruning rule'
1,'Remove dependency on Twitter4J repository This maven repository is blocked in China We should get rid of that dependency so people in China can compile Spark'
1,'Remove use of special Maven repo for Akka Akka is now published to Maven Central so our documentation and POM files dont need to use the old Akka repo It will be one less step for users to worry about'
1,'Revert HADOOP13534 Remove unused TrashPolicy getInstance and initialize code Per discussion on HADOOP13700 Id like to revert HADOOP13534 It removes a deprecated API but the 2x line does not have a release with the new replacement API This places a burden on downstream applications'
1,'Remove package grouping in genjavadoc In 14 and earlier releases we have package grouping in the generated Java API docs However this disappeared in 150 Rather than fixing it Id suggest removing grouping Because it might take some time to fix and it is a manual process to update the grouping in SparkBuildscala No one complained about missing groups since 150'
1,'remove numpy from RDDSampler of PySpark There is no much performance gain from numpy numpy is not a dependent of pyspark so it maybe introduce some problem such as there is no numpy installed in slaves but only installed master as reported in xxxx It also complicate the code a lot so we may should remove numpy from RDDSampler'
1,'Stop bundling HTML source code in javadoc JARs We generate source code with line numbers for inclusion in the javadoc JARs Given that theres github and other online viewers this doesnt seem so useful these days Disabling the linkSource option saves us 40MB for the hadoopcommon javadoc jar'
1,'Remove snapshot version of SDK dependency from Azure Data Lake Store File System Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency This JIRA removes the SDK snapshot dependency to released SDK candidate There is not functional change in the SDK and no impact to live contract test'
1,'Remove FileFormatprepareRead Interface method FileFormatprepareRead was added in PR 12088 to handle a special case in the LibSVM data source However the semantics of this interface method isnt intuitive it returns a modified version of the data source options map Considering that the LibSVM case can be easily handled using schema metadata inside inferSchema we can remove this interface method to keep the FileFormat interface clean'
1,'Ignore x and response headers when copying an Amazon S3 object The EMC ViPR ECS object storage platform uses proprietary headers starting by xemc like Amazon does with xamz Headers starting by xemc should be included in the signature computation but its not done by the Amazon S3 Java SDK its done by the EMC S3 SDK When s3a copy an object it copies all the headers but when the object includes xemc headers it generates a signature mismatch Removing the xemc headers from the copy would allow s3a to be compatible with the EMC ViPR ECS object storage platform Removing the x which arent xamz headers from the copy would allow s3a to be compatible with any object storage platform which is using proprietary headers'
1,'Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page There are known complaints cribs about History Servers Application List not updating quickly enough when the event log files that need replay are huge Currently the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing refer the method mergeApplicationListing fileStatus FileStatus The process of replay involves each line in the event log being read as a string parsing the string to a Json structure converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive Tests show that majority of time spent in replay is in doing this work When the replay is performed for building the application listing the only two events that the code really cares for are SparkListenerApplicationStart and SparkListenerApplicationEnd since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener This means that when processing an event log file with a huge number hundreds of thousands can be more of events the work done to deserialize all of these event and then replay them is not needed Only two events are what were interested in and this can be used to ensure that when replay is performed for the purpose of building the application list we only make the effort to replay these two events and not others My tests show that this drastically improves application list load time For a 150MB event log from a user with over 100000 events the load time local on my mac comes down from about 16 secs to under 1 second using this approach For customers that typically execute applications with large event logs and thus have multiple large event logs present this can speed up how soon the history server UI lists the apps considerably I will be updating a pull request with take at fixing this'
1,'Kill multiple executors together to reduce lock contention To regulate pending and running executors we determine the executors which are eligible to kill and kill them iteratively rather than a loop This does an RPC call and is synchronized leading to lock contention for SparkListenerBus Side effect listener bus is blocked while we iteratively remove executors'
1,'Remove OpenHashSet for the old aggregate'
1,'Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown As we upgrade to Parquet 182 which includes the fix for the pushdown of optional columns we dont need this metadata now'
1,'Removed calling size length in while condition to avoid extra JVM call'
1,'Deprecate and later remove YARN alpha support Spark 12 Deprecate YARNalpha Spark 13 Remove YARNalpha ie require YARNstable'
1,'Remove InsertIntoHiveTable From Logical Plan LogicalPlan InsertIntoHiveTable is useless Thus we can remove it from the code base'
1,'Remove individual commit messages from the squash commit message I took a look at the commit messages in git log it looks like the individual commit messages are not that useful to include but do make the commit messages more verbose They are usually just a bunch of extremely concise descriptions of bug fixes merges etc'
1,'Eliminate unnecessary submitStage call Currently a method submitStage for waiting stages is called on every iteration of the event loop in DAGScheduler to submit all waiting stages but most of them are not necessary because they are not related to Stage status The case we should try to submit waiting stages is only when their parent stages are successfully completed This elimination can improve DAGScheduler performance'
1,'Allow singlecategory features for GBT models There is a remaining issue with using singlecategory features for GBTRegressor and GBTClassifier They technically already work but they include a validity check which is too strict This is to remove that check'
1,'remove remaining references to cygwin cygpath from scripts branchtrunkwin still contains a few references to Cygwin and the cygpath command that need to be removed now that they are no longer needed'
1,'Removal of HiveSharedState Since HiveClient is used to interact with the Hive metastore it should be hidden in HiveExternalCatalog After moving HiveClient into HiveExternalCatalog HiveSharedState becomes a wrapper of HiveExternalCatalog Thus removal of HiveSharedState becomes straightforward After removal of HiveSharedState the reflection logic is directly applied on the choice of ExternalCatalog types based on the configuration of CATALOGIMPLEMENTATION HiveClient is also used invoked by the other entities besides HiveExternalCatalog we defines the following two APIs'
1,'Remove Bitset in BytesToBytesMap Since we have 4 bytes as number of records in the beginning of a page then the address can not be zero so we do not need the bitset'
1,'Remove SqlNewHadoopRDDs generated Tuple2 and InterruptibleIterator A small performance optimization we dont need to generate a Tuple2 and then immediately discard the key We also dont need an extra wrapper'
1,'Remove Alias from MetastoreRelation and SimpleCatalogRelation Different from the other leaf nodes MetastoreRelation and SimpleCatalogRelation have a predefined alias which is used to change the qualifier of the node However based on the existing alias handling alias should be put in SubqueryAlias This PR is to separate alias handling from MetastoreRelation and SimpleCatalogRelation to make it consistent with the other nodes For example below is an example query for MetastoreRelation which is converted to LogicalRelation For SimpleCatalogRelation the existing code always generates two Subqueries Thus no change is needed'
1,'Remove UnsafeRowConverter in favor of UnsafeProjection'
1,'Whitelist the list of Hive fallback functions and remove blind fallback This patch removes the blind fallback into Hive for functions Instead it creates a whitelist and adds only a small number of functions to the whitelist ie the ones we intend to support in the long run in Spark'
1,'Remove the deprecated Syncablesync method The Syncablesync was deprecated in 021 We should remove it'
1,'Remove bin rcc script When oahrecord was moved bin rcc was never updated to pull those classes from the streaming jar'
1,'Cleanup Extend the Vectorized Parquet Reader Per ~nonglis suggestions We should do these things Remove the nonvectorized parquet reader code Support the remaining types just big decimals Move the logic to determine if our parquet reader can be used to planning Only complex types should fall back to the parquetmr reader'
1,'Remove hardcoded absolute path for ls Shelljava has a hardcoded path to bin ls which is not correct on all platforms eg not on NixOS'
1,'remove the createCode and createStructCode and replace the usage of them by createStructCode'
1,'Eliminate Unnecessary Window If the Window does not have any window expression it is useless It might happen after column pruning'
1,'remove MaxOf and MinOf'
1,'Remove LazyFileRegion LazyFileRegion was created so we didnt create a file descriptor before having to send the file The change has been pushed back into Netty to support the same things under the DefaultFileRegion It looks like that went into 4025Final I believe at the time we created LazyFileRegion we were on 4023Final and we are now using 4029Final so we should be able to use the netty  directly'
1,'Remove SQLContextcatalog internal method Our code can go through SessionStatecatalog This brings two small benefits Reduces internal dependency on SQLContext Removes another public method in Java Java does not obey package private visibility More importantly according to the design in SPARK13485 wed need to claim this catalog function for the userfacing public functions rather than having an internal field'
1,'Remove SortPartitions and RedistributeData SortPartitions and RedistributeData logical operators are not actually used and can be removed Note that we do have a Sort operator with global flag false that subsumed SortPartitions'
1,'Remove DAGSchedulerrunLocallyWithinThread and sparklocalExecutionenabled Spark has an option called sparklocalExecutionenabled according to the docs Enables Spark to run certain jobs such as first or take on the driver without sending tasks to the cluster This can make certain jobs execute very quickly but may require shipping a whole partition of data to the driver This feature ends up adding quite a bit of complexity to DAGScheduler especially in the runLocallyWithinThread method but as far as I know nobody uses this feature I searched the mailing list and havent seen any recent mentions of the configuration nor stacktraces including the runLocally method As a step towards scheduler complexity reduction I propose that we remove this feature and all code related to it for Spark 15'
1,'remove workaround to pickle array of float for Pyrolite'
1,'Remove some uses of obsolete guava APIs from the hadoop codebase Along the same vein as HADOOP11286 there are now several remaining usages of guava APIs that are now incompatible with a more recent version This JIRA proposes eliminating those usages With this the hadoop code base should run compile cleanly even if guava 16 is used for example'
1,'Remove InternalRows inheritance from Row It is a big change but it lets us use the type information to prevent accidentally passing internal types to external types'
1,'Remove unused TaskMetricsUIDataupdatedBlockStatuses field The TaskMetricsUIDataupdatedBlockStatuses field is assigned to but never read increasing the memory consumption of the web UI We should remove this field'
1,'Simplify whole stage codegen interface remove consumeChild always create code for UnsafeRow and variables'
1,'Remove hashCode and euqals in ArrayBasedMapData We need hashCode and euqals in UnsafeMapData because of the behaivour of UnsafeMapData is different from that of ArrayBasedMapData'
1,'Remove references to preferredNodeLocalityData in javadoc and print warning when used We should remove any references to that feature and print a warning when it is used saying it doesnt work'
1,'Prune Filters based on Constraints Remove all the deterministic conditions in a Filter that are contained in the Child'
1,'Remove hadoopant from hadooptools The hadoopant code is an ancient kludge unlikely to have any users still We can delete it from trunk as a scream test for 3x'
1,'remove GenericInternalRowWithSchema'
1,'Remove HashJoinCompatibilitySuite They dont bring much value since we now have better unit test coverage for hash joins This will also help reduce the test time'
1,'Remove an Extra Distinct in Union Union Distinct has two Distinct that generate two Aggregation in the plan'
1,'Remove IN type coercion from PromoteStrings The removed codes are not reachable because InConversion already resolve the type coercion issues'
1,'Remove noop SortOrder in Sort When SortOrder does not contain any reference it has no effect on the sorting Remove the noop SortOrder in Optimizer'
1,'Remove HiveTypeCoercion trait It is easier to test rules if they are in the companion object'
1,'Remove privatesql and privatespark from catalyst package The catalyst package is meant to be internal and as a result it does not make sense to mark things as privatesql or privatespark It simply makes debugging harder when Spark developers need to inspect the plans at runtime'
1,'Remove preemption from the capacity scheduler code base In an effort to simplify the code base we would like to remove the preemption related code in the capacity scheduler We would reintroduce this possibly with some revisions to the original design after a while This will be an incompatible change'
1,'Remove vestigal templates directories creation the share hadoop component template directories are from when RPM and such were built as part of the build system that no longer happens and now those files cause more harm than good since they are in the classpath lets remove them'
1,'Remove DeveloperApi annotation from private classes For a variety of reasons we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi'
1,'Drop VD type parameter from EdgeRDD Due to vertex attribute caching EdgeRDD previously took two type parameters ED and VD However this is an implementation detail that should not be exposed in the interface so this PR drops the VD type parameter This requires removing the filter method from the EdgeRDD interface because it depends on vertex attribute caching'
1,'Remove some internal classes dependency on SQLContext In general it is better for internal classes to not depend on the external  in this case SQLContext to reduce coupling between userfacing APIs and the internal implementations'
1,'Remove sparkdeploymesosrecoveryMode and use sparkdeployrecoveryMode Remove sparkdeploymesosrecoveryMode and use sparkdeployrecoveryMode configuration for cluster mode'
1,'remove catalog table type INDEX'
1,'Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter'
1,'Introduce a mechanism to ban creating new root SQLContexts in a JVM For some use cases it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts At here root SQLContext means the first SQLContext that gets created'
1,'Remove DistCpV1 and Logalyzer distcpv1 is pretty much unsupported we should just remove it'
1,'Remove physical Distinct operator in favor of Aggregate We can just rewrite distinct using groupby ie aggregate operator'
1,'Remove distcp dependency on FileStatus serialization DistCp uses an internal struct CopyListingFileStatus to record metadata Because this record extends FileStatus it also relies on the Writable contract from that type Because DistCp performs its checks on a subset of the fields ie does not actually rely on FileStatus as a supertype these types should be independent'
1,'Remove comparable requirement from Offset For some sources it can be hard to define a strict ordering that is based only on the data in the offset Since we dont really utilize this comparison lets remove it'
1,'Remove HiveClient and setCurrentDatabase from HiveSessionCatalog This is the first step to clean HiveClient from HiveSessionState In the metastore interaction we always set fully qualified names when accessing operating a table That means we always specify the database Thus it is not necessary to use HiveClient to change the active database in Hive metastore In HiveSessionCatalog setCurrentDatabase is the only function that uses HiveClient Thus we can remove it after removing setCurrentDatabase'
1,'Remove legacy SCPbased Jenkins log archiving code As of SPARK7568 we no longer need to use our custom SCPbased mechanism for archiving Jenkins logs on the master machine this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them We should remove the legacy log syncing code since this is a blocker to disabling Worker Master SSH on Jenkins'
1,'PySpark core test should not use SerDe from PythonMLLibAPI Currently PySpark core test uses the SerDe from PythonMLLibAPI which includes many MLlib things It should use SerDeUtil instead'
1,'Remove hardcoded absolute path for shell executable Shelljava has a hardcoded path to bin bash which is not correct on all platforms'
1,'Speed up SQL query performance by removing redundant executePlan call in Dataset Currently there are a few reports about Spark 20 query performance regression for large queries This issue speeds up SQL query processing performance by removing redundant consecutive executePlan call in DatasetofRows function and Dataset instantiation Specifically this issue aims to reduce the overhead of SQL query execution plan generation not real query execution So we can not see the result in the Spark Web UI Please use the following query script'
1,'Remove sparksqlnativeView and sparksqlnativeViewcanonical config When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider'
1,'Update sbin commands and documentation to use new slaves option We need to remove usages of yarndaemonssh and hadoopdaemonssh from the start and stop scripts converting them to use the new slaves option'
1,'Remove sparkdeploymesoszookeeperdir and use sparkdeployzookeeperdir Remove sparkdeploymesoszookeeperdir and use existing configuration sparkdeployzookeeperdir for Mesos cluster mode'
1,'ParamssetDefault should not keep varargs annotation and should be removed See SPARK7498 We added varargs again Though it is technically correct it often requires that developers do clean assembly rather than not clean assembly which is a nuisance during development This JIRA will remove it for now pending a fix to the Scala compiler'
1,'Remove ExtractValueWithOrdinal abstract  It is unnecessary and makes the type hierarchy slightly more complicated than needed'
1,'Remove setCheckpointDir from LDA and tree Strategy CheckpoingDir is a global Spark configuration which should not be altered by an ML algorithm We could check whether checkpointDir is set if checkpointInterval is positive'
1,'Removal of useless CreateHiveTableAsSelectLogicalPlan CreateHiveTableAsSelectLogicalPlan is a dead code after refactoring'
1,'Remove Direct Usage of HiveClient in InsertIntoHiveTable This is another step to get rid of HiveClient from HiveSessionState All the metastore interactions should be through ExternalCatalog interface However the existing implementation of InsertIntoHiveTable still requires Hive clients Thus we can remove HiveClient by moving the metastore interactions into ExternalCatalog'
1,'Optimizer should remove unnecessary distincts in multiple unions Only one distinct should be necessary This makes a bunch of unions slower than a bunch of union alls followed by a distinct'
1,'Remove all of the CHANGEStxt files With the commit of HADOOP11731 the CHANGEStxt files are now EOLed We should remove them'
1,'Remove dependency on Twitter4J repository This maven repository is blocked in China We should get rid of that dependency so people in China can compile Spark'
1,'Remove import scalaconcurrentExecutionContextImplicitsglobal Learnt a lesson from SPARK7655 Spark should avoid to use scalaconcurrentExecutionContextImplicitsglobal because the user may submit blocking actions to scalaconcurrentExecutionContextImplicitsglobal and exhaust all threads in it This could crash Spark So Spark should always use its own thread pools for safety'
1,'Remove getaclstatus call for nonacl commands in getfacl Remove getaclstatus call for nonacl commands in getfacl'
1,'Remove TypeCheck in debug package TypeCheck no longer applies in the new Tungsten world'
1,'Remove unused codes in subexpressionEliminationForWholeStageCodegen Some codes in subexpressionEliminationForWholeStageCodegen are never used actually Remove them using this jira'
1,'Remove DescribeCommands dependency on LogicalPlan DescribeCommand should just take a TableIdentifier and ask the metadata catalog for tables information'
1,'Remove orgapachesparksqlexecutionlocal We introduced some local operators in orgapachesparksqlexecutionlocal package but never fully wired the engine to actually use these We still plan to implement a full local mode but its probably going to be fairly different from what the current iteratorbased local mode would look like Lets just remove them for now and we can always reintroduced them in the future by looking at branch16'
1,'remove the supportsPartial flag in AggregateFunction'
1,'Remove LeafNode UnaryNode BinaryNode from TreeNode They are not very useful and cause problems with toString due to the order they are mixed in'
1,'Remove duplicated SQL metrics For lots of SQL operators we have metrics for both of input and output the number of input rows should be exactly the number of output rows of child we could only have metrics for output rows After we improve the performance using whole stage codegen the overhead of SQL metrics are not trivial anymore we should avoid that if its not necessary Some of the operator does not have SQL metrics we should add that for them For those operators that have the same number of rows from input and output for example Projection we may dont need that'
1,'Remove typeId in columnar cache typeId is not needed in columnar cache its confusing to having them'
1,'Removed diffSum which is theoretical zero in LinearRegression and coding formating'
1,'Remove returnValues from BlockStore APIs In preparation for larger refactorings I think that we should remove the confusing returnValues option from the BlockStore put APIs returning the value is only useful in one place caching and in other situations such as block replication its simpler to put and then get'
1,'Remove compatibleWith meetsRequirements and needsAnySort checks from Exchange While reviewing ~yhuais patch for SPARK2205 I noticed that Exchanges compatible check may be incorrectly returning false in many cases As far as I know this is not actually a problem because the compatible meetsRequirements and needsAnySort checks are serving only as shortcircuit performance optimizations that are not necessary for correctness In order to reduce code complexity I think that we should remove these checks and unconditionally rewrite the operators children This should be safe because we rewrite the tree in a single bottomup pass'
1,'Remove submitJobThreadPool since submitJob doesnt create a separate thread to wait for the job result Before 9264 submitJob would create a separate thread to wait for the job result submitJobThreadPool was a workaround in ReceiverTracker to run these waitingjobresult threads Now 9264 has been merged to master and resolved this blocking issue submitJobThreadPool can be removed now'
1,'remove JDK7 from Dockerfile We should slim down the Docker image by removing JDK7 now that trunk no longer supports it'
1,'Removing unnecessary self types in Catalyst'
1,'Remove all extra JoinedRows They were added to improve performance so JIT can inline the JoinedRow calls However we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators'
1,'remove trait Queryable'
1,'Remove MRv1 terms from HttpAuthenticationmd We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker'
1,'Remove layers of abstraction in YARN code no longer needed after dropping yarnalpha For example YarnRMClient and YarnRMClientImpl can be merged YarnAllocator and YarnAllocationHandler can be merged'
1,'Remove projectList from Windows projectList is useless Remove it from the  Window It simplifies the codes in Analyzer and Optimizer'
1,'Remove the internal implicit conversion from LogicalPlan to DataFrame DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame This has been fairly confusing to a few new contributors Since it doesnt buy us much we should just remove that implicit conversion'
1,'Display a better message for not finding classes removed in Spark 20 We removed some classes in Spark 20 If the user uses an incompatible library he may see ClassNotFoundException Its better to give an instruction to ask people using a correct version'
1,'Remove Term Code type aliases in code generation From my perspective as a code reviewer I find them more confusing than using String directly So we should remove it'
1,'Remove EvaluatedType from SQL Expression Its not a very useful type to use We can just remove it to simplify expressions slightly'
1,'Ban use of JavaConversions and migrate all existing uses to JavaConverters Sparks style checker should ban the use of Scalas JavaConversions which provides implicit conversions between Java and Scala collections types Instead we should be performing these conversions explicitly using JavaConverters or forgoing the conversions altogether if theyre occurring inside of performancecritical code'
1,'Remove InternalRow type alias in expressions package The type alias was there because initially when I moved Row around I didnt want to do massive changes to the expression code But now it should be pretty easy to just remove it One less concept to worry about'
1,'Remove Bagel test suites Bagel has been deprecated and we havent done any changes to it There is no need to run those tests'
1,'Remove implicit conversion from Expression to Column'
1,'remove FromUnsafe and add its codegen version to GenerateSafe In 7752 we added FromUnsafe to convert nexted unsafe data like array map struct to safe versions Its a quick solution and we already have GenerateSafe to do the conversion which is codegened So we should remove FromUnsafe and implement its codegen version in GenerateSafe'
1,'Remove 3 second sleep before starting app on YARN After acquiring allocations from YARN and launching containers Spark currently waits for 3 seconds for executors to connect to the driver On Spark standalone nothing like this happens Im wondering whether we can just remove this sleep entirely Is there a reason Im missing why YARN is different than standalone in this regard At the least we could do something smarter like wait until all executors have registered'
1,'PythonUDF could process UnsafeRow Currently There will be ConvertToSafe for PythonUDF thats not needed actually'
1,'Remove GenerateProjection Based on discussion offline with ~marmbrus we should remove GenerateProjection'
1,'Cleanup DiskChecker interface The DiskChecker  has a few unused public methods We can remove them'
1,'Rename remove nonHADOOP etc from the shell scripts We should make an effort to clean up the shell env var name space by removing unsafe variables See comments for list'
1,'Remove SparkSqlSerializer2 in favor of Unsafe exchange GenerateUnsafeProjection can be used directly as a code generated serializer We no longer need SparkSqlSerializer2'
1,'Remove databaseName from SimpleCatalogRelation Remove useless databaseName from SimpleCatalogRelation'
1,'Removal of TestHiveSharedState Remove TestHiveSharedState Otherwise we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOGIMPLEMENTATION'
1,'Remove ConnectionManager We introduced the Netty network module for shuffle in Spark 12 and has turned it on by default for 3 releases The old ConnectionManager is difficult to maintain Its time to remove it'
1,'Remove LeftSemiJoinBNL Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin it has the same implementation as LeftSemiJoinBNL we should remove that'
1,'Remove GeneratedAggregate It is subsumed by the new aggregate implementation'
1,'remove unneeded commonshttpclient dependencies from POM files in Hadoop and subprojects In branch28 and later the patches for various child and related bugs listed in HADOOP10105 most recently including HADOOP11613 HADOOP12710 HADOOP12711 HADOOP12552 and HDFS10623 eliminate all use of commonshttpclient from Hadoop and its subprojects except for hadooptools hadoopopenstack see HADOOP11614 However after incorporating these patches commonshttpclient is still listed as a dependency in these POM files We wish to remove these but since commonshttpclient is still used in many files in hadooptools hadoopopenstack well need to add the dependency Well add a note to HADOOP11614 to undo this when commonshttpclient is removed from hadoopopenstack In 28 this was mostly done by HADOOP12552 but the version info formerly inherited from hadoopproject pomxml also needs to be added so that is in the branch28 version of the patch Other projects with undeclared transitive dependencies on commonshttpclient previously provided via hadoopcommon or hadoopclient may find this to be an incompatible change Of course that also means such project is exposed to the commonshttpclient CVE and needs to be fixed for that reason as well'
1,'Cleanup options for DataFrame reader API in Python There are some duplicated code for options we should remove them'
1,'Remove dead classes in hadoopstreaming Hadoopstreaming no longer requires many classes in oahrecord This jira removes the dead code'
1,'Remove PrepareRDD Since SPARK10342 is resolved MapPartitionWithPrepare is not needed anymore'
1,'Exclude duplicate jars in hadoop package under different components lib In the hadoop package distribution there are more than 90 of the jars are duplicated in multiple places For Ex almost all jars in share hadoop hdfs lib are already there in share hadoop common lib Same case for all other lib in share directory Anyway for all the daemon processes all directories are added to classpath So to reduce the package distribution size and the classpath overhead remove the duplicate jars from the distribution'
1,'delete spurious master branch Right now the git repo has a branch named master in addition to our trunk branch Since master is the commonplace name of the most recent branch in git repositories this is misleading to new folks It looks like the branch is from ~11 months ago We should remove it'
1,'Remove metrics v1 After HADOOP7266 we should remove metrics v1 from trunk'
1,'Remove FileUtil copyMerge Need to remove FileUtil copyMerge'
1,'SQL Support coalesce and repartition in Dataset APIs repartition Returns a new Dataset that has exactly numPartitions partitions coalesce Returns a new Dataset that has exactly numPartitions partitions Similar to coalesce defined on an RDD this operation results in a narrow dependency eg if you go from 1000 partitions to 100 partitions there will not be a shuffle instead each of the 100 new partitions will claim 10 of the current partitions'
1,'Verification of Functionrelated ExternalCatalog APIs Functionrelated HiveExternalCatalog APIs do not have enough verification logics After the PR HiveExternalCatalog and InMemoryCatalog become consistent in the error handling'
1,'ML 20 QA Scala APIs audit for feature See containing JIRA for details SPARK14811'
1,'Enhance mutate to support replace existing columns mutate in the dplyr package supports adding new columns and replacing existing columns But currently the implementation of mutate in SparkR supports adding new columns only Also make the behavior of mutate more consistent with that in dplyr Throw error message when there are duplicated column names in the DataFrame being mutated When there are duplicated column names in specified columns by arguments the last column of the same name takes effect'
1,'Support window functions in SQLContext'
1,'Expose more executor stats in stable status API Currently the stable status API is quite limited it exposes only a small subset of the things exposed by JobProgressListener It is useful for very high level querying but falls short when the developer wants to build an application on top of Spark with more integration In this issue I propose that we expose at least two things Which executors are running tasks and Which executors cached how much in memory and on disk The goal is not to expose exactly these two things but to expose something that would allow the developer to learn about them These concepts are very much fundamental in Sparks design so theres almost no chance that they will go away in the future'
1,'inputfileblockstart and inputfileblocklength function We currently have function inputfilename to get the path of the input file but dont have functions to get the block start offset and length This patch introduces two functions inputfileblockstart returns the file block start offset or 1 if not available inputfileblocklength returns the file block length or 1 if not available'
1,'Security support for ZK Failover controller To keep the initial patches manageable kerberos security is not currently supported in the ZKFC implementation This JIRA is to support the following important pieces for security integrate with ZK authentication kerberos or passwordbased allow the user to configure ACLs for the relevant znodes add keytab configuration and login to the ZKFC daemons ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons'
1,'Add a new interface for retrieving FS and FC Statistics Currently FileSystemStatistics exposes the following statistics These are inturn exposed as job counters by MapReduce and other frameworks There is logic within DfsClient to map operations to these counters that can be confusing for instance mkdirs counts as a writeOp Proposed enhancement Add a statistic for each DfsClient operation including create append createSymlink delete exists mkdirs rename and expose them as new properties on the Statistics object The operationspecific counters can be used for analyzing the load imposed by a particular job on HDFS For example we can use them to identify jobs that end up creating a large number of files Once this information is available in the Statistics object the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary'
1,'make re2j dependency consistent Make the re2j dependency consistent with other parts of Hadoop Seeing some weird rare failures with older versions of maven that appear to be related to this'
1,'Improve the implementation of collect on DataFrame in SparkR currently in SparkR collect on a DataFrame collects the data within the DataFrame into a local dataframe R users are used to using dataframe However collect currently cant collect data of nested types from a DataFrame because The serializer in JVM backend does not support nested types collect in R side assumes each column is of simple atomic type that can be combinded into a atomic vector'
1,'Avoid the serialization multiple times during unrolling of complex types The serialize will be called by actualSize and append we should use UnsafeProjection before unrolling'
1,'Accept Dataset instead of DataFrame in MLlib APIs In Spark 20 DataFrame is an alias of DatasetRow MLlib API actually works for other types of Dataset so we should accept Dataset instead It maps to Dataset in Java This is a source compatible change'
1,'Build BytesToBytesMap in HashedRelation method Currently for the key that can not fit within a long we build a hash map for UnsafeHashedRelation its converted to BytesToBytesMap after serialization and deserialization We should build a BytesToBytesMap directly to have better memory efficiency'
1,'Improve window function frame boundary API in DataFrame ANSI SQL uses the following to specify the frame boundaries for window functions In Sparks DataFrame API we use integer values to indicate relative position I think using numeric values to indicate relative positions is actually a good idea but the reliance on LongMinValue and LongMaxValue to indicate unbounded ends is pretty confusing The API is not selfevident There is no way for a new user to figure out how to indicate an unbounded frame by looking at just the API The user has to read the doc to figure this out It is weird LongMinValue or LongMaxValue has some special meaning Different languages have different min max values eg in Python we use sysmaxsize and +sysmaxsize To make this API less confusing we have a few options Add the following additional methods'
1,'Optimize metadata only query that has an aggregate whose children are deterministic project or filter operators when query only use metadata example partition key it can return results based on metadata without scanning files'
1,'Improve error messages for RDD API When you have an error in your R code using the RDD API you always get an error message This is not very useful and I think it might be better to catch the R exception and show it instead'
1,'RPC Metrics Add the ability track and log slow RPCs This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs Slow RPCs are RPCs that fall at 99th percentile This is useful to troubleshoot why certain services like name node freezes under heavy load'
1,'Support external calls in the RPC call queue Leveraging HADOOP13465 will allow nonrpc calls to be added to the call queue This is intended to support routing webhdfs calls through the call queue to provide a unified and protocolindependent QoS'
1,'Add functionality in spark history sever API to query applications by end time Currently spark history server REST API provides functionality to query applications by application start time range based on minDate and maxDate query parameters but it lacks support to query applications by their end time In this Jira we are proposing optional minEndDate and maxEndDate query parameters and filtering capability based on these parameters to spark history server REST API This functionality can be used for following queries For backward compatibility we can keep existing minDate and maxDate query parameters as they are and they can continue support filtering based on start time range'
1,'Support UnsafeRow in Coalesce Except Intersect functions'
1,'SortMergeJoin and BroadcastHashJoin functions should support condition Right now We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions the result projection of join could be very expensive if they generate lots of rows could be reduce mostly by condition'
1,'Significant amount of CPU is being consumed in SnappyNative arrayCopy method While running a Spark job which is spilling a lot of data in reduce phase we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method Please see the stack trace below The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data which is expensive We should fix Snappy java to use with nonJNI based SystemarrayCopy method in this case'
1,'Add support for launching multiple Mesos dispatchers Currently the sbin start|stopmesosdispatcher scripts only assume there is one mesos dispatcher launched but potentially users that like to run multitenant dispatcher might want to launch multiples It also helps local development to have the ability to launch multiple ones'
1,'Add Python API for PrefixSpan Add Python API for mllibfpmPrefixSpan'
1,'hadoopdaemonssh should just call hdfs directly There is littletono reason for it to call hadoopdaemonsh anymore'
1,'Make Logistic Linear Regression Model evaluate method public The following method in LogisticRegressionModel is marked as private which prevents users from creating a summary on any given data set This method is definitely necessary to test model performance By the way the name evaluate is already pretty good for me'
1,'Flag to close Write Ahead Log after writing Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made S3 does not support flushing of data data is written once the stream is actually closed In case of failure the data for the last minute default rolling interval will not be properly written Therefore we need a flag to close the stream after the write so that we achieve read after write consistency'
1,'Capacity Scheduler needs to reread its configuration An external application an Ops script or some CLIbased tool can change the configuration of the Capacity Scheduler change the capacities of various queues for example by updating its config file This application then needs to tell the Capacity Scheduler that its config has changed which causes the Scheduler to reread its configuration Its possible that the Capacity Scheduler may need to interact with external applications in other similar ways'
1,'Azure Add a new SAS key mode for WASB Current implementation of WASB only supports Azure storage keys and SAS key being provided via orgapachehadoopconfConfiguration which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers Added to the fact that WASB does not inherently support ACLs WASB is its current implementation cannot be securely used for environments like secure hadoop cluster This JIRA is created to add a new mode in WASB which operates on Azure Storage SAS keys which can provide fine grained timed access to containers and blobs providing a segway into supporting WASB for secure hadoop cluster More details about the issue and the proposal are provided in the design proposal document'
1,'Improve performance of JSON schema inferences inferField step JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up including eliminating unnecessary sorting and the use of inefficient collections'
1,'ViewFileSystem should support snapshot methods Currently ViewFileSystem does not dispatch snapshot methods through the mount table All snapshot methods throw UnsupportedOperationException even though the underlying mount points could be HDFS instances that support snapshots We need to update ViewFileSystem to implement the snapshot methods'
1,'Python API for bisecting kmeans Implement Python API for bisecting kmeans'
1,'Check the lowerBound and upperBound whether equal None in jdbc API When we use the jdbc in pyspark if we check the lowerBound and upperBound we can give a more friendly suggestion'
1,'YarnShuffleService should use YARN getRecoveryPath for leveldb location The YarnShuffleService currently just picks a directly in the yarn local dirs to store the leveldb file YARN added an interface in hadoop 25 getRecoverPath to get the location where it should be storing this We should change to use getRecoveryPath This does mean we will have to use reflection or similar to check for its existence though since it doesnt exist before hadoop 25'
1,'Predicate Push Down Through Window Operator Push down the predicate through the Window operator In this JIRA predicates are pushed through Window if and only if the following conditions are satisfied Predicate involves one and only one column that is part of window partitioning key Window partitioning key is just a sequence of attributeReferences ie none of them is an expression Predicate must be deterministic'
1,'ObjectFile on top of TFile Problem We need to have Object Serialization Deserialization support for TFile'
1,'fsck to show checksum corrupted files Currently only way to find files with all replica being corrupt is when we read those files Instead can we have fsck report those Using the corrupted blocks found by the periodic verification'
1,'Add computeCost method to KMeansModel in sparkml We should add a method analogous to sparkmllibclusteringKMeansModelcomputeCost to sparkmlclusteringKMeansModel This will be a temp fix until we have proper evaluators defined for clustering'
1,'Invoke task failure callbacks before calling outputstreamclose We need to submit another PR against Spark to call the task failure callbacks before Spark calls the close function on various output streams For example we need to intercept an exception and call TaskContextmarkTaskFailed before calling close functions Changes to Spark should include unit tests to make sure this always work in the future'
1,'Add A and a formats for fs stat command to print permissions This patch adds to fs shell Statjava the missing options of a and A FileStatus already contains the getPermission method required for returning symbolic permissions FsPermission contains the method to return the binary short but nothing to present in standard Octal format Most UNIX admins base their work on such standard octal permissions Hence this patch also introduces one tiny method to translate the toShort return into octal Build has already passed unit tests and javadoc'
1,'Add utilities to load chukwa sequence file to database When data need to be reprocessed in the database there is currently no manual method to reload the chukwa sequence files into database A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this'
1,'Dataframedrop supported multicolumns in spark api and should make python api also support it Dataframedrop supported multicolumns in spark api and should make python api also support it'
1,'Add safely flag to rm to prompt when deleting many files We have seen many cases with customers deleting data inadvertently with skipTrash The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though skipTrash is being used'
1,'collectlist and collectset should accept struct types as argument Hive already supports this Currently Sparksql still supports only primitive types'
1,'Support formula in sparkkmeans in SparkR In SparkR sparkkmeans take a DataFrame with double columns This is different from other ML methods we implemented which support R model formula We should add support for that as well'
1,'Allow handoff on the server side for RPC requests An RPC server handler thread is tied up for each incoming RPC request This isnt ideal since this essentially implies that RPC operations should be short lived and most operations which could take time end up falling back to a polling mechanism Some use cases where this is useful YARN submitApplication which currently submits followed by a poll to check if the application is accepted while the submit operation is written out to storage This can be collapsed into a single call YARN allocate requests and allocations use the same protocol New allocations are received via polling The allocate protocol could be split into a request heartbeat along with a awaitResponse The request heartbeat is sent only when theres a request or on a much longer heartbeat interval awaitResponse is always left active with the RM and returns the moment something is available MapReduce Tez task to AM communication is another example of this pattern The same pattern of splitting calls can be used for other protocols as well This should serve to improve latency as well as reduce network traffic since the keepalive heartbeat can be sent less frequently I believe theres some cases in HDFS as well where the DN gets told to perform some operations when they heartbeat into the NN'
1,'Support timeouts in LDAP queries in LdapGroupsMapping LdapGroupsMapping currently does not set timeouts on the LDAP queries This can create a risk of a very long infinite wait on a connection'
1,'Optimize sequential projections In ML pipelines each transformer estimator appends new columns to the input DataFrame For example it might produce DataFrames like the following columns a b c d where a is from raw input b udfb a c udfc b and d udfd c Some UDFs could be expensive However if we materialize c and d udfb and udfc are triggered twice ie value c is not reused It would be nice to detect this pattern and reuse intermediate values'
1,'WritableComparator must implement noarg constructor Comparators should be serializable To make deserialization work it is required that all superclasses have noarg constructor Simply add noarg constructor to WritableComparator'
1,'Should add version to the serialization of DelegationToken Now that we are adding the serialized form of delegation tokens into the http interfaces we should include some version information'
1,'Expose NumOpenConnectionsPerUser as a metric To track user level connections How many connections for each user in busy cluster where so many connections to server'
1,'Add support for nested groups in LdapGroupsMapping When using LdapGroupsMapping with Hadoop nested groups are not supported So for example if user jdoe is part of group A which is a member of group B the group mapping currently returns only group A Currently this facility is available with ShellBasedUnixGroupsMapping and SSSD or similar tools but would be good to have this feature as part of LdapGroupsMapping directly'
1,'Support partition batch pruning with EqualNullSafe predicate in InMemoryTableScanExec It seems EqualNullSafe filter was missed for batch pruneing partitions in cached tables Supporting this improve the performance roughly ~75 it will vary'
1,'Support perserver IPC configuration Currently different IPC servers in Hadoop use the same config variables names starting with ipcserver This makes it difficult and confusing to maintain configuration for different IPC servers'
1,'Add an HDFS metrics sink We need a metrics2 sink that can write metrics to HDFS The sink should accept as configuration a directory prefix and do the following in putMetrics Get yyyyMMddHH from current timestamp If HDFS dir dir prefix + yyyyMMddHH doesnt exist create it Close any currently open file and create a new file called hostname log in the new directory Write metrics to the current log file'
1,'Push Down Filter Through BatchEvalPython Currently when users use Python UDF in Filter BatchEvalPython is always generated below FilterExec However not all the predicates need to be evaluated after Python UDF execution Thus we can push down the predicates through BatchEvalPython'
1,'In DistCp prevent unnecessary getFileStatus call when not preserving metadata After DistCp copies a file it calls getFileStatus to get the FileStatus from the destination so that it can compare to the source and update metadata if necessary If the DistCp command was run without the option to preserve metadata attributes then this additional getFileStatus call is wasteful'
1,'Implement getLinkTarget for ViewFileSystem ViewFileSystem doesnt override FileSystem getLinkTarget So when view filesystem is used to resolve the symbolic links the default FileSystem implementation throws UnsupportedOperationException The proposal is to define getLinkTarget for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links Path thus returned is preferred to be a viewfs qualified path so that it can be used again on the ViewFileSystem handle'
1,'Support streaming data using network library As part of the work to implement SPARK11140 it would be nice to have the network library efficiently stream data over a connection Currently all it has is the shuffle data protocol which is not very efficient for large files it requires the whole file to be buffered on the receiver side before the receiver can do anything For large files that comes at a huge cost in memory You can chunk large files but that requires the client to ask for each chunk separately Instead a similar approach but allowing the data to be processed as it arrives would be a lot more efficient and make it easier to implement the file server in the referenced bug'
1,'Allow custom timing control in microbenchmarks The current benchmark framework runs a code block for several iterations and reports statistics However there is no way to exclude periteration setup time from the overall results'
1,'Access Control support for Nonsecure deployment of Hadoop on Windows'
1,'Survival analysis in SparkR Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis'
1,'Add ifelse Column function to SparkR Add a column function on a DataFrame like ifelse in R to SparkR I guess we could implement it with a combination with when and otherwise'
1,'LinearRegression should supported weighted data In many modeling application data points are not necessarily sampled with equal probabilities Linear regression should support weighting which account the over or under sampling'
1,'DataFrame UDFs in R This depends on some internal interface of Spark SQL should be done after merging into Spark'
1,'Improve SparkStatusTracker function to also track executor information'
1,'Make setting up the build environment easier As discussed with ~aw In AVRO1537 a docker based solution was created to setup all the tools for doing a full build This enables much easier reproduction of any issues and getting up and running for new developers This issue is to copy port that setup into the hadoop project in preparation for the bug squash'
1,'AFTSurvivalRegression should support feature standardization This bug is reported by Stuti Awasthi The lossSum has possibility of infinity because we do not standardize the feature before fitting model we should support feature standardization Another benefit is that standardization will improve the convergence rate'
1,'Support UnsafeRow in MapPartitions MapGroups CoGroup functions'
1,'Support UnsafeRow in LocalTableScan function'
1,'JSON serialization of Vectors We want to support JSON serialization of vectors in order to support SPARK11764'
1,'Improve delegation token handling in secure clusters In a way Id consider this a parent bug of SPARK7252 Sparks current support for delegation tokens is a little all over the place for HDFS theres support for recreating tokens if a principal and keytab are provided for HBase and Hive Spark will fetch delegation tokens so that apps can work in cluster mode but will not recreate them so apps that need those will stop working after 7 days for anything else Spark doesnt do anything Lots of other services use delegation tokens and supporting them as data sources in Spark becomes more complicated because of that eg Kafka will hopefully soon support them It would be nice if Spark had consistent support for handling delegation tokens regardless of who needs them Id list these as the requirements Spark to provide a generic interface for fetching delegation tokens This would allow Sparks delegation token support to be extended using some plugin architecture eg Java services meaning Spark itself doesnt need to support every possible service out there This would be used to fetch tokens when launching apps in cluster mode and when a principal and a keytab are provided to Spark A way to manually update delegation tokens in Spark For example a new SparkContext API or some configuration that tells Spark to monitor a file for changes and load tokens from said file This would allow external applications to manage tokens outside of Spark and be able to update a running Spark application think for example a job sever like Oozie or something like HiveonSpark which manages Spark apps running remotely A way to notify running code that new delegation tokens have been loaded This may not be strictly necessary it might be possible for code to detect that eg by peeking into the UserGroupInformation structure But an event sent to the listener bus would allow applications to react when new tokens are available eg the Hive backend could recreate connections to the metastore server using the new tokens'
1,'Add JavaStreamingListener functionality Add Java friendly API for StreamingListener'
1,'add gapplyCollect for SparkDataFrame Add a new API method called gapplyCollect for SparkDataFrame It does gapply on a SparkDataFrame and collect the result back to R Compared to gapply + collect gapplyCollect offers performance optimization as well as programming convenience as no schema is needed to be provided This is similar to dapplyCollect'
1,'Open up SparkILoopgetAddedJars SparkILoopgetAddedJars is a useful method to use so we can programmatically get the list of jars added'
1,'Support multiple authentication schemes via AuthenticationFilter The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter HADOOP9054 added a support to plugin custom authentication scheme in addition to Kerberos via AltKerberosAuthenticationHandler  But it is based on selecting the authentication mechanism based on UserAgent HTTP header which does not conform to HTTP protocol semantics HTTP protocol provides a simple challengeresponse authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information This mechanism is initiated by server sending the 401 Authenticate response with WWWAuthenticate header which includes at least one challenge that indicates the authentication scheme s and parameters applicable to the RequestURI In case server supports multiple authentication schemes it may return multiple challenges with a 401 Authenticate response and each challenge may use a different authscheme A user agent MUST choose to use the strongest authscheme it understands and request credentials from the user based upon that challenge The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses Negotiate as the challenge as part of WWWAuthenticate response header As per the following documentation Negotiate challenge scheme is only applicable to Kerberos and Windows NTLM authentication schemes On the other hand for LDAP authentication typically Basic authentication scheme is used Note TLS is mandatory with Basic authentication scheme Hence for this feature the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes Kerberos via Negotiate auth challenge and LDAP via Basic auth challenge During the authentication phase it would send both the challenges and let client pick the appropriate one If client responds with an Authorization header tagged with Negotiate it will use Kerberos authentication If client responds with an Authorization header tagged with Basic it will use LDAP authentication Note some HTTP clients eg curl or Apache Http Java client need to be configured to use one scheme over the other eg curl tool supports option to use either Kerberos via negotiate flag or username password based authentication via basic and u flags Apache HttpClient library can be configured to use specific authentication scheme Typically web browsers automatically choose an authentication scheme based on a notion of strength of security eg take a look at the design of Chrome browser for HTTP authentication'
1,'Support to specify join type when calling join with usingColumns Currently the method join right DataFrame usingColumns SeqString only supports inner join It is more convenient to have it support other join types'
1,'Use XInclude in hadoopazure test configuration to isolate Azure Storage account keys for service integration tests The hadoopazure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account The configuration works by overwriting the src test resources azuretestxml file This can be an errorprone process The azuretestxml file is checked into revision control to show an example There is a risk that the tester could overwrite azuretestxml containing the keys and then accidentally commit the keys to revision control This would leak the keys to the world for potential use by an attacker This issue proposes to use XInclude to isolate the keys into a separate file ignored by git which will never be committed to revision control This is very similar to the setup already used by hadoopaws for integration testing'
1,'RollingFileSystemSink should eagerly rotate directories The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in The issue is that HDFS does not update the file size until its closed HDFS5478 and if no new metrics record comes in then the file size will never be updated This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour'
1,'PySpark mlevaluation should support save load functions Since mlevaluation has supported save load at Scala side supporting it at Python side is very straightforward and easy'
1,'Enable AzureStorage Client Side logging AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative AzureStorage SDK supports client side logging that can be enabled that logs relevant information wrt request made from the Storage client This JIRA is created to enable Azure Storage Client Side logging at the Job submission level User should be able to configure Client Side logging on a Per Job bases'
1,'Set Streaming MaxRate Independently For Multiple Streams We use multiple DStreams coming from different Kafka topics in a Streaming application Some settings like maxrate and backpressure enabled disabled would be better passed as config to KafkaUtilscreateStream and KafkaUtilscreateDirectStream instead of setting them in SparkConf Being able to set a different maxrate for different streams is an important requirement for us we currently workaround the problem by using one receiverbased stream and one direct stream We would like to be able to turn on backpressure for only one of the streams as well'
1,'Port StreamInputFormat to new Map Reduce API As of now hadoop streaming uses old Hadoop M R API This JIRA ports it to the new M R API'
1,'Add support for offheap memory to MemoryManager In order to lay the groundwork for proper offheap memory support in SQL Tungsten we need to extend our MemoryManager to perform bookkeeping for offheap memory'
1,'Use Spark BitSet in BroadcastNestedLoopJoin method We use scalacollectionmutableBitSet in BroadcastNestedLoopJoin method now We should use Sparks BitSet'
1,'the metrics system in the job tracker is running too often The metrics system in the JobTracker is defaulting to every 5 seconds computing all of the counters for all of the jobs This work is a substantial amount of work showing up as running in 20 of the snapshots that Ive seen Id like to lower the default interval to once every 60 seconds and make it a low priority thread'
1,'Disable hiding field style checks in  setters'
1,'Add Java API for trackStateByKey'
1,'Python API for text data source We should add text to DataFrameReader and DataFrameWriter'
1,'Support UnsafeRow in all SparkPlan if possible There are still some SparkPlan does not support UnsafeRow or does not support well'
1,'ServiceLevelAuth still references hadoop dfsadmin mradmin This should be hdfs dfsadmin and yarn rmadmin'
1,'AggregateFunction should not ImplicitCastInputTypes AggregateFunction currently implements ImplicitCastInputTypes which enables implicit input type casting This can lead to unexpected results and should only be enabled when it is suitable for the function at hand'
1,'When wrapping catalyst datatype to Hive data type avoid pattern matching Profiling a job we saw that patten matching in wrap function of HiveInspector is consuming around 10 of the time which can be avoided A similar change in the unwrap function was made in SPARK15956'
1,'FileSystem should have mkdir and create file apis which do not create parent path FileSystem should have mkdir and create file apis which do not create parent path'
1,'Design ServerCall to be extensible for unified call queue The RPC layer supports QoS but other protocols ex webhdfs are completely unconstrained Generalizing ServerCall to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols'
1,'Deprecate FileUtil copyMerge FileUtil copyMerge is currently unused in the Hadoop source tree In branch1 it had been part of the implementation of the hadoop fs getmerge shell command In branch2 the code for that shell command was rewritten in a way that no longer requires this method'
1,'Implement drop method for DataFrame in SparkR'
1,'update InternalRowtoSeq to make it accept data type info'
1,'isnotnull operator not pushed down for JDBC datasource IsNotNull filter is not being pushed down for JDBC datasource It looks it is SQL standard according to SQL92 SQL 1999 SQL 2003 and SQL 201x and I believe most databases support this'
1,'Support weighted instances in naive Bayes In naive Bayes we expect inputs to be individual observations In practice people may have the frequency table instead It is useful for us to support instance weights to handle this case'
1,'Add MessageHandler function to KinesisUtilscreateStream similar to Direct Kafka There is support for message handler in Direct Kafka Stream which allows arbitrary T to be the output of the stream instead of ArrayByte This is a very useful function therefore should exist in Kinesis as well'
1,'support wildcard in libjars argument There is a problem when a user job adds too many dependency jars in their command line The HADOOPCLASSPATH part can be addressed including using wildcards \ But the same cannot be done with the libjars argument Today it takes only fully specified file paths We may want to consider supporting wildcards as a way to help users in this situation The idea is to handle it the same way the JVM does it \ expands to the list of jars in that directory It does not traverse into any child directory Also it probably would be a good idea to do it only for libjars ie dont do it for files and archives'
1,'Improve performance of Decimaltimes and casting from integral'
1,'Propagate data source options to Hadoop configurations We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work For example there are various options in parquetmr that users might want to set but the data source API does not expose a perjob way to set it This patch propagates the userspecified options also into Hadoop Configuration'
1,'Fix usage of isnan isNaN Add isNaN to Column for SparkR Column should has three related variable functions isNaN isNull isNotNull Replace DataFrameisNaN with DataFrameisnan at SparkR side Because DataFrameisNaN has been deprecated and will be removed at Spark 20 Add isnull to DataFrame for SparkR DataFrame should has two related functions isnan isnull'
1,'Add overwrite option for get shell command I think itd be good to add an argument to specify that the local file be overwritten if it exists when doing a DFS Get operation'
1,'Repartition operator should use Exchange to perform its shuffle Spark SQLs Repartition operator is implemented in terms of Spark Cores repartition operator which means that it has to perform lots of unnecessary row copying and inefficient row serialization Instead it would be better if this was implemented using some of Exchanges internals so that it can avoid row format conversions and generic getters hashcodes'
1,'support null ordering for DataFrame API SPARK10747 has added support for NULLS FIRST | LAST in ORDER BY clause for SQL interface This JIRA is to complete this feature by adding same support for DataFrame Dataset APIs
1,'Allow smartapplypatchsh to add new files in binary git patches When a new file is added the source is dev null rather than the root of the tree which would mean a a b prefix'
1,'Avoid memory copy in JavaSerializerInstanceserialize JavaSerializerInstanceserialize uses ByteArrayOutputStreamtoByteArray to get the serialized data ByteArrayOutputStreamtoByteArray needs to copy the content in the internal array to a new array However since the array will be converted to ByteBuffer at once we can avoid the memory copy'
1,'Add a method to list the referenced columns in data source Filter It would be useful to support listing the columns that are referenced by a filter'
1,'IPC server max queue size should be configurable Currently max queue size for IPC server is set to 100 handlers Usually when RPC failures are observed eg HADOOP1763 we increase number of handlers and the problem goes away I think a big part of such a fix is increase in max queue size I think we should make maxQsize per handler configurable with a bigger default than 100 There are other improvements also HADOOP1841 Server keeps reading RPC requests from clients When the number inflight RPCs is larger than maxQsize the earliest RPCs are deleted This is the main feedback Server has for the client I have often heard from users that Hadoop doesnt handle bursty traffic Say handler count is 10 default and Server can handle 1000 RPCs a sec quite conservative low for a typical server it implies that an RPC can wait for only for 1 sec before it is dropped If there 3000 clients and all of them send RPCs around the same time not very rare with heartbeats etc 2000 will be dropped In stead of dropping the earliest RPCs if the server delays reading new RPCs the feedback to clients would be much smoother I will file another jira regd queue management For this jira I propose to make queue size per handler configurable with a larger default may be 500'
1,'Deprecate HADOOPSERVERNAMEOPTS replace with command  subcommand OPTS Big features like YARN2928 demonstrate that even senior level Hadoop developers forget that daemons need a custom OPTS env var We can replace all of the custom vars with generic handling just like we do for the username check This makes it consistent across the entire project consistent for every subcommand eliminates almost all of the custom appending in the case statements Its worth pointing out that subcommands like distcp that sometimes need a higher than normal clientside heapsize or custom options are a huge win Combined with hadooprc and or dynamic subcommands it means users can easily do customizations based upon their needs without a lot of weirdo shell aliasing or one line shell scripts off to the side'
1,'SQL Code refactoring and comment correction in Dataset APIs Created a new private variable boundTEncoder that can be shared by multiple functions RDD select and collect Replaced all the queryExecutionanalyzed by the function call logicalPlan A few API comments are using wrong  names eg DataFrame or parameter names eg n A few API descriptions are wrong eg mapPartitions'
1,'Need mapping from long principal names to local OS user names We need a configurable mapping from full user names eg omalleyAPACHEORG to local user names eg omalley For many organizations it is sufficient to just use the prefix however in the case of shared clusters there may be duplicated prefixes A configurable mapping will let administrators resolve the issue'
1,'Implement SQL data source API for reading LIBSVM data It is convenient to implement data source API for LIBSVM format to have a better integration with DataFrames and ML pipeline API This JIRA covers the following Read LIBSVM data as a DataFrame with two columns label Double and features Vector Accept numFeatures as an option The implementation should live under orgapachesparkmlsourcelibsvm'
1,'New binary file format SequenceFiles block compression format is too complex and requires 4 codecs to compress or decompress It would be good to have a file format that only needs'
1,'Execute multiple Python UDFs in single batch'
1,'Enable MaxInactiveInterval for hadoop http auth token During http authentication a cookie which contains the authentication token is dropped The expiry time of the authentication token can be configured via hadoophttpauthenticationtokenvalidity The default value is 10 hours For clusters which require enhanced security it is desirable to have a configurable MaxInActiveInterval for the authentication token If there is no activity during MaxInActiveInterval the authentication token will be invalidated The MaxInActiveInterval will be less than hadoophttpauthenticationtokenvalidity The default value will be 30 minutes'
1,'RPC timeout should not override IPC ping interval Currently if the value of ipcclientrpctimeoutms is greater than 0 the timeout overrides the ipcpinginterval and client will throw exception instead of sending ping when the interval is passed RPC timeout should work without effectively disabling IPC ping'
1,'Enable TLS v11 and 12 Java 7 supports TLSv11 and TLSv12 which are more secure than TLSv1 which was all that was supported in Java 6 so we should add those to the default list for hadoopsslenabledprotocols'
1,'Update Chukwa Demux process simplify Parsers implementation add map and reduce side to the demux add dynamic link between RecordType and Parsers using configuration file and alias encapsulate data files creation location and naming convention inside the core demux classes sort all data by TimePartition Machine Timestamp by default'
1,'Change untar to use Java API on Windows instead of spawning tar process Currently FileUtilunTar spawns tar utility to do the work Tar may not be present on all platforms by default eg Windows So changing this to use JAVA APIs would help make it more crossplatform FileUtilunZip uses the same approach'
1,'Speedup distcp buildListing using threadpool For very large source trees on s3 distcp is taking long time to build file listing client code before starting mappers For a dataset I used 15M files 50K dirs it was taking 65 minutes before my fix in HADOOP11785 and 36 minutes after the fix'
1,'Add noniterator interface to RandomSampler RandomSamplersample currently accepts iterator as input and output another iterator This makes it inappropriate to use in wholestage codegen of Sampler operator We should add noniterator interface to RandomSampler'
1,'to optimize hudsonBuildHadoopNightlysh script'
1,'testpatch should verify that FindBugs version used for verification is correct one Theres a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson testpatch script has to verify if the version of FindBugs is correct'
1,'Deprecate DistCpV1 and Logalyzer deprecate DistCpV1 and Logalyzer which are no longer used'
1,'Add UserDefinedType support to RowEncoder method RowEncoder method doesnt support UserDefinedType now We should add the support for it'
1,'Support group by ordinal in SQL This is to support order by position in SQL eg We only convert integer literals not foldable expressions For positions that are aggregate functions an analysis exception should be thrown eg in postgres This should be controlled by config option sparksqlgroupByOrdinal'
1,'Deprecate usage of NativeIO link Since our min version is now JDK7 theres hardlink support via Files This means we can deprecate the JNI implementation and discontinue usage'
1,'Allow pluggable audit loggers in KMS Currently KMS audit log is using log4j to write a text format log We should refactor this so that people can easily add new format audit logs The current text format log should be the default and all of its behavior should remain compatible'
1,'FileContext and AbstractFileSystem should be annotated as a Stable interface The FileContext  currently is annotated as Evolving However at this point we really need to treat it as a Stable interface'
1,'Reload cached groups in background after expiry In HADOOP11238 the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background avoiding many slow group lookups Even with this change I have seen quite a few clusters with issues due to slow group lookups The problem is most prevalent in HA clusters where a slow group lookup on the hdfs user can fail to return for over 45 seconds causing the Failover Controller to kill it The way the current Guava cache implementation works is approximately On initial load the first thread to request groups for a given user blocks until it returns Any subsequent threads requesting that user block until that first thread populates the cache When the key expires the first thread to hit the cache after expiry blocks While it is blocked other threads will return the old value I feel it is this blocking thread that still gives the Namenode issues on slow group lookups If the call from the FC is the one that blocks and lookups are slow if can cause the NN to be killed Guava has the ability to refresh expired keys completely in the background where the first thread that hits an expired key schedules a background cache reload but still returns the old value Then the cache is eventually updated This patch introduces this background reload feature There are two new parameters hadoopsecuritygroupscachebackgroundreload default false to keep the current behaviour Set to true to enable a small thread pool and background refresh for expired keys hadoopsecuritygroupscachebackgroundreloadthreads only relevant if the above is set to true Controls how many threads are in the background refresh pool Default is 1 which is likely to be enough'
1,'Use inmemory for execution hives derby metastore Starting from Hive 013 the derby metastore can use a inmemory backend Since our execution hive is a fake metastore if we use inmemory mode we can reduce the time that is used on creating the execution hive'
1,'Have a builtin CSV data source implementation CSV is the most common data format in the small data world It is often the first format people want to try when they see Spark on a single node Making this builtin for the most common source can provide a better experience for firsttime users'
1,'Add support for DataFrameStatFunctions in SparkR The stat functions are defined Currently only crosstab is supported Functions to be supported include corr cov freqItems'
1,'Add support for writing partitioned csv json text formats in Structured Streaming Support for partitioned parquet format in FileStreamSink was added in Spark14716 now lets add support for partitioned csv json text format'
1,'Python API for MaxAbsScaler After SPARK13028 we should add Python API for MaxAbsScaler'
1,'Automatically use Kryo serializer when shuffling RDDs with simple types Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDDs types are guaranteed to be compatible with Kryo eg RDDs whose key value and or combiner types are primitives arrays of primitives or strings This is likely to result in a large performance gain for many RDD API workloads'
1,'RPC Layer improvements to support protocol compatibility'
1,'Add toLocalIterator method to pyspark rdd toLocalIterator method is available in Java and Scala If we add this functionality to Python then we can also be able to use PySpark to iterate over a dataset partition by partition'
1,'Rolling mechanism for demux output In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output avoid immediate merging if theres already file for the same time range create a spill file instead merge all raw files every hours merge all hourly files every days'
1,'Add a new instrumented readwrite lock Add a new instrumented readwrite lock in hadoop common so that the HDFS9668 can use this to improve the locking in FsDatasetImpl'
1,'Dateset Time Windowing API for Python R and SQL The time windowing function window was added to Datasets This JIRA is to track the status for the R Python and SQL API'
1,'Add a command to FsShell stat to get a files block location information Adding an option to FsShell stat to get a files block location information will be very useful'
1,'Make the mllibml linalg type conversion APIs public We should open up the APIs for converting between new old linear algebra types in sparkmlliblinalg I made these private originally but they will be useful for users transitioning workloads'
1,'Add getOrCreate method for SparkContext SQLContext for Python Also SQLContextnewSession method'
1,'Add deleteWithJob hook to internal commit protocol API Currently in SQL we implement overwrites by calling fsdelete directly on the original data This is not ideal since we the original files end up deleted even if the job aborts We should extend the commit protocol to allow file overwrites to be managed as well'
1,'Add direct flag option for fs copy so that user can choose not to create COPYING file Because CLI is using CommandWithDestinationjava which add COPYING to the tail of file name when it does the copy For blobstore like S3 and Swift to create COPYING file and rename it is expensive direct flag can allow user to avoiding the COPYING file'
1,'Add reencryptEncryptedKey interface to KMS This is the KMS part Please refer to HDFS10899 for the design doc'
1,'Make FTPFileSystems data connection mode and transfer mode configurable The FTP transfer mode used by FTPFileSystem is BLOCKTRANSFERMODE FTP Data connection mode used by FTPFileSystem is ACTIVELOCALDATACONNECTIONMODE This jira makes them configurable'
1,'Add KMeanSummary in KMeans of PySpark Theres no corresponding python api for KMeansSummary it would be nice to have it'
1,'Support USING clause in JOIN Support queries that JOIN tables with USING clause'
1,'Make additional KMS tomcat settings configurable Doing some Tomcat performance tuning on a loaded cluster we found that acceptCount acceptorThreadCount and protocol can be useful Lets make these configurable in the kms startup script Since the KMS is Jetty in 3x this is targeted at just branch2'
1,'Deprecate shell vars It is a very common shell pattern in 3x to effectively replace subproject specific vars with generics We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated Additionally we should use this shell function to deprecate the shell vars that are holdovers already'
1,'Add support for offheap caching We should add support for caching serialized data offheap within the same process ie using direct buffers or sunmiscunsafe'
1,'Expose HadoopKerberosName as a hadoop subcommand HadoopKerberosName has been around as a secret hack for quite a while We should clean up the output and make it official by exposing it via the hadoop command'
1,'Reject invalid Windows URIs This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous'
1,'YarnShuffleService should use YARN getRecoveryPath for leveldb location The YarnShuffleService currently just picks a directly in the yarn local dirs to store the leveldb file YARN added an interface in hadoop 25 getRecoverPath to get the location where it should be storing this We should change to use getRecoveryPath This does mean we will have to use reflection or similar to check for its existence though since it doesnt exist before hadoop 25'
1,'Introduce additonal implementation with a dense format for UnsafeArrayData It would be good to have an additional implementation which uses dense format for UnsafeArrayData to reduce memory footprint Current UnsafeArrayData implementation uses only a sparse format It is useful for an UnsafeArrayData that is created by a method fromPrimitiveArray which have no null value'
1,'Consider nullability of expression in codegen In codegen we didnt consider nullability of expressions Once considering this we can avoid lots of null check reduce the size of generated code also improve performance Before that we should doublecheck the correctness of nullablity of all expressions and schema or we will hit NPE or wrong results'
1,'hadoopconfigsh needs to be updated post mavenization hadoopcommon src main bin hadoopconfigsh needs to be updated post mavenization eg it still refers to build classes etc'
1,'Updated utility to create modify token files hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations Additionally the token files that are created use Java serializations which are hard impossible to deal with in other languages It should be replaced with a better utility in common that can read write protobufbased token files has enough flexibility to be used with other services and offers key functionality such as append and rename The old version file format should still be supported for backward compatibility but will be effectively deprecated A followon JIRA will deprecrate fetchdt'
1,'Add slaves shell option Add a slaves shell option to hadoopconfigsh to trigger the given command on slave nodes This is required to deprecate hadoopdaemonssh and yarndaemonssh'
1,'All functions should show usages by command DESC FUNCTION Currently many functions do now show usages like the followings This PR adds descriptions for functions and adds a testcase prevent adding function without usage The only exceptions are cube grouping groupingid rollup window'
1,'Guard against race condition when recaching spilled bytes in memory When reading data from the DiskStore and attempting to cache it back into the memory store we should guard against race conditions where multiple readers are attempting to recache the same block in memory'
1,'Change ipcClient to support asynchronous calls In ipcClient the underlying mechanism is already supporting asynchronous calls the calls shares a connection the call requests are sent using a thread pool and the responses can be out of order Indeed synchronous call is implemented by invoking wait in the caller thread in order to wait for the server response In this JIRA we change ipcClient to support asynchronous mode In asynchronous mode it return once the request has been sent out but not wait for the response from the server'
1,'Extend CSRF Filter with UserAgent Checks To protect against CSRF attacks HADOOP12691 introduces a CSRF filter that will require a specific HTTP header to be sent with every REST API call This will affect all API consumers from web apps to CLIs and curl Since CSRF is primarily a browser based attack we can try and minimize the impact on nonbrowser clients This enhancement will provide additional configuration for identifying nonbrowser useragents and skipping the enforcement of the header requirement for anything identified as a nonbrowser This will largely limit the impact to browser based PUT and POST calls when configured appropriately'
1,'Make runs no effect in kmeans We deprecated runs in Spark 16 SPARK11358 In 20 we can either remove runs or make it no effect with warning messages So we can simplify the implementation I prefer the latter for better binary compatibility'
1,'SparkR sparkglm should have configurable regularization parameter Spark has configurable L2 regularization parameter for generalized linear regression It is very important to have them in SparkR so that users can run ridge regression'
1,'Make some ML APIs public VectorUDT Identifiable ProbabilisticClassifier This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages We plan to make these APIs public in Spark 15 However they will be marked DeveloperApi and are very likely to be broken in the future VectorUDT To define a relation with a vector field VectorUDT must be instantiated Identifiable trait The trait generates a unique identifier for the associated pipeline component Nice to have a consistent format by reusing the trait ProbabilisticClassifier Thirdparty components should leverage the complex logic around computing only selected columns We will not yet make these public SchemaUtils Thirdparty pipeline components have a need for checking column types and appending columns This will probably be moved into Spark SQL Users can copy the methods into their own code as needed Shared Params HasLabel HasFeatures This is covered in SPARK7146 but reiterating it here We need to discuss whether these should be standardized public APIs Users can copy the traits into their own code as needed'
1,'Add XFS Filter for UIs to Hadoop Common Cross Frame Scripting XFS prevention for UIs can be provided through a common servlet filter This filter will set the XFrameOptions HTTP header to DENY unless configured to another valid setting There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all its proxied UIs if appropriate'
1,'Support Futureget with timeout in ipc async calls Currently the Future returned by ipc async call only support Futureget but not Futureget timeout unit We should support the latter as well'
1,'DataFrametransform function Similar to Datasettransform function'
1,'Metrics for codegen size and perf We should expose codahale metrics for the codegen source text size and how long it takes to compile The size is particularly interesting since the JVM does have hard limits on how large methods can get'
1,'Improve session management functionality for SQL Currently we try to support multiple sessions in SQL within a Spark Context but its broken and not complete We should isolate these for each session For added jar and cached tables they should be accessible for all sessions'
1,'Support to retrieve specific property from configuration via REST API Currently we can use rest API to retrieve all configuration properties per daemon but unable to get a specific property by name This causes extra parse work at client side when dealing with Hadoop configurations and also its quite over head to send all configuration in a http response over network Propose to support following a name parameter in the http request by issuing This change is fully backwards compatible'
1,'Add bround function in Python R This issue aims to expose Scala bround function in Python R API bround function is implemented in SPARK14614 by extending current round function'
1,'Add w r options in dfs test command Currently the dfs test command only supports options It would be helpful if we add to verify permission of r w before actual read or write This will help script programming'
1,'Implement trackStateByKey for improved state management This is the first cut implementation of trackStateByKey new improvement state management method in Spark Streaming'
1,'AESbased authentication mechanism for Spark In SPARK13331 support for AES encryption was added to the Spark network library But the authentication of different Spark processes is still performed using SASLs DIGESTMD5 mechanism That means the authentication part is the weakest link since the AES keys are currently encrypted using 3des strongest cipher supported by SASL Spark cant really claim to provide the full benefits of using AES for encryption We should add a new auth protocol that doesnt need these disclaimers'
1,'Improve the type check of CollectSet in CheckAnalysis CollectSet cannot have maptyped data because MapTypeData does not implement equals So if we find map type in CollectSet queries fail'
1,'Collapse adjacent Repartition operations Spark SQL should collapse adjacent Repartition operators and only keep the last one'
1,'Implement xpath user defined functions Spark SQL currently falls back to Hive for xpath related functions'
1,'Limit the number of outstanding async calls In async RPC if the callers dont read replies fast enough the buffer storing replies could be used up This is to propose limiting the number of outstanding async calls to eliminate the issue'
1,'Add readorc writeorc to SparkR This issue adds readorc writeorc to SparkR for API parity'
1,'QueryPlanexpressions should always include all expressions'
1,'Implement struct encodedecode methofs in SparkR Implement struct encodedecode methods in SparkR'
1,'Python API for Generalized Linear Regression Summary We should add an interface to the GLR summaries in Python for feature parity'
1,'Support decoding KMS Delegation Token with its own Identifier kmsdt currently does not have its own token identifier  to decode it properly as shown in the HDFS logs below This JIRA is opened to add support for that'
1,'Python API for GeneralizedLinearRegression After SPARK12811 we should add Python API for generalized linear regression'
1,'Retry until TGT expires even if the UGI renewal thread encountered exception The UGI has a background thread to renew the tgt On exception it terminates itself If something temporarily goes wrong that results in an IOE even if it recovered no renewal will be done and client will eventually fail to authenticate We should retry with our best effort until tgt expires in the hope that the error recovers before that'
1,'Keep two generations of fsimage Checkpoint to verify the fsimage each time it creates the new one'
1,'Allow ditscp to accept bandwitdh in fraction MegaBytes DistCp uses ThrottleInputStream which provides a bandwidth throttling on a specified stream Currently Distcp allows the max bandwidth value in Mega Bytes which does not accept fractional values It would be better if it accepts the Max Bandwitdh in fractional MegaBytes Due to this we are not able to throttle the bandwidth in KBs in our prod setup'
1,'Forward port SequenceFile syncFs and friends from Hadoop 1x HDFS200 added a new public API SequenceFile syncFs we need to forward port this for compatibility Looks like it might have introduced other APIs that need forward porting as well eg LocaltedBlocks setFileLength and DataNode getBlockInfo'
1,'Expose event time time stats through StreamingQueryProgress API'
1,'Add CSRF Filter for REST APIs to Hadoop Common CSRF prevention for REST APIs can be provided through a common servlet filter This filter would check for the existence of an expected configurable HTTP header such as XXSRFHeader The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin'
1,'Add varargstype dropDuplicates function in SparkR This is for API parity of Scala API'
1,'EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied Consider SortMergeJoin which requires a sorted clustered distribution of its input rows Say that both of SMJs children produce unsorted output but are both single partition In this case we will need to inject sort operators but should not need to inject exchanges Unfortunately it looks like the Exchange unnecessarily repartitions using a hash partitioning We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied Id like to fix this for Spark 15 since it makes certain types of unit tests easier to write'
1,'Add Python API for mlregressionIsotonicRegression Add Python API for mlregressionIsotonicRegression'
1,'Make ML APIs in SparkR consistent In current master we have 4 ML methods in SparkR We tried to keep the signatures similar to existing ones in R However if we put them together they are not consistent One example is kmeans which doesnt accept a formula Instead of looking at each method independently we might want to update the signature of kmeans We can also discuss possible global changes here For example glm puts family before data while kmeans puts centers after data This is not consistent And logically the formula doesnt mean anything without associating with a DataFrame If we make this change we might want to avoid name collisions because they have different signature We can use mlkmeans mlglm etc Sorry for discussing API changes in the last minute But I think it would be better to have consistent signatures in SparkR'
1,'Native database table system catalog As of Spark 16 Spark SQL internally has only a limited catalog and does not support any of the DDLs This is an umbrella ticket to introduce an internal API for a system catalog and the associated DDL implementations using this API'
1,'Direct consume ColumnVector in generated code when ColumnarBatch method is used When generated code accesses a ColumnarBatch object it is possible to get values of each column from ColumnVector instead of calling getRow'
1,'Introduce a JVM object based aggregate operator The new Tungsten execution engine has very robust memory management and speed for simple data types It does however suffer from the following For userdefined aggregates Hive UDAFs Dataset typed operators it is fairly expensive to fit into the Tungsten internal format For aggregate functions that require complex intermediate data structures Unsafe on raw bytes is not a good programming abstraction due to the lack of structs The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases This operator however should limit its memory usage to avoid putting too much pressure on GC eg falling back to sortbased aggregate as soon the number of objects exceeds a very low threshold Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speedups over existing Spark'
1,'Add bround function This issue aims to add bound function aka Bankers round by extending current round implementation Hive supports bround since 130'
1,'Control http authentication cookie persistence via configuration During http authentication a cookie is dropped This is a persistent cookie The cookie is valid across browser sessions For clusters which require enhanced security it is desirable to have a session cookie so that cookie gets deleted when the user closes browser session It should be possible to specify cookie persistence session or persistent via configuration'
1,'Fast serialization for collecting DataFrame UnsafeRowSerializer should be more efficient than JavaSerializer or KyroSerializer for DataFrame'
1,'JDBC writer change to use batch insert for performance Currently JDBC write is using single row insert using executeUpdate command instead change to executeBatch which will handle multiple inserts by most databases in more efficient manner'
1,'Add ShortType support to UnsafeRowParquetRecordReader By enabling vectorized parquet scanner by default the unit test ParquetHadoopFsRelationSuite based on HadoopFsRelationTest will be failed due to the lack of short type support in UnsafeRowParquetRecordReader We should fix it'
1,'Cooperative memory management We have memory starving problems for a long time it become worser in 15 since we use larger page In order to increase the memory usage reduce unnecessary spilling also reduce the risk of OOM we should manage the memory in a cooperative way it means all the memory consume should be also responsive to release memory spilling upon others requests The requests of memory could be different hard requirement will crash if not allocated or soft requirement worse performance if not allocated Also the costs of spilling are also different We could introduce some kind of priority to make them work together better'
1,'hadoopconfigsh needs to be updated post MR2 hadoopcommon src main bin hadoopconfigsh needs to be updated post MR2 eg the layout of mapred home has changed'
1,'Expose API on UnsafeRowRecordReader to just run on files This is beneficial just from a code testability point of view to be able to exercise individual components Also makes it easy to benchmark it It would be able to read data without need to create al the associate hadoop input split etc components'
1,'Expose ml summary function in PySpark for classification and regression models I think model summary interface which is available in Sparks scala Java and R interfaces should also be available in the python interface'
1,'Dont use Javascript for web UIs paginated table navigation controls The web UIs paginated table uses Javascript to implement certain navigation controls such as table sorting and the go to page form This is unnecessary and should be simplified to use plain HTML form controls and links'
1,'Let SQLBuilder convert logical plan without a Project on top of it It is possibly that a logical plan has been removed Project from the top of it Or the plan doesnt has a top Project from the beginning Currently the SQLBuilder cant convert such plans back to SQL This issue is opened to add this feature'
1,'Add drop support for DataFrames subset function SparkR DataFrame can be subset to get one or more columns of the dataset The current implementation does not support drop when is asked for just one column We should add the drop support'
1,'Herriot Implement a functionality for getting proxy users definitions like groups and hosts Gridmix should require a proxy users file for impersonating various jobs So implement couple of methods for getting the proxy users list and a proxy users file its a combination of proxy users and groups based on cluster configuration The proxy users list should require for map reduce jobs and proxy users file should require for gridmix jobs The following are methods signature public ProxyUserDefinitions getHadoopProxyUsers get the list of proxy users list based on cluster configuration'
1,'Faster pivot implementation for many distinct values with two phase aggregation The existing implementation of pivot translates into a single aggregation with one aggregate per distinct pivot value When the number of distinct pivot values is large say 1000+ this can get extremely slow since each input value gets evaluated on every aggregate even though it only affects the value of one of them Im proposing an alternate strategy for when there are 10+ somewhat arbitrary threshold distinct pivot values We do two phases of aggregation In the first we group by the grouping columns plus the pivot column and perform the specified aggregations one or sometimes more In the second aggregation we group by the grouping columns and use the new non public PivotFirst aggregate that rearranges the outputs of the first aggregation into an array indexed by the pivot value Finally we do a project to extract the array entries into the appropriate output column'
1,'Dont cache MEMORYANDDISK blocks as bytes in memory store when reading spills When a cached block is spilled to disk and read back in serialized form ie as bytes the current BlockManager implementation will attempt to reinsert the serialized block into the MemoryStore even if the blocks storage level requests deserialized caching This behavior adds some complexity to the MemoryStore but I dont think it offers many performance benefits and Id like to remove it in order to simplify a larger refactoring patch Therefore I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels There are two places where we request serialized bytes from the BlockStore getLocalBytes which is only called when reading local copies of TorrentBroadcast pieces Broadcast pieces are always cached using a serialized storage level so this wont lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store the nonshuffleblock branch in getBlockData which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks Caching the serialized bytes in memory will only benefit us if those cached bytes are read before theyre evicted and the likelihood of that happening seems low since the frequency of remote reads of nonbroadcast cached blocks seems very low Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms since those blocks seem more likely to be read in local computation Therefore I think this is a safe change'
1,'Incrementally serialize blocks while unrolling them in MemoryStore When a block is persisted in the MemoryStore at a serialized storage level the current MemoryStoreputIterator code will unroll the entire iterator as Java objects in memory then will turn around and serialize an iterator obtained from the unrolled array This is inefficient and doubles our peak memory requirements Instead I think that we should incrementally serialize blocks while unrolling them A downside to incremental serialization is the fact that we will need to deserialize the partiallyunrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk However Im hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefullyrare case'
1,'SQL API audit for Spark 16 Umbrella ticket to walk through all newly introduced APIs to make sure they are consistent'
1,'Add StreamingContextgetActiveOrCreate to python API'
1,'Improve performance of Range APIs via adding logical physical operators Creating an actual logical physical operator for range for matching the performance of RDD Range APIs Compared with the old Range API the new version is 3 times faster than the old version'
1,'ML Evaluator should indicate if metric should be maximized or minimized ML Evaluator currently requires that metrics be maximized bigger is better That is counterintuitive for some metrics Currently we hackily negate some metrics in RegressionEvaluator which is weird Instead we should Return the metric as expected eg rmse should return RMSE not its negation Provide an indicator of whether the metric should be maximized or minimized Model selection algorithms can use the indicator as needed'
1,'Hadoop Core should support source filesfor multiple schedulers Besides the default JT scheduling algorithm there is work going on with at least two more schedulers HADOOP3445 HADOOP3746 HADOOP3412 makes it easier to plug in new schedulers into the JT Where do we place the source files for various schedulers so that its easy for users to choose their scheduler of choice during deployment and easy for developers to add in more schedulers into the framework without inundating it'
1,'Overhaul metrics framework Per discussions with Arun Chris Hong and Rajiv et al we concluded that the current metrics framework needs an overhaul to Allow multiple plugins for different monitoring systems simultaneously see also HADOOP6508 Refresh metrics plugin config without server restart Including filtering of metrics per plugin Support metrics schema for plugins'
1,'ShutdownHookManager should have a timeout for each of the Registered shutdown hook HADOOP8325 added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook For each of the shutdown hook registered we currently dont have an upper bound for its execution time We have seen namenode failed to shutdown completely waiting for shutdown hook to finish after failover for a long period of time which breaks the namenode high availability scenarios This ticket is opened to allow specifying a timeout value for the registered shutdown hook'
1,'JobConf option for minimum progress threshold before reducers are assigned A specific subcase of the general priority inversion problem noted in HADOOP4557 is when many lower priority jobs are submitted and are waiting for mappers to free up Even though they havent actually done any work they will be assigned any free reducers If a higher priority job is submitted priority inversion results not just due to the lower priority tasks that are in the midst of completing but also due to the ones that havent yet started but have claimed all the free reducers A simple workaround is to require a job to complete some useful work before assigning it a reducer This can be done in a tunable and backwards compatible manner by adding a minimum map progress percentage before assigning a reducer option to the JobConf Setting this to 0 would eliminate the common case above and setting it to 100 would technically eliminate the inversion of HADOOP4557 though likely at an unacceptably high cost'
1,'Add SQLUserDefinedType support for encoder We should add SQLUserDefinedType support for encoder'
1,'Add a configuration to set ipcClients traffic  with IPTOSLOWDELAY|IPTOSRELIABILITY During heavy shuffle packet loss for IPC packets was observed from a machine Avoid packetloss and speed up transfer by using 0x14 QOS bits for the packets'
1,'Add Python API for MultilayerPerceptronClassifier Add Python API for MultilayerPerceptronClassifier'
1,'Improve filter push down Right now filter push down only works with Project Aggregate Generate and Join they cant be pushed through many other plans'
1,'Create external block store API Would be great to create APIs for external block stores rather than doing a bunch of if statements everywhere'
1,'Implement Python pickler methods for mlVector and mlMatrix under sparkmlpython Now picklers for both new and old vectors are implemented under PythonMLlibAPI To separate sparkmllib from sparkml we should implement them under sparkmlpython instead I set the target to 21 since those are private APIs'
1,'Expose RDD localCheckpoint in PySpark As of today I could not access rddlocalCheckpoint in pyspark This is an important issue for machine learning people as we often have to iterate algorithms and perform operations like joins in each iteration If the lineage is not truncated the memory usage the lineage and computation time explode rddlocalCheckpoint seems like the most straightforward way of truncating the lineage but the python API does not expose it'
1,'Reserve a page in all unsafe operators to avoid starving an operator Eg currently we can do up to 3 sorts within a task During the aggregation During a sort on the same key During the shuffle In environments with tight memory restrictions the first operator may acquire so much memory such that the subsequent ones in the same task are starved A simple fix is to reserve at least a page in advance in each of these places The reserved page size need not be the same as the normal page size This is a sister problem to SPARK4452 in Spark Core'
1,'ML 20 QA Scala APIs audit for evaluation tuning'
1,'WholeTextFileRDD function should return Text rather than String If it returns Text we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF8String without extra string decoding and encoding'
1,'Add recoverPartitions API to Catalog interface Currently we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog MSCK REPAIR TABLE or ALTER TABLE table RECOVER PARTITIONS Actually very hard for me to remember MSCK and have no clue what it means After the new Scalable Partition Handling the table repair becomes much more important for making visible the data in the created data source partitioned table It is desriable to add it into the Catalog interface so that users can repair the table'
1,'Add withWatermark and checkpoint methods to python dataframe These two methods were added to Scala Datasets but are not available in Python yet'
1,'Add asserttrue function This issue aims to implement asserttrue function Its since 12 The following is function description of Hive 12'
1,'SparkSessiontime a simple timer function Many Spark developers often want to test the runtime of some function in interactive debugging and testing Itd be really useful to have a simple sparktime method that can test the runtime'
1,'Add computeCost and clusterCenters methods to KMeansModel in sparkml The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier'
1,'Improve DataFrame API compatibility with Pandas This is not always possible but whenever possible we should reduce the differences between Pandas and Spark DataFrames in Python'
1,'Change toBreeze to asBreeze in Vector and Matrix Were using asML to convert the mllib vector matrix to ml vector matrix now Using as is more correct given that this conversion actually shares the same underline data structure As a result in this PR toBreeze will be changed to asBreeze This is a private API as a result it will not affect any users application'
1,'Make FileStream function be able to start with most recent files When starting a stream with a lot of backfill and maxFilesPerTrigger the user could often want to start with most recent files first This would let you keep low latency for recent data and slowly backfill historical data Its better to add an option to control this behavior'
1,'hadoop dfs ls Do not expand directories was HDFS1475 In a nutshell ls needs the ability to list a directory but not its contents W o d it is impossible to list the root directorys owner permissions etc See the original hdfs bug for details'
1,'Add a REST api to spark streaming trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming it let us no choice but to implement one for ourself this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of sparkcore and will be available for running applications only'
1,'AutoHA Allow manual failover to be invoked from zkfc HADOOP8247 introduces a configure flag to prevent potential status inconsistency between zkfc and namenode by making auto and manual failover mutually exclusive However as described in 272 section of design doc at HDFS2185 we should allow manual and auto failover coexist by adding some rpc interfaces at zkfc manual failover shall be triggered by haadmin and handled by zkfc if auto failover is enabled'
1,'DataFrame API should simplify defining frame boundaries without partitioning ordering When I was creating the example code for SPARK10496 I realized it was pretty convoluted to define the frame boundaries for window functions when there is no partition column or ordering column The reason is that we dont provide a way to create a WindowSpec directly with the frame boundaries We can trivially improve this by adding rowsBetween and rangeBetween to Window object'
1,'want InputFormat for bzip2 files Unlike gzip the bzip file format supports splitting Compression is by blocks 900k by default and blocks are separated by a synchronization marker a 48bit approximation of Pi This would permit very large compressed files to be split into multiple map tasks which is not currently possible unless using a Hadoopspecific file format'
1,'SQL function IFNULL NULLIF NVL and NVL2 It will be great to have these SQL functions The meaning of these functions could be found in oracle docs'
1,'Add R API for stddev variance'
1,'Read s3a creds from a Credential Provider It would be good if we could read s3 creds from a source other than via a java property Hadoop configuration option'
1,'Support for incremental generation in the protoc plugin The protoc maven plugin currently generates new Java classes every time which means Maven always picks up changed files in the build It would be better if the protoc plugin only generated new Java classes when the source protoc files change'
1,'Decrease communication in BlockMatrix multiply and increase performance The BlockMatrix multiply sends each block to all the corresponding columns of the right BlockMatrix even though there might not be any corresponding block to multiply with Some optimizations we can perform are Simulate the multiplication on the driver and figure out which blocks actually need to be shuffled Send the block once to a partition and join inside the partition rather than sending multiple copies to the same partition'
1,'Subexpression elimination in wholestage codegen version of TungstenAggregate Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination We should support it'
1,'IPC Wire Compatibility'
1,'Improve the PushDownPredicate rule to pushdown predicates currectly in nondeterministic condition Currently our Optimizer may reorder the predicates to run them more efficient but in nondeterministic condition change the order between deterministic parts and nondeterministic parts may change the number of input rows'
1,'Stddev Variance etc should support columnName as arguments Spark SQL aggregate function should support columnName as arguments like other aggregate function max min count sum'
1,'Replace uses of ThreadLocal Random with JDK7 ThreadLocalRandom ThreadLocalRandom should be used when available in place of ThreadLocal Random For JDK7 the difference is minimal but JDK8 starts including optimizations for ThreadLocalRandom'
1,'pull argument parsing into a function In order to enable significantly better unit testing as well as enhanced functionality large portions of configsh should be pulled into functions See first comment for more'
1,'Estimator interface for generalized linear models GLMs In Spark 16 MLlib provides logistic regression and linear regression with L1 L2 elasticnet regularization We want to expand the support of generalized linear models GLMs in 20 eg Poisson Gamma families and more link functions SPARK9835 implements a GLM solver for the case when the number of features is small We also need to design an interface for GLMs In SparkR we can simply follow glm or glmnet On the Python Scala Java side the interface should be consistent with LinearRegression and LogisticRegression eg It would be great if LinearRegression and LogisticRegression can reuse code from GeneralizedLinearModel'
1,'Simplify accumulators and task metrics The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics They are unnecessarily convoluted and we should be able to simplify them quite a bit This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on At a high level Id would like to create better abstractions for internal implementations as well as creating a simplified accumulator v2 external interface that doesnt involve a complex type hierarchy'
1,'Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries ie we will do a client side copy of the blob and then copy it back to destination This operation will not be subject to throttling and hence should provide a stronger mitigation However it is more expensive hence we do it only in the case we fail after all retries'
1,'align map splits on sorted files with key boundaries this is something that we have implemented in the application layer may be useful to have in hadoop itself long term log storage systems often keep data sorted by some sortkey future computations on such files can often benefit from this sort order if the job requires grouping by the sortkey then it should be possible to do reduction in the map stage itself this is not natively supported by hadoop except in the degenerate case of 1 map file per task since splits can span the sortkey however aligning the data read by the map task to sort key boundaries is straightforward and this would be a useful capability to have in hadoop the definition of the sort key should be left up to the application its not necessarily the key field in a Sequencefile through a generic interface but otherwise the sequencefile and text file readers can use the extracted sort key to align map task data with key boundaries'
1,'Send back nicer error to clients using outdated IPC version When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump the client currently just gets a nonuseful error message like EOFException Instead the IPC server code can speak just enough of prior IPC protocols to send back a fatal message indicating the version mismatch'
1,'Avoid the copy in whole stage codegen when there is no joins When we generate code for join we copy the output row because there could be multiple output row from single input row We could avoid this copy when there is no join or the join will not generate multiple output rows from single input row'
1,'Expose Rlike summary statistics in SparkR glm for more family and link functions This continues the work of SPARK11494 SPARK9837 and SPARK12566 to expose Rlike model summary in more family and link functions'
1,'Support update DecimalType with precision 18 in UnsafeRow Currently we dont support using DecimalType with precision 18 in new unsafe aggregation its good to support it'
1,'SessionCatalog needs to check function existence Right now operations for an existing functions in SessionCatalog do not really check if the function exists We should add this check and avoid of doing the check in command'
1,'Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs The actual authentication is done by some external service that the handler will redirect to when there is no hadoopauth cookie and no JWT token found in the incoming request Using JWT provides a number of benefits It is not tied to any specific authentication mechanism so buys us many SSO integrations It is cryptographically verifiable for determining whether it can be trusted Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbusjosejwt library for processing validating and parsing JWT tokens'
1,'Create native DDL commands We currently delegate most DDLs directly to Hive through NativePlaceholder in HiveQlscala In Spark 20 we want to provide native implementations for DDLs for both SQLContext and HiveContext The first step is to properly parse these DDLs and then create logical commands that encapsulate them The actual implementation can still delegate to HiveNativeCommand As an example we should define a command for RenameTable with the proper fields and just delegate the implementation to HiveNativeCommand we might need to track the original sql query in order to run HiveNativeCommand but we can remove the sql query in the future once we do the next step Once we flush out the internal persistent catalog API we can then switch the implementation of these newly added commands to use the catalog API'
1,'ChukwaAgent controller should retry to register for a longer period but not as frequent as now Watchdog is watching for ChukwaAgent only once every 5 minutes so theres no point in retrying more than once every 5 mins In practice if the watchdog is not able to automatically restart the agent it will take more than 20 minutes to get Ops to restart it Also Ops want us to limit the number of communications between Hadoop and Chukwa thats why 30 minutes'
1,'VectorUDT MatrixUDT should take primitive arrays without boxing In SPARK9390 we switched to use GenericArrayData to store indices and values in vector matrix UDTs However GenericArrayData is not specialized for primitive types This might hurt MLlib performance badly We should consider either specialize GenericArrayData or use a different container'
1,'Add assertNotPartitioned check in DataFrameWriter method Sometimes it doesnt make sense to specify partitioning parameters eg when we write data out from Datasets DataFrames into jdbc tables or streaming ForeachWriters We probably should add checks against this in DataFrameWriter'
1,'CLASSPATH handling should be consolidated debuggable As part of HADOOP9902 java execution across many different shell bits were consolidated down to effectively two routines Prior to calling those two routines the CLASSPATH is exported This export should really be getting handled in the exec function and not in the individual shell bits so that bash x would show the content of the classpath or even a debug classpath option that would echo the classpath to the screen prior to java exec to help with debugging'
1,'Adding Append API support for WASB Currently the WASB implementation of the HDFS interface does not support Append API This JIRA is added to design and implement the Append API support to WASB The intended support for Append would only support a single writer'
1,'Configuration sends too much data to log4j Configuration objects send a DEBUGlevel log message every time theyre instantiated which include a full stack trace This is more appropriate for TRACElevel logging as it renders other debug logs very hard to read'
1,'Support excluding weak Ciphers in HttpServer2 through sslserverconf Currently Embeded jetty Server used across all hadoop services is configured through sslserverxml file from their respective configuration section However the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites This code changes aims to add following functionality Add logic in hadoop common HttpServer2java and associated interfaces to spawn jetty servers with ability to exclude weak cipher suites I propose we make this though sslserverxml and hence each service can choose to disable specific ciphers Modify DFSUtiljava used by HDFS code to supply new parameter sslserverexcludecipherlist for hadoopcommon code so it can exclude the ciphers supplied through this key'
1,'Add a config to disable the logs endpoints We should add a config to disable the logs endpoint in HttpServer2 Listing a directory like this can be dangerous from a security perspective We can keep it enabled by default for compatibility though'
1,'Add capability to resolve compression codec based on codec name When setting up a compression codec in an MR job the full  name of the codec must be used To ease usability compression codecs should be resolved by their codec name ie gzip deflate zlib bzip2 instead their full codec  name Besides easy of use for Hadoop users who would use the codec alias instead the full codec  name it could simplify how HBase resolves loads the codecs'
1,'Skip local processing in PrefixSpan if there are no small prefixes There exists a chance that the prefixes keep growing to the maximum pattern length Then the final local processing step becomes unnecessary'
1,'Handling of Trash with quota Currently with quota turned on user cannot call rmr on large directory that causes over quota Besides from error message being unfriendly how should this be handled'
1,'Update scripts to be smarter when running with privilege As work continues on HADOOP13397 its become evident that we need better hooks to start daemons as specifically configured users Via the command  subcommand USER environment variables in 3x we actually have a standardized way to do that This in turn means we can make the sbin scripts super functional with a bit of updating Consolidate startdfssh and startsecurednssh into one script Make start\ sh and stop\ sh know how to switch users when run as root Undeprecate start stopallsh so that it could be used as root for production purposes and as a single user for nonproduction users'
1,'Make DefaultParamsReadableWritable public APIs Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines Making them public should be safe even if we change internal formats
1,'NetworkTopology is not efficient adding getting removing nodes NetworkToplogy uses nodes with a list of children The access to these children is slow as its a linear search'
1,'LinearSVC Python API Create a Python wrapper for sparkmlclassificationLinearSVC'
1,'Parse Drop Function DDL command We only parse create function command In order to support native drop function command we need to parse it too'
1,'SparkR support hash function Add hash function for SparkR'
1,'ViewFileSystem should support storage policy related API Current ViewFileSystem does not support storage policy related API it will throw UnsupportedOperationException'
1,'Umbrella Dynamic subcommands for hadoop shell scripts Umbrella for converting hadoop hdfs mapred and yarn to allow for dynamic subcommands See first comment for more details'
1,'Deprecate metrics v1'
1,'Avoid duplicated broadcasts An broadcasted table could be used multiple times in a query we should cache them'
1,'Support SQL generation for inline tables Inline tables currently do not support SQL generation and as a result a view that depends on inline tables would fail'
1,'Improve ActiveStandbyElectors behavior when session expires Currently when the ZK session expires it results in a fatal error being sent to the application callback This is not the best behavior for example in the case of HA if ZK goes down we would like the current state to be maintained rather than causing either NN to abort When the ZK clients are able to reconnect they should sort out the correct leader based on the normal locking schemes'
1,'Python API for AFTSurvivalRegression After SPARK10686 we should add Python API for AFTSurvivalRegression'
1,'Support MetricsSource interface for DecayRpcScheduler Metrics This allows metrics collector such as AMS to collect it with MetricsSink The per user RPC call counts schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues'
1,'bucketed table support cc ~nongli please attach the design doc'
1,'randomwriter should complain if there are too many arguments A user was moving from 013 to 014 and was invoking randomwriter with a config on the command line like bin hadoop jar hadoop examplesjar randomwriter output confxml which worked in 013 but in 014 it ignores the confxml without complaining The equivalent is bin hadoop jar hadoop examplesjar randomwriter conf confxml output'
1,'Add support for custom coalescers Per our discussion on the mailing list it would be nice to specify a custom coalescing policy as the current coalesce method only allows the user to specify the number of partitions and we cannot really control much The need for this feature popped up when I wanted to merge small files by coalescing them by size'
1,'Hudson should kill long running tests Hudson should kill long running tests I believe it is supposed to but doesnt quite seem to do the job if the test is really hung up It would be nice if when the timer goes off Hudson did a codekill QUITcode to try to get a thread dump and then followed that with a codekill 9code'
1,'Deprecate WriteableRPCEngine The WriteableRPCEninge depends on Javas serialization mechanisms for RPC requests Without proper checks it has be shown that it can lead to security vulnerabilities such as remote code execution eg COLLECTIONS580 HADOOP12577 The current implementation has migrated from WriteableRPCEngine to ProtobufRPCEngine now This jira proposes to deprecate WriteableRPCEngine in branch2 and to remove it in trunk'
1,'Replace Texthashcode with a better hash function for nonascii strings Should we change the hash function for Text to something that handles nonascii characters better'
1,'JarFinder getJar should delete the jar file upon destruction of the JVM Once JarFindergetJar is invoked by a client app it would be really useful to destroy the generated JAR after the JVM is destroyed by setting tempJardeleteOnExit In order to preserve backwards compatibility a configuration setting could be implemented eg testbuilddirpurgeonexit'
1,'Add feature interaction as a transformer Add feature interaction as a transformer which takes a list of vector double columns and generate a single vector column that contains the interactions multiplication among them with proper handling of feature names'
1,'Support more external data source API in SparkR Currently we support readdf writedf jsonFile parquetFile in SQLContext we should support more external data source API such as readjson readparquet readorc readjdbc readcsv and so on Some of the exist API is deprecated and will remove at Spark 20 we should also deprecate them at SparkR Note we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with Rlike style'
1,'Feature Importance for GBT Random Forests have feature importance but GBT do not It would be great if we can add feature importance to GBT as well Perhaps the code in Random Forests can be refactored to apply to both types of ensembles'
1,'Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler'
1,'Kmeans wrapper in SparkR Implement a simple wrapper in SparkR to support kmeans'
1,'sparksqlcodegenmaxCaseBranches config option We currently disable codegen for CaseWhen if the number of branches is greater than 20 in CaseWhenMAXNUMCASESFORCODEGEN It would be better if this value is a nonpublic config defined in SQLConf'
1,'PySpark TrainValidationSplitModel should support validationMetrics validationMetrics in TrainValidationSplitModel should also be supported in pysparkmltuning'
1,'Use reference counting to prevent blocks from being evicted during reads As a prerequisite to offheap caching of blocks we need a mechanism to prevent pages blocks from being evicted while they are being read With onheap objects evicting a block while it is being read merely leads to memoryaccounting problems because we assume that an evicted block is a candidate for garbagecollection which will not be true during a read but with offheap memory this will lead to either data corruption or segmentation faults To address this we should add a referencecounting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely I propose to do this in two phases first add a safe conservative approach in which all BlockManagerget calls implicitly increment the reference count of blocks and where tasks references are automatically freed upon task completion This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions In phase two we should incrementally add release calls in order to fix the eviction of unreferenced blocks The latter change may need to touch many different components which is why I propose to do it separately in order to make the changes easier to reason about and review'
1,'Add metrics and source for external shuffle service ExternalShuffleService is essential for spark In order to better monitor shuffle service we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system'
1,'add a subcommand for gridmix gridmix shouldnt require a raw java command line to run'
1,'Make MutableRates metrics threadlocal write aggregateonread Currently the MutableRates metrics  serializes all writes to metrics it contains because of its use of MetricsRegistryadd ie even two increments of unrelated metrics contained within the same MutableRates object will serialize wrt each other This  is used by RpcDetailedMetrics which may have many hundreds of threads contending to modify these metrics Instead we should allow updates to unrelated metrics objects to happen concurrently To do so we can let each thread locally collect metrics and on a snapshot aggregate the metrics from all of the threads I have collected some benchmark performance numbers in HADOOP13747 which indicate that this can bring significantly higher performance in high contention situations'
1,'Expose VectorUDT MatrixUDT in a public API Both VectorUDT and MatrixUDT are private APIs because UserDefinedType itself is private in Spark However in order to let developers implement their own transformers and estimators we should expose both types in a public API to simply the implementation of transformSchema transform etc Otherwise they need to get the data types using reflection Note that this doesnt mean to expose VectorUDT MatrixUDT classes We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type There are two ways to implement this following DataTypesjava in SQL so Java users doesnt need the extra Define DataTypes in Scala'
1,'Add Python API for mlfeatureCountVectorizer Add Python API user guide and example for mlfeatureCountVectorizerModel'
1,'Need to add fs shim to use QFS Quantcast has released QFS 10 a C++ distributed filesystem based on Kosmos File System KFS QFS comes with various feature performance and stability improvements over KFS A hadoop fs shim needs be added to support QFS through qfs URIs'
1,'Support async call retry and failover In current Async DFS implementation file system calls are invoked and returns Future immediately to clients Clients call Future get to retrieve final results Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB ProtobufRpcEngine and ipcClient The callback path bypasses the original retry layer logic designed for synchronous DFS This proposes refactoring to make retry also works for Async DFS'
1,'Optimize SerializeFromObject function for primitive array In logical plan SerializeFromObject for an array always use GenericArrayData as a destination UnsafeArrayData could be used for an primitive array This is a simple approach to solve issues that are addressed by SPARK16043 Here is a motivating example'
1,'JoinedRowanyNull should delegate to the underlying rows JoinedRowanyNull currently loops through every field to check for null which is inefficient if the underlying rows are UnsafeRows It should just delegate to the underlying implementation'
1,'add a new extending interface in Analyzer for posthoc resolution To implement DDL commands we added several analyzer rules in sql hive module to analyze DDL related plans However our Analyzer currently only have one extending interface extendedResolutionRules which defines extra rules that will be run together with other rules in the resolution batch and doesnt fit DDL rules well because DDL rules may do some checking and normalization but we may do it many times as the resolution batch will run rules again and again until fixed point and its hard to tell if a DDL rule has already done its checking and normalization Its fine because DDL rules are idempotent but its bad for analysis performance some DDL rules may depend on others and its pretty hard to write if conditions to guarantee the dependencies It will be good if we have a batch which run rules in one pass so that we can guarantee the dependencies by rules order'
1,'Support intersect or except functions in Hive SQL'
1,'Ability to clean up subprocesses spawned by Shell when the process exits The runCommand code in Shelljava can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned We need to allow for the subprocess to be interrupted and killed when the shell process gets killed Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed'
1,'Spark 20 SQL API audit This is an umbrella ticket to list issues I found with APIs for the 20 release'
1,'Support shuffle spill encryption in Spark Like shuffle file encryption in SPARK5682 spills data should also be encrypted'
1,'Add option to accept quoting of all character backslash quoting mechanism We can provide the option to choose JSON parser can be enabled to accept quoting of all character or not'
1,'Create a utility to convert binary sequence and compressed files to strings It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings It could then be hooked up to bin hadoop fs cat and the web ui to textify sequence and compressed files'
1,'S3Credentials should support use of CredentialProvider Right now S3Credentials only works with cleartext passwords in configs as a secret access key or the URI The nonURI version should use credential providers with a fallback to the clear text option'
1,'Add a DataFrame API that provides functionality similar to HiveQLs DISTRIBUTE BY DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications'
1,'Datasetoriented API evolution in Spark 20 As part of Spark 20 we want to create a stable API foundation for Dataset to become the main userfacing API in Spark This ticket tracks various tasks related to that The main high level changes are Merge Dataset DataFrame Create a more natural entry point for Dataset SQLContext HiveContext are not ideal because of the name SQL Hive and SparkContext is not ideal because of its heavy dependency on RDDs First  support for sessions First  support for some system catalog See the design doc for more details'
1,'Add API for updateStateByKey to provide batch time as input The StateDStream currently does not provide the batch time as input to the state update function This is required in cases where the behavior depends on the batch start time We Conviva have been patching it manually for the past several Spark versions but we thought it might be useful for others as well'
1,'SparkR Implement repartitionByColumn method on DataFrame Implement repartitionByColumn on DataFrame This will allow us to run R functions on each partition identified by column groups with dapply method'
1,'Extensions to FsShell Our project Pig exposes FsShell functionality to our end users through a shell command We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign for instance removing a nonexistent directory We have 2 asks related to this issue Meaningful error code returned from FsShell we use java  so that we can take different actions on different errors Unix like ways to tell the command to ignore certain behavior Here are the commands that we would like to be expanded implemented'
1,'Make sparkml KMeansModel methof load backwards compatible SPARK14646 makes KMeansModel store the clusters one per row KMeansModelload method needs to be updated in order to load models saved with Spark 16'
1,'Make hadoop dockerfile usable by Yetus It would be better if Hadoops dockerfile could be used by Yetus so that external dependencies are owned by the project'
1,'Faster LDAP group name resolution with ActiveDirectory The typical LDAP group name resolution works well under typical scenarios However we have seen cases where a user is mapped to many groups in an extreme case a user is mapped to more than 100 groups The way its being implemented now makes this case super slow resolving groups from ActiveDirectory The current LDAP group resolution implementation sends two queries to a ActiveDirectory server The first query returns a user object which contains DN distinguished name The second query looks for groups where the user DN is a member If a user is mapped to many groups the second query returns all group objects associated with the user and is thus very slow After studying a user object in ActiveDirectory I found a user object actually contains a memberOf field which is the DN of all group objects where the user belongs to Assuming that an organization has no recursive group relation that is a user A is a member of group G1 and group G1 is a member of group G2 we can use this properties to avoid the second query which can potentially run very slow I propose that we add a configuration to only enable this feature for users who want to reduce group resolution time and who does not have recursive groups so that existing behavior will not be broken'
1,'Considering output for statistics of logical plan The current implementation of statistics of UnaryNode does not considering output for example Project we should considering it to have a better guess'
1,'Add codegen for Elt function Elt function doesnt support codegen execution It is better to provide the support'
1,'testpatch should run tests with fn to avoid masking test failures If there are known failures testpatch will bail out as soon as it sees them This causes the precommit builds to potentially not find real issues with a patch because the tests that would fail might come after a known failure We should add fn to just the mvn test command in testpatch to get the full list of failures'
1,'expose calculated paths It would be useful for 3rd party apps to know the locations of things when hadoop is running without explicit path env vars set'
1,'Vectorize parquet decoding using ColumnarBatch Parquet files benefit from vectorized decoding ColumnarBatches have been designed to support this This means that a single encoded parquet column is decoded to a single ColumnVector'
1,'Add toLocalIterator method for Dataset The toLocalIterator of RDD is super slow we should have a optimized implementation for Dataset DataFrame'
1,'Apply fast serialization on collect limit Recently the fast serialization has been introduced to collecting DataFrame Dataset The same technology can be used on collect limit operator too'
1,'sparkml API for linear SVM Provide API for SVM algorithm for DataFrames I would recommend using OWLQN rather than wrapping sparkmllibs SGDbased implementation The API should mimic existing sparkmlclassification APIs'
1,'JMX Context for Metrics The way metrics are currently exposed to the JMX in the NameNode is not helpful since only the current counters in the record can be fetched and without any context those number mean little For example the number of files created equal to 150 only means that in the last period there were 150 files created but when the new period will end is unknown so fetching 150 again will either mean another 150 files or we are fetching the same time period One of the solutions for this problem will be to have a JMX context that will accumulate the data being child  of AbstractMetricsContext and expose different records to the JMX through custom MBeans This way the information fetched from the JMX will represent the state of things in a more meaningful way'
1,'Python API for Generalized Linear Regression Summary We should add an interface to the GLR summaries in Python for feature parity'
1,'Store serialized blocks as multiple chunks in MemoryStore Instead of storing serialized blocks in individual ByteBuffers the BlockManager should be capable of storing a serialized block in multiple chunks each occupying a separate ByteBuffer This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks Our current serialization code uses a ByteBufferOutputStream which doubles and reallocates its backing byte array this increases the peak memory requirements during serialization since we need to hold extra memory while expanding the array In addition we currently dont account for the extra wasted space at the end of the ByteBuffers backing array so a 129 megabyte serialized block may actually consume 256 megabytes of memory After switching to storing blocks in multiple chunks well be able to efficiently trim the backing buffers so that no space is wasted This change is also a prerequisite to being able to cache blocks which are larger than 2GB although full support for that depends on several other changes which have not bee implemented yet'
1,'Implement collection functions in SparkR Collection functions documented at are size explode arraycontains and sortarray size explode are already implemented arraycontains and sortarray are to be implemented'
1,'Undeprecate createNonRecursive FileSystem createNonRecursive is deprecated However there is no DistributedFileSystem create implementation which throws exception if parent directory doesnt exist This limits clients migration away from the deprecated method For HBase IO fencing relies on the behavior of FileSystem createNonRecursive Variant of create method should be added which throws exception if parent directory doesnt exist'
1,'Support for pushing down filters for decimal and timestamp types in ORC Currently filters for TimestampType and DecimalType are not being pushed down in ORC data source although ORC filters support both'
1,'When the FileTailingAdaptor is unable to read a file it should take action instead of trying 300 times In the FileTailingAdaptor if when trying to read a file a File does not exist & Permission denied exception is throws then that file should be log and removed'
1,'Implement dropDuplicates method of DataFrame in SparkR distinct and unique drop duplicated rows on all columns While dropDuplicates can drop duplicated rows on selected columns'
1,'DiskChecker should perform some disk IO DiskChecker can fail to detect total disk controller failures indefinitely We have seen this in real clusters DiskChecker performs simple permissionsbased checks on directories which do not guarantee that any disk IO will be attempted A simple improvement is to write some data and flush it to the disk'
1,'Add debugonly socket source in Structured Streaming This is a debugonly version of SPARK15842 for tutorials and debugging of streaming apps it would be nice to have a textbased socket source similar to the one in Spark Streaming It will clearly be marked as debugonly so that users dont try to run it in production applications because this type of source cannot provide HA without storing a lot of state in Spark'
1,'UGI should contain authentication method The UserGroupInformation should contain authentication method in its subject This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients'
1,'Avoid perrecord type dispatch in JDBC when writing Currently JdbcUtilssavePartition is doing typebased dispatch for each row to write appropriate values So appropriate writers can be created first according to the schema and then apply them to each row This approach is similar with CatalystWriteSupport'
1,'Disable comments in generated code in order to avoid performance issues'
1,'Estimator interface for generalized linear models GLMs In Spark 16 MLlib provides logistic regression and linear regression with L1 L2 elasticnet regularization We want to expand the support of generalized linear models GLMs in 20 eg Poisson Gamma families and more link functions SPARK implements a GLM solver for the case when the number of features is small We also need to design an interface for GLMs In SparkR we can simply follow glm or glmnet On the Python Scala Java side the interface should be consistent with LinearRegression and LogisticRegression eg It would be great if LinearRegression and LogisticRegression can reuse code from GeneralizedLinearModel'
1,'Cleanup TestFilterFileSystem Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem This also cleans up the current  that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods'
1,'Open up already widelyused APIs for delegationtoken fetching & renewal to ecosystem projects Storm would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS STORM346 But to do so we need to open up access to some of APIs Most notably FileSystemaddDelegationTokens Tokenrenew CredentialsgetAllTokens and UserGroupInformation but there may be others At a minimum adding in storm to the list of allowed API users But ideally making them public Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR or tools that reuse MR input formats'
1,'LocalDirAllocator should avoid holding locks while accessing the filesystem As noted in MAPREDUCE5584 and HADOOP7016 LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations'
1,'Deprecate registerTempTable and add datasetcreateTempView Our current datasetregisterTempTable does not actually materialize data So it should be considered as creating a temp view We can deprecate it and create a new method called datasetcreateTempView replaceIfExists Boolean The default value of replaceIfExists should be false For registerTempTable it will call datasetcreateTempView replaceIfExists true'
1,'Add authenticated TokenIdentifiers to UGI so that they can be used for authorization When token is used for authentication over RPC information other than username may be needed for access authorization This information is typically specified in TokenIdentifier This is especially true for block tokens used for clienttodatanode accesses where authorization is based on access permissions specified in TokenIdentifier and not on username Block tokens used to be called access tokens and one can think of them as capability tokens See HADOOP4359 for more info'
1,'Kerberos relogin interval in UserGroupInformation should be configurable Currently the check done in the hasSufficientTimeElapsed method is hardcoded to 10 mins wait The wait time should be driven by configuration and its default value for clients should be 1 min'
1,'Exclude weak ciphers in SSLFactory through sslserverxml HADOOP12668 added support to exclude weak ciphers in HttpServer2 which is good for name nodes But data node web UI is based on Netty which uses SSLFactory and does not read sslserverxml to exclude the ciphers We should also add the same support for Netty for consistency I will attach a full patch later'
1,'Support scatter chart for HICC HICC chart only supports line graph at the moment For looking at large dataset charts it would be better to represent the data in scatter graph format'
1,'Read ADLS credentials from Credential Provider Read ADLS credentials using Hadoop CredentialProvider API'
1,'The distance between sync blocks in SequenceFiles should be configurable Currently SequenceFiles put in sync blocks every 2000 bytes It would be much better if it was configurable with a much higher default 1mb or so'
1,'Noninterleaved checksums would optimize block transfers Currently when a block is transfered to a datanode the client interleaves data chunks with the respective checksums This requires creating an extra copy of the original data in a new buffer interleaved with the crcs We can avoid extra copying if the data and the crc are fed to the socket one after another'
1,'security implementation for new FileSystem FileContext API New file system API HADOOP 4952 should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations MAPREDUCE 2020'
1,'Implement a native OS runtime for Hadoop It would be useful to implement a JNIbased runtime for Hadoop to get access to the native OS runtime This would allow us to stop relying on execing bash to get access to information such as usergroups process limits etc and for features such as chown chgrp orgapachehadooputilShell'
1,'testpatch needs to verify Herriot integrity Whenever a new patch is submitted for verification testpatch process has to make sure that none of Herriot bindings were broken'
1,'Allow RPC scheduler callqueue backoff using response times Currently back off policy from HADOOP10597 is hard coded to base on whether call queue is full This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities'
1,'testpatchs issue matching regex should be configurable we have the issue matching regex configurable via altering the default add in a cli arg to update it on invocation'
1,'Add capability to turn on security in unit tests We should be able to start a kdc server for unit tests so that security could be turned on This will greatly improve the coverage of unit tests'
1,'When a serializer  is missing return null not throw an NPE When you have a key value  thats non Writable and you forget to attach ioserializers for the same an NPE is thrown by the tasks with no information on why or whats missing and what led to it I think a better exception can be thrown by SerializationFactory instead of an NPE when a  is not found accepted by any of the loaded ones'
1,'Hadoop Token Command This JIRA is to define commands for Hadoop token The scope of this task is highlighted as following Token init authenticate and request an identity token then persist the token in token cache for later reuse Token display show the existing token with its info and attributes in the token cache Token revoke revoke a token so that the token will no longer be valid and cannot be used later Token renew extend the lifecycle of a token before its expired'
1,'Add haadmin getAllServiceState option to get the HA state of all the NameNodes ResourceManagers Currently we have one command to get state of namenode It will be good to have command which will give state of all the namenodes'
1,'Chukwa Add a config parameter to allow agent to talk to the same collector until connection fails In chukwa agent we added ability to rotate collector on each HTTP post RAD Lab is using Chukwa Agent differently and would like to have the option to keep agent on the same collector On the smaller scale of the Chukwa it makes sense to process data without the map reduce job A new interface needs to be created as a T pipe line to stream data from collector directly into a Database or external storage In this configuration it is better to channel similar data to the same collector to reduce the cost of reassembling chunks'
1,'IPC layer optimizations Umbrella jira for y! optimizations to reduce object allocations more efficiently use protobuf APIs unified ipc and webhdfs callq to enable QoS etc'
1,'Allow daemon startup when at least 1 or configurable disk is in an OK state The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided I have defined multiple local disks defined for a datanode When one of those disks breaks and is unmounted then the mountpoint such as data 03 in this example becomes a regular directory which doesnt have the valid permissions and possible directory structure Hadoop is expecting When this situation happens the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed The only way around this is to alter the configuration and omit that specific disk configuration To my opinion It would be more practical to let Hadoop daemons start when at least 1 disks partition in the provided list is in a usable state This prevents having to roll out custom configurations for systems which have temporarily a disk and therefor directory layout missing This might also be configurable that at least X partitions out of he available ones are in OK state'
1,'Add support for hadooprc The system should be able to read in userdefined env vars from ~ hadooprc'
1,'Restore security in Hadoop 022 branch This is to track changes for restoring security in 022 branch'
1,'FileSystem listStatus should throw IOE upon access error In HADOOP6201 and HDFS538 it was agreed that FileSystem listStatus should throw FileNotFoundException instead of returning null when the target directory did not exist However in LocalFileSystem implementation today FileSystem listStatus still may return null when the target directory exists but does not grant read permission This causes NPE in many callers for all the reasons cited in HADOOP6201 and HDFS538 See HADOOP7327 and its linked issues for examples'
1,'Disable spurious checkstyle checks Some of the checkstyle checks are not realistic like the line length leading to spurious 1 in precommit Lets disable'
0,'Adding a GROUP BY 1 where first column is literal results in wrong answer Here this fails the second assertion by returning a single row It appears that running group by 1 where column 1 is a constant causes filter conditions to be ignored Both PostgreSQL and SQLite return empty result sets for the query containing the GROUP BY'
0,'In the cast expression casting from empty string to interval type throws NullPointerException When the cast expression is applied on empty string to cast it to interval type it throws Null pointer exceptionGetting the same exception when I tried reproducing the same through test case checkEvaluation Cast Literal CalendarIntervalType null'
0,'Spark on mesos is broken due to race condition in Logging End error is MySQLSyntaxErrorException'
0,'Web UI prevents sparksubmit application to be finished The application creates a thread with infinite loop for web UI communication and never stops it The application is waiting for the thread to be finished instead even if you close the web page'
0,'Cannot use SparkR sql Caching multiple replicas of blocks is currently broken Again doesnt replicate data and executors show the same ClassNotFoundException'
0,'Add test suite for EliminateSubQueries'
0,'Broken Spark SQL Codegen this is me on purpose trying to break spark sql codegen to uncover potential issues by creating arbitrately complex data structures using primitives strings basic collections map seq option tuples and case classes'
0,'Current TreeNodetoJSON may trigger OOM under some corner cases In SPARK17356 we fix the OOM issue when Metadata is super big There are other cases that may also trigger OOM Current implementation of TreeNodetoJSON will recursively search and print all fields of current TreeNode even if the fields type is of type Seq or type Map The following example triggers a StackOverflowError when calling toJSON on a plan with user defined UDF'
0,'ExecutorId in HearbeatReceiverSuite is incorrect This unit test can pass currently because of askWithRetry when catching exception RPC will call again thus it will go if branch and return true'
0,'Json serialzation of accumulators are failing with ConcurrentModificationException This is the stack trace See ConcurrentModificationException'
0,'Use best time and average time in micro benchmark Best time should be more stable than average time in benchmark together with average time they could show more information'
0,'Python UDF failed when there is no arguments For an application with YarnApplicationStateFINISHED and FinalApplicationStatusFAILED invoking sparksubmit from command line will got Exception Also because the above fact in test YarnClusterSuite assert with false condition will not+ fail the test'
0,'Spark prints an avalanche of warning messages from Parquet when reading parquet files written by older versions of Parquetmr It looks like broke parquet log output redirection After that patch when querying parquet files written by Parquetmr 160 Spark prints a torrent of harmless warning messages from the Parquet reader This only happens during execution not planning and it doesnt matter what log level the SparkContext is set to This is a regression I noted as something we needed to fix as a follow up to PR 14690 I feel responsible so Im going to expedite a fix for it I suspect that PR broke Sparks Parquet log output redirection Thats the premise Im going by'
0,'Spark worker throw Exception when uber jars http url contains query string If the url contains any query strings downloadUserJar method will throw Did not see expected jar exception This is a problem when your jar is located on some web service which requires some additional information to retrieve the file For example to download a jar from s3 bucket via http the url contains signature datetime etc as query string Hence all the query string should be removed before checking jar existance I created a pr to fix this if anyone can review it'
0,'Throw Exceptions for DDLs of Partitioned Views CREATE VIEW and ALTER VIEW Because the concept of partitioning is associated with physical tables we disable all the supports of partitioned views which are defined in the following three commands in Hive DDL Manual An exception is thrown when users issue any of these three DDL commands'
0,'dropDuplicates uses the same expression id for Alias and Attribute and breaks attribute replacement Right now if you use dropDuplicates in a stream you get an exception because attribute replacement is broken'
0,'ML GaussianMixture training failed due to feature column type mistake ML GaussianMixture training failed due to feature column type mistake The feature column type should be mllinalgVectorUDT but got mlliblinalgVectorUDT by mistake This bug is easy to reproduce by the following code Why the unit tests did not complain this errors'
0,'Document Spark 16s offheap memory configurations and add config validation We need to document the new offheap memory limit configurations which were added in Spark 16 add simple configuration validation for instance you shouldnt be able to enable offheap execution when the offheap memory limit is zero and alias the old and confusing sparkunsafeoffHeap configuration to something that lives in the sparkmemory namespace'
0,'Roundtrip encoding of array struct fields is wrong when wholestage codegen is disabled The following failing test demonstrates a bug where Spark misencodes arrayofstruct fields if wholestage codegen is disabled When wholestage codegen is enabled the default this works fine'
0,'Fix sorting of part files while reconstructing RDD partition from checkpointed files If we exceed this no the sort logic in ReliableCheckpointRDD gets messed up and fails This is because of partfiles are sorted and compared as strings Possible solutions Bump the padding to allow more partitions or Sort the part files extracting a subportion as string and then verify the RDD'
0,'Temporary shuffle data files may be leaked following exception in write SPARK8029 modified shuffle writers to first stage their data to a temporary file in the same directory as the final destination file and then to atomically rename the file at the end of the write job However this change introduced the potential for the temporary output file to be leaked if an exception occurs during the write because the shuffle writers existing error cleanup code doesnt handle this new temp file'
0,'Capture errors from R workers in daemonR to avoid deletion of R session temporary directory If we have a linkage error in the user code Spark executors get killed immediately This is not great for user experience especially in shared environments in which multiple applications are multiplexed through one context'
0,'Spark distributed cache should throw exception if same file is specified to dropped in files archives Recently for the changes to SPARK14423 Handle jar conflict issue when uploading to distributed cache If by default yarn client will upload all the files and archives in assembly to HDFS staging folder It should throw if file appears in both files and archives exception to know whether uncompress or leave the file compressed'
0,'Collapse Window optimizer rule changes column order The recently added CollapseWindow optimizer rule changes the column order of attributes This actually modifies the schema of the logical plan which optimization should not do and breaks collect in a subtle way we bind the row encoder to the output of the logical plan and not the optimized plan'
0,'SparkR glm Gamma family results in error'
0,'Erroneous computation in multiplication of transposed SparseMatrix with SparseVector There is a bug in how a transposed SparseMatrix isTransposedtrue does multiplication with a SparseVector The bug is present for v 200 This bug can be verified by running the following snippet in a Spark shell here using v161'
0,'orgapachesparkmlliblinalgVectorUDT cannot be cast to orgapachesparksqltypesStructType The issue in subject happens on attempt to transform DataFrame in Parquet format into ORC while DF contains SparseVector DenseVector data It looks like that there shouldnt be any serialization issues but they happens'
0,'Wrong ApproximatePercentile answer when multiple records have the minimum value When multiple records have the minimum value the answer of ApproximatePercentile is wrong'
0,'AssertOnQuerycondition should be consistent in requiring Boolean return type AssertOnQuery has two apply constructor one that accepts a closure that returns boolean and another that accepts a closure that returns Unit This is actually very confusing because developers could mistakenly think that AssertOnQuery always require a boolean return type and verifies the return result when indeed the value of the last statement is ignored in one of the constructors'
0,'Jenkins should verify mvn site if the patch contains aptvm changes Its should be good to make Jenkins verify mvn site if the patch contains aptvm changes to avoid some obvious build failure such as YARN2732 Its not the first time that the similar issues have been raised Having an automative verification can inform us an alert before us encounter an actual build failure which involves site lifecycle'
0,'make the default params in sparkR sparkmlp consistent with MultilayerPerceptronClassifier several default params in sparkR sparkmlp is wrong layers should be null tol should be 1e6 stepSize should be 003 seed should be 763139545'
0,'RM fails to start in nonsecure mode due to authentication filter failure RM fails to start in the nonsecure mode with the following exception This is likely a regression introduced by HADOOP10670'
0,'constraints propagation may fail the query The following simple SQL query reproduces this issue Exception thrown This bug is a regression Spark 16 doesnt have this issue'
0,'DistributedLDAModel returns different logPrior for original and loaded model While adding DistributedLDAModel training summary for SparkR I found that the logPrior for original and loaded model is different'
0,'RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses noncurly quotes Symlink tests failure happened from time to time Failed Error Message Path is not a symbolic link Stacktrace javaioIOException'
0,'Spark 20 CSV does not cast null values to certain data types properly If we create a table pointing to a parquet json datasets without specifying the schema describe table command does not show the schema at all It only shows Schema of this table is inferred at runtime'
0,'Remove the version and author information from distcps README file That text shouldnt be there'
0,'Even timeline for a stage doesnt core 100 of the bar timeline bar in chrome Actually proportions are calculated correctly but due to css issue theyre not expanded to the size of the full bar'
0,'Document LIBSVM data source options in public doc and minor improvements We should document options in public API doc Otherwise it is hard to find out the options without looking at the code'
0,'YARN shuffle service should throw errors when it fails to start Spark applications running on Mesos throw exception upon exit as follows Applications result is not affected by this error'
0,'Utilsscala terminateProcess should call ProcessdestroyForcibly if and only if Processdestroy fails When we do not turn on the Hive Support the following query generates a confusing error message'
0,'Oracle JDBC table creation fails with ORA00902 invalid datatype Please see the attached notebook Seems lag lead somehow fail to recognize that a offset row does not exist and generate wrong results'
0,'Spark SQL Catalyst doesnt handle ISO 8601 date without colon in offset When parsing a CSV with a date time column that contains a variant ISO 8601 that doesnt include a colon in the offset casting to Timestamp fails'
0,'JDK8 Fix javadoc errors caused by incorrect or illegal tags in hadooptools mvn package Pdist DskipTests fails with JDK8 caused by incorrect or illegal tags in doc comments'
0,'hadoop fs text of zerolength file causes EOFException'
0,'Analysis error for DataSet typed selection If U1 contains subfields then it reports AnalysisException'
0,'Incorrect usage of config parameters in token manager of KMS The usage of the following configs of Key Management Server KMS are problematic'
0,'Incorrectly Set Nullability to False in FilterExec When FilterExec contains isNotNull which could be inferred and pushed down or users specified we convert the nullability of the involved columns if the toplayer expression is nullintolerant However this is not true if the toplayer expression is not a leaf expression it could still tolerate the null when it has nulltolerant child expression When the nullability is wrong we could generate incorrect results in different cases'
0,'IS NOT NULL clause gives false for nested not empty column The wrong value numCompletedTasks has been assigned to the variable numSkippedTasks'
0,'Unable to retrieve data from a parquet table whose name starts with underscore It looks like there is some bug introduced in Spark 210 preventing to read data from a parquet table hive support is enabled whose name starts with underscore CREATE and INSERT statements on the same table instead seems to work as expected The problem can be reproduced from sparkshell through the following steps'
0,'establish a Powered by Hadoop logo We should agree on a Powered By Hadoop logo'
0,'prepare architecture documentation'
0,'JDBC source Wrong Partition Generation when numPartitions is More than the number of rows between upper and lower bounds Pythononly UDTs cant work well'
0,'CREATE TABLE LIKE generates a nonempty table when source is a data source table When the source table is a data source table the table generated by CREATE TABLE LIKE is nonempty The expected table should be empty'
0,'Fixed Insert Failure To Data Source Tables when the Schema has the Comment Field The insert attempt will fail if the target table has a column with comments The error is strange to the external users'
0,'pyspark reduceByKeyAndWindow with invFuncNone requires checkpointing When invFunc argument of reduceByKeyAndWindow is None checkpointing should not be required Scala implementation of reduceByKeyAndWindow handles this correctly and does not require checkpointing Python version requires that checkpoint directory be specified it is a bug'
0,'Some SQL metrics is broken when wholestage codegen enabled The spill size of Aggregate and Sort is broken the data size is actual peak memory'
0,'toLocalIterator yields time out error on pyspark2 I run the example straight out of the api docs for toLocalIterator and it gives a time out exception'
0,'SSL redirect handler only redirects the servers root The redirect handler that is started in the HTTP port when SSL is enabled only redirects the root of the server Additional handlers do not go through the handler so if you have a deep link to the nonhttps server you wont be redirected to the https port I tested this with the history server but it should be the same for the normal UI the fix should be the same for both too'
0,'spark should be able to control the number of executor and should not throw stack overflow When running Sql queries on large datasets Job fails with stack overflow warning and it shows it is requesting lots of executors Looks like there is no limit to number of executors or not even having an upperbound based on yarn available resources If you notice in the error above YARN is trying to request 24576 executor containers whereas the available cores are 1719 The Driver is requesting for 52902 executor s which too high This exception should be fixed'
0,'Exchange reuse incorrectly reuses scans over different sets of partitions This happens because the file scan operator does not take into account partition pruning in its implementation of sameResult As a result executions may be incorrect on selfjoins over the same base file relation Heres a minimal test case to reproduce'
0,'No encoder implicits for SeqPrimitive Dataset aggregators with complex types fail with unable to find encoder for type stored in a Dataset Though Datasets with these complex types are supported'
0,' grows beyond 64 KB I have 2 wide dataframes that contain nested data structures when I explode one of the dataframes it doesnt include records with an empty nested structure outer explode not supported So I create a similar dataframe with null values and union them together See SPARK13721 for more details as to why I have to do this I was hoping that SPARK16845 was going to address my issue but it does not I will attach a code snippet that can be pasted into sparkshell that duplicates my code and the exception This worked just fine in Spark 16x'
0,'JVMObjectTrackerobjMap may leak JVM objects We observed that JVM objects that are not used anymore are still trapped in this map which prevents those object get GCed Seems it makes sense to use weak reference like persistentRdds in SparkContext'
0,'fix random generator for map type'
0,'Errors thrown by UDFs cause TreeNodeException when the query has an ORDER BY clause'
0,'Add documentation for Streaming Rest API'
0,'Update documentation with instructions to enable block manager wire encryption See SPARK6229 The code was added but the docs were never updated'
0,'Some batches might not get marked as fully processed in JobGenerator Right now the YARN shuffle service will swallow errors that happen during startup and just log them This causes two undesirable things to happen'
0,'Minor doc usage changes related to removal of Spark assembly While poking around with 20 I noticed that a few places still referred to spark assembly jar so I updated where it made sense I also updated the usage section of sparksubmit since you can now use runexample argument to run an example from sparksubmit which was mostly undocumented'
0,'Python UDF may fail because of six In GBTClassificationModel the overloaded method trees casts the DecisionTree to a DecisionTreeRegressionModel however the import for this  is missing and leads to a NameError global name DecisionTreeRegressionModel is not defined'
0,'GeneralizedLinearRegression Wrong Value Range for Poisson Distribution The current implementation of Poisson GLM seems to allow only positive values See below This is not correct since the support of Poisson includes the origin'
0,'Analyzer rule for resolving using joins should respect case sensitivity setting When the underlying file changes it can be very confusing to users when they see a FileNotFoundException'
0,'Performing arithmetic in VALUES can lead to ClassCastException MatchErrors during query parsing The following example fails with a ClassCastException Its surprising to me that this error is occurring during query parsing'
0,'ClassCastException during count distinct While trying to reproduce SPARK18172 in SQL I found the following SQL query fails in Spark 201 via sparkbeeline with a CassCastException'
0,'Spark20error occurs when execute the sql statement which includes nvl function while spark16 supports However if a user only set orccompress in the writer option we should not use the default value of compression snappy as the codec Instead we should respect the value of orccompress'
0,'Test that expressions can be serialized SPARK18368 fixes regexpreplace when it is serialized One of the reviews requested updating the tests so that all expressions that are tested using checkEvaluation are first serialized That caused several new so this issue is to add serialization to the tests and fix the bugs serialization exposes'
0,'Provide more informative error message when direct parquet output committer is used and there is a file already exists error When saving data to S3 eg saving to parquet if there is an error during the query execution the partial file generated by the failed task will be uploaded to S3 and the retries of this task will throw file already exist error It is very confusing to users because they may think that file already exist error is the error causing the job failure They can only find the real error in the spark ui in the stage page'
0,'RelationalGroupedDatasetagg should be order preserving and allow duplicate column names The above implementation is not order preserving and does not allow duplicate names'
0,'DFS Summary Page DFS Summary page should consists of the DFS summary data and links to Live Datanodes and Dead datanodes instead of loading all nodes information in one page It would be great if you can give the summary in XML format also'
0,'Hadoop streaming mapper and reducer options are wrongly documented as required reducer is listed as a Required option but is a reducer still required if the Streaming job is Map only reducer should not be listed as a Required option'
0,'PySpark RDD Repartitioning Results in Highly Skewed Partition Sizes Calling repartition on a PySpark RDD to increase the number of partitions results in highly skewed partition sizes with most having 0 rows The repartition method should evenly spread out the rows across the partitions and this behavior is correctly seen on the Scala side Please reference the following code for a reproducible example of this issue The issue here is that highly skewed partitions can result in severe memory pressure in subsequent steps of a processing pipeline resulting in OOM errors'
0,'Web UI error accessing links which need authorization when Kerberos User Hadoop on secure mode Get 403 authorization errors only hdfs user could access logs Would expect as a user to be able to web interface logs link Same results if using curl HTTP 11 403 User tester is unauthorized to access this page'
0,'hadoopdist distlayoutstitchingsh does not work with dash Saw this while building the EC branch pretty sure itll repro on trunk though too'
0,'RewriteDistinctAggregates UnresolvedException when a UDAF has a foldable TypeCheck when run a sql with distinct on spark github master branch it throw UnresolvedException'
0,'Specified database in JDBC URL is ignored when connecting to thriftserver However anything specified in database results in being put in default schema Im running these with e commands but the shell shows the same behavior'
0,'After SparkSession has been created setting hadoop conf through sparkSessionsparkContexthadoopConfiguration does not affect hadoop conf used by the SparkSession This issue adds randomSplit into SparkR for API parity'
0,'Redact sensitive information from Spark logs and UI This JIRA is to track the work to make sure sensitive information is redacted from all logs and UIs in Spark while still being passed on to all relevant places it needs to get passed on to'
0,'Fix hadoopopenstack undeclared and unused dependencies Attempting to compile openstack on a fairly fresh maven repo fails due to commonshttpclient not being a declared dependency After that is fixed doing a maven dependency analyze shows other problems'
0,'ReloadingX509TrustManager should keep reloading in case of exception Chances are that the reload happens when the key store file is being written The reload fails probably with EOFException and wont load until key store filess last modified time changes A simple fix is to update the lastLoaded only when the reload succeeds ReloadingX509TrustManager will keep reloading in case of exception'
0,'Add a new scalastyle NoScalaDoc to prevent ScalaDocstyle multiline comments'
0,'Update user guide to address minor comments during code review Cleanup user guides to address some minor comments Some code examples were introduced in 12 before createDataFrame We should switch to that'
0,'Audit MiMa excludes added in SPARK13948 to make sure none are unintended incompatibilities The patch for SPARK13948 added a number of MiMa excludes for cases which were missed due to our old way of programatically generating excludes in GenerateMIMAIgnore Before Spark 20 we need to audit the additional excludes that I added to make sure that none represent unintentional incompatibilities which should be fixed'
0,'hadoopaws documentation missing In HADOOP10714 the documentation source files for hadoopaws were moved from src site to src main site The build is no longer actually generating the HTML site from these source files because src site is the expected path'
0,'History Server main page does not honor APPLICATIONWEBPROXYBASE The root of the history server is rendered dynamically with javascript and this doesnt honor APPLICATIONWEBPROXYBASE This means the links on the history server root page are broken when deployed behind a proxy'
0,'Update changelog and release notes 20160304 Added and updated changelog and release notes based upon Yetus 020SNAPSHOT'
0,'UserProvider is not thread safe While JavaKeyStoreProvider is thread safe UserProvider is not thread safe'
0,'Test harness to prevent expression code generation from reusing variable names Run below SQL and get transformation script error for python script like below error message'
0,'testpatchsh does not give any detail on its 1 findbugs warning report testpatchsh does not give any detail on its 1 warning report This has been seen in Jenkins run of HDFS8830 Summary Warning Type Number'
0,'Replace daemon with better name in script subcommands Per discussion in HDFS7204 creating this jira Thanks ~aw for the work on HDFS7204'
0,'Fix bugs in the initialization of the ISAL library JNI bindings When run hadoop checknative it also failed'
0,'InferFiltersFromConstraints rule never terminates for query The following complicated example becomes stuck in the InferFiltersFromConstraints rule and never runs However it doesnt fail with a stack overflow and doesnt hit the limit on optimization passes so I think theres some sort of nonobvious infinite loop within the rule itself'
0,'Fix a potential ExprId conflict for SubexpressionEliminationSuiteSemantic equals and hash SubexpressionEliminationSuiteSemantic equals and hash assumes the default AttributeReferences exprId wont be ExprId 1 However that depends on when this test runs It may happen to use ExprId 1'
0,'Project UnaryNode is way too aggressive in estimating statistics This has a few issues'
0,'Revert HADOOPPREFIX go back to HADOOPHOME Today Windows and parts of the Hadoop source code still use HADOOPHOME The switch to HADOOPPREFIX back in 021 or so didnt really accomplish what it was intended to do and only helped confuse the situation HOME is a much more standard suffix and is in fact used for everything in Hadoop except for the top level project home I think it would be beneficial to use HADOOPHOME in the shell code as the Official tm variable still honoring HADOOPPREFIX if it is set'
0,'add test for setting location for managed table'
0,'Add logs to help investigate the network performance It would be very helpful for network performance investigation if we log the time spent on connecting and resolving host'
0,'Ignore package line length checkstyle rule The packages related to the DockerLinuxContainerRuntime all exceed the 80 char line length limit enforced by checkstyle This causes every build to fail with a 1 I would like to exclude this rule from causing a failure'
0,'truncate will fail when we use viewfilesystem truncate will fail when use viewFS'
0,'Fix kill command behavior under some Linux distributions After HADOOP12317 kill commands execution will be failure under Ubuntu12 After NM restarts it cannot get if a process is alive or not via pid of containers and it cannot kill process correctly when RM AM tells NM to kill a container Logs from NM customized logs'
0,'Provide consistent format output for all file formats We currently rely on FileFormat implementations to override toString in order to get a proper explain output Itd be better to just depend on shortName for those'
0,'SQLContext should import DataStreamReader Running SparkR unit tests randomly has the following error The problem about forking R worker is that all forked R processes share a temporary directory as documented at Also all future R workers that will be forked from the daemon will be affected if they use tempdir or tempfile to get tempoaray files because they will fail to create temporary files under the alreadydeleted session temporary directory However this is a bug in daemonR that when there is any execution error in R workers the error handling of R will finally go into the cleanup procedure So try should be used in daemonR to catch any error in R workers so that R workers will directly exit'
0,'add new logo to project site HADOOP2810 added a new logo for Core to the released documenation The same logo should also be used on the Core project web site'
0,'RetryInvocationHandler need wrap InterruptedException in IOException when call Threadsleep RetryInvocationHandler need wrap InterruptedException in IOException when call Threadsleep Otherwise InterruptedException cant be handled correctly by other components such as HDFS'
0,'executor page fails to show log links if executors are added after an app is launched How to reproduce with standalone mode However there is no link to stdout stderr on the executor page'
0,'Code generation including too many mutable states exceeds JVM size limit Code generation including too many mutable states exceeds JVM size limit to extract values from references into fields in the constructor'
0,'Strange Errors When Creating View With Unmatched Column Num The following Java code because of type erasing We should use retag to restore the type to prevent the following exception ClassCastException'
0,'Fix sourcelevel compatibility after HADOOP11252 Since 273 release Clientget setPingInterval is changed from public to packageprivate because Giraph is one of the broken examples for this changes'
0,'Incompatible change to SortedMapWritable Hive does not compile against Hadoop280SNAPSHOT Looks like the change in HADOOP10465 causes this'
0,'Add benchmark codes for Encoder compress in CompressionSchemeBenchmark'
0,'Better error message if path is not specified We should improve the error message'
0,'javalangIndexOutOfBoundsException running query 68 Spark SQL on 100TB Running query 68 with decreased executor memory using 12GB executors instead of 24GB on 100TB parquet database using the Spark master dated 11 04 gave IndexOutOfBoundsException The query is as follows'
0,'Add link to distributions wiki page from releases page'
0,'Inpage TOC of documentation should be automatically generated by doxia macro Inpage TOC of each documentation page is maintained by hand now It should be automatically generated once doxia macro is supported by doxiamodulemarkdown'
0,'SELECT COUNT NULL OVER throws UnsupportedOperationException during analysis Running SELECT COUNT NULL OVER throws an UnsupportedOperationException during analysis Works fine my hunch is that this is uncovering a bug in the ordering of our optimizer rules or a bug in the constantfolding rule itself This particular example is probably unimportant by itself but may be an indicator of other problems'
0,'testpatchsh mv does wrong math cleanupandexit uses the wrong result code check and fails to mv the patchdir when it should and mvs it when it shouldnt'
0,'hadoopdaemonssh throws host1 bash host3 command not found hadoopdaemonssh throws command not found hadoopdaemonssh is mainly used to start the cluster for ex startdfssh Without this cluster will not be able to start'
0,'Returned an Empty Result after Loading a Hive Table The returned result is empty after table loading'
0,'InputFileBlockHolder doesnt work with Python UDF for datasource other than FileFormat For the datasource other than FileFormat such as sparkxml which is based on BaseRelation and uses HadoopRDD NewHadoopRDD InputFileBlockHolder doesnt work with Python UDF'
0,'Always Identical Name for UDF in the EXPLAIN output Although udf1 and udf2 are different UDF but the name in the plans are the same It looks confusing'
0,'Update Apache documentation regarding watermarking in Structured Streaming'
0,'Add Apache to Hadoop project logo Many ASF projects include Apache in their logo We should add it to Hadoop'
0,'Dynamic allocation race condition Containers getting marked failed while releasing While trying to reach launch multiple containers in pool if running executors count reaches or goes beyond the target running executors the container is released and marked failed This can cause many jobs to be marked failed causing overall job failure'
0,'maven 333 missing from mirror breaks older builds StreamingQueryexplain shows N A when no data arrives Its pretty confusing'
0,'TestLocalFsFCStatistics testStatisticsThreadLocalDataCleanUp times out occasionally TestLocalFsFCStatistics has been failing sometimes and when it fails it appears to be from FCStatisticsBaseTesttestStatisticsThreadLocalDataCleanUp The test is timing out when it fails'
0,'Run FileSystem contract tests with hadoopazure This issue proposes to implement the Hadoop FileSystem contract tests for hadoopazure WASB The contract tests define the expected semantics of the FileSystem so running these for hadoopazure is likely to catch potential problems and improve overall quality'
0,'WASB Logging Improve WASB Logging around deletes reads and writes Logging around the WASB component is very limited and it is disabled by default This improvement is created to add logging around Reads Writes and Deletes when Azure Storage Exception to capture the blobs that hit the exception This information is useful while communicating with the Azure storage team for debugging purposes'
0,'Create a new logo for Hadoop security related uses Weve created a new logo for Hadoop security topics that we wanted to share with the community'
0,'Fix dead link in sitexml Documents for FileSystem API definition were created in HADOOP9361 but not linked'
0,'Document hadoop properties expected to be set by the shell code in envsh There are quite a few Java properties that are expected to be set by the shell code These are currently undocumented'
0,'Spark 201 SQL Thrift Error After deploy spark thrift server on YARN then I tried to execute from the beeline following command Ive got this error message'
0,'SQL query on parquet table javalangArrayIndexOutOfBoundsException This is a weird corner case Users may hit this issue if they have a schema that Exception thrown Note that the two element fields are different This is because the nested struct field name happens to be element which is used as a dedicated name of the element type container group in the standard 3level layout and lead to the ambiguity Currently Spark 16x 200SNAPSHOT and master chose the 2nd one We can fix this issue by giving the standard 3level layout a higher priority when trying to match schema patterns'
0,'Spark SQL fails to load tables created without providing a schema I have a old table that was created without providing a schema Seems branch 21 fail to load it and says that the schema is corrupt'
0,'Bump master version to 210SNAPSHOT This should now be doable with SPARK16476'
0,'improve readability of generated code try to avoid the suffix unique id remove multiple empty lines in code formater remove the comment if there is no code generated'
0,'QuantilesSummaries returns the wrong result after compression Found the following corner case that returns the wrong quantile off by 1 The value of the LEFT column represents the output when using QuantileSummaries in Window function the value on the RIGHT column represents the expected result'
0,'Fix EventTimeWatermarkSuite delay in months and years handled correctly'
0,'ObjectHashAggregateSuite fails under Maven builds This fails ObjectHashAggregateSuite because the randomized test cases there register a temporary Hive function right before creating a test case and can be cleared while initializing other successive test cases'
0,'Bug in return value for delete calls in WASB Current implementation of WASB does not correctly handle multiple threads clients calling delete on the same file However in the current implementation even though only one thread deletes the file multiple clients incorrectly get true as the return from delete call'
0,'mvn package Pdistdocs DskipTests Dtar fails because of nonascii characters The command fails because following files include nonascii characters'
0,'Distcp with delete feature on raw data not implemented When doing distcp of raw data using delete feature following bug appears The issue is not with the distributed copy the issue is when it tries to delete things in the target that no longer exist in the source it revalidates to make sure NONE is in the reserved raw domain'
0,'Fix warnings and a failure in SparkR test cases with testthat version 101 After upgrading testthat package to version 101 new warnings and a new failure were found in SparkR test cases'
0,'Complete output mode does not output updated aggregated value in Structured Streaming When a query containing LIMIT TABLESAMPLE 0 the statistics could be zero Results are correct but it could cause a huge performance regression The statistics of both df and df2 are zero The statistics values should never be zero otherwise sizeInBytes of BinaryNode will also be zero'
0,'Fetch large directly result from executor is very slow Given two task with 100+M result on each it take more than 50 seconds to fetch the results The RPC may be not designed to handle large block we should use block manager for that But currently this is based on sparkrpcmessagemaxSize which is usually very large 128M for safe its too large for handling results We also counting the time to fetch the direct result also deserialize it as schedule delay it also make sense to only fetch much smaller blocks via DirectResult'
0,'better error message for exceptions during ScalaUDF execution'
0,'Incomplete algorithm for name resolution in Catalyst paser may lead to incorrect result While investigating SPARK16951 I found an incorrect results case from a NOT IN subquery I thought originally it is an edge case'
0,'Document safely option of rm command HADOOP12358 introduced safely option to prevent accidental deletion of large directories with lots of files with rmr skipTrash'
0,'website should link to ASF sponsor page The Hadoop website should link to the following pages This should be done in the boilerplate of the site so that every page links to these pages This is a requirement of all ASF sites'
0,'Tracing in IPC Server is broken Looks like the test has been failing since HADOOP13438 was committed'
0,'StorageException complaining no lease ID during HBase distributed log splitting This is similar to HADOOP11523 but in a different place During HBase distributed log splitting multiple threads will access the same folder called recoverededits However lots of places in our WASB code did not acquire lease and simply passed null to Azure storage which caused this issue'
0,'Regression Hive variables no longer work in Spark 20 The behavior of variables in the SQL shell has changed from 16 to 20 Specifically hivevar namevalue and SET hivevar namevalue no longer work Queries that worked correctly in 16 will either fail or produce unexpected results in 20 so I think this is a regression that should be addressed In 20 hiveconf sparkconf and conf variable prefixes are all removed then the value in SQLConf for the rest of the key is returned SET adds properties to the session config and the Hadoop configuration during I O'
0,'Incorrect error message by fs put local dir without permission When the user doesnt have access permission to the local directory the hadoop fs put command prints a confusing error message No such file or directory It will be more informative if the message If the source is a local file the error message is ok'
0,'JDK8 Fix javadoc errors caused by incorrect or illegal tags mvn package Pdist DskipTests fails with JDK8 by illegal tags'
0,'JVM stdout output is dropped in SparkR Whenever there are stdout outputs from Spark in JVM typically when calling println they are dropped by SparkR'
0,'TestMetricsSystemImpl testQSize occasionally fail I have seen this test failed a few times in the past Error Message metricsSinkputMetrics'
0,'use svn eolstyle native for html to prevent line ending issues shows up as modified even though I havent touched it and I cant check it out or reset to a previous version to make that go away The only thing I can do to neutralize it is to put it in a dummy commit but I have to do this every time I switch branches or rebase'
0,'CredentialProviderFactory fails at  loading from libhdfs JNI This bug was discovered when trying to run Impala libhdfsso with s3a and Java KeyStore credentials Because JNI threads have a different classloader bootstrap we fail to load JavaKeyStoreProvider'
0,'NumberFormatException when reading csv for a nullable column Having a schema with a nullable column thrown an javalangNumberFormatException null when the data + delimeter isnt specified in the csv The NullpointerException is then given to the CSVTypeCastcastTo datum String as the datum value The subsequent NumberFormatException is thrown due to the fact that a NullpointerException cannot be cast into the Type'
0,'PhantomReference for filesystem statistics can trigger OOM I saw an OOM that appears to have been caused by the phantom references introduced for file system statistics management'
0,'inputfilename function does not work with UDF does not return the file name but empty string instead when it is used as input for UDF in PySpark as below This seems PySpark specific issue'
0,'Fix some release build issues Found some build issues while doing some test runs with the createreleasesh script'
0,'Using HiveContext after recreating SparkContext in Spark 20 throws JavalangillegalStateException Cannot call methods on a stopped sparkContext After stopping SparkSession if we recreate it and use HiveContext in it it will throw error JavalangillegalStateException Cannot call methods on a stopped sparkContext Above error occurs only in case of Pyspark not in SparkShell'
0,'Its hard for the user to see the failure if StreamExecution fails to create the logical plan If the logical plan fails to create eg some Source options are invalid the user cannot use the code to detect the failure The only place receiving this error is Threads UncaughtExceptionHandler This bug is because logicalPlan is lazy and when we try to create StreamingQueryException to wrap the exception thrown by creating logicalPlan it calls logicalPlan agains'
0,'ListingFileCatalog does not list in parallel anymore This unfortunately hit a genjavadoc bug and broken doc generation'
0,'StorageException complaining no lease ID when updating FolderLastModifiedTime in WASB This is a similar issue as HADOOP11523 and HADOOP12089 which I found in a customers HBase cluster logs but the piece of code is in a different place'
0,'Exception may be swallowed in KMSClientProvider Exception may be swallowed when close highlighted in the code throws exception BTW I think we should be able to consolidate the two isclose in the above code so we dont close the same stream twice The one in the finally block may be called after an exception is thrown or not and it may throw exception too we need to be careful not to swallow exception here too'
0,'Cannt read broadcast if broadcast blocks are stored ondisk How to reproduce it Exception will throw since SPARK17503 if a broadcast cannot cache in memory The reason is that that change cannot cover'
0,'Fix hadoop patch testing using jiracli tool'
0,'CheckAnalysis rejects TPCDS query 32 It seems the CheckAnalysis rule introduced by SPARK18504 is incorrect rejecting this TPCDS query which ran fine in Spark 20 There doesnt seem to be any obvious error in the query or the check rule though in the plan below the scalar subquerys condition field is which should reference csitemsk 39 Nonetheless CheckAnalysis complains that csitemsk 39 is not referenced by the scalar subquery predicates'
0,'indexhtml only lists versions up to 22 Although the releases page is current the stops at release 22'
0,'Missing HADOOPCONFDIR generates strange results If HADOOPCONFDIR is defined but points to a directory that either doesnt exist or isnt actually a viable configuration directory all sorts of weird things happen especially for logging The shell code should do a better job of verifying the directory is valid and exit if it detects if it is broken in some way'
0,'Datasets crash compile exception when mapping to immutable scala map'
0,'UserGroupInformation created from a Subject incorrectly tries to renew the Kerberos ticket The reloginFromKeytab method then fails with an IOException The problem is that the keytabFile UGI instance variable is NULL and that triggers the mentioned IOException'
0,'select $column1 explode $column2 is extremely slow Using a Dataset containing 10000 rows each containing null and an array of 5000 Ints I observe the following performance in local mode'
0,'Spark should throw analysis exception for invalid casts to date type Spark currently throws exceptions for invalid casts for all other data types except date type Somehow date type returns null It should be consistent and throws analysis exception as well'
0,'HiveComparision test should print out dependent tables'
0,'testpatch takes 45min! The runtime of testpatch has increased to 45min'
0,'Show table name or path in string of DataSourceScan right now the string of DataSourceScan is only HadoopFiles xxx without any information about the table name or path Since we have that in 16 this is kind of regression'
0,'Weird Plan Output when CTE used in RunnableCommand Currently when CTE is used in RunnableCommand the Analyzer does not replace the logical node With The child plan of RunnableCommand is not resolved However the output of the With plan node looks very confusing'
0,'Make precommit checks run against the correct branch The Hudson precommit tests are presently only capable of testing a patch against trunk Itd be nice if this could be extended to automatically run against the correct branch'
0,'Write a new group mapping service guide LdapGroupsMapping has lots of configurable properties and is thus fairly complex in nature HDFS Permissions Guide has a minimal introduction to LdapGroupsMapping with reference to More information on configuring the group mapping service is available in the Javadocs However its Javadoc provides no information about how to configure it Coredefaultxml has descriptions for each property but still lacks a comprehensive tutorial Without a tutorial guide these configurable properties would be buried under the sea of properties'
0,'SparkSQL ThriftServer hangs while extracting huge data volumes in incremental collect mode We are trying to run a sql query on our spark cluster and extracting around 200 million records through SparkSQL ThriftServer interface This query works fine for Spark 163 version however for spark 202 thrift server hangs after fetching data from a few partitions we are using incremental collect mode with 400 partitions As per documentation max memory taken up by thrift server should be what is required by the biggest data partition But we observed that Thrift server is not releasing the old partitions memory whenever the GC occurs even though it has moved to next partition data fetches which is not the case with 163 version On further investigation we found that SparkExecuteStatementOperationscala was modified for SPARK16563SQL fix spark sql thrift server FetchResults bug and result set iterator was duplicated to keep a reference to the first set'
0,'TestFairCallQueue fails TestFairCallQueue testPutBlocksWhenAllFull fails on trunk and branch2'
0,'Maintain HTTP host as SPNEGO SPN support and fix KerberosName parsing This breaks the following test in trunk This ticket is opened to bring back the support of HTTP host as valid SPNEGO SPN KerberosName parsing bug was discovered fixed and included as a necessary part of this ticket along with additional unit test to cover parsing different form of principals'
0,'Cannot create view which includes interval arithmetic the following view creation SQL failes with Failed to analyze the canonicalized SQL It is possible there is a bug in Spark'
0,'JMXJsonServlet fails when used within Tomcat Because of this the JMX servlet fails to work in KMS'
0,'FileContextjava fixRelativePart should check for not null for a more informative exception Following will come when job failed and deletion service trying to delete the log fiels'
0,'Executor loss may cause TaskSetManager to be leaked Due to a bug in TaskSchedulerImpl the complete sudden loss of an executor may cause a TaskSetManager to be leaked'
0,'Empty Table Remains After CREATE TABLE AS SELECT fails After the change the load API does not add the value of path into the options'
0,'task not serializable with groupByKey + mapGroups + map spark says Task not serializable'
0,'CSV data source does not write date and timestamp correctly Currently CSV data source write DateType and TimestampType as below It would be nicer if it write dates and timestamps as a formatted string just like JSON data sources Also CSV data source currently supports dateFormat option to read dates and timestamps in a custom format It might be better if this option can be applied in writing as well'
0,'Wrong messages when CTAS with a Partition By clause Till K75 it os working fine but when I set k100 it fails with javautilNoSuchElementException key not found I suspect it is failing because of lack of some resources but somehow exception does not convey anything as why this spark job failed Please can someone point me to root cause of this exception why it is failing Issue is that it is failing but not giving any explicit message as to why it failed'
0,'Correctness issue in INNER join result with window functions I have stumbled onto a corner case where an INNER join appears to return incorrect results I believe the join should behave as the identity but instead some values are shuffled around and some are just plain wrong'
0,'Analyzer incorrectly optimizes plan to empty LocalRelation My suspicion is that theres a bug in constraint propagation or filter pushdown This issue doesnt seem to affect Spark 20 so I think its a regression in master'
0,'Improve SaslRpcClient failure logging In SaslRpcClient getServerPrincipal it only printed out server advertised principal The actual principal we expect from configuration is quite useful while debugging security related issues It should also be logged'
0,'Spelling errors in logging and exceptions for code Found a set of spelling errors in the logging and exception messages'
0,'MemoryStoreputIteratorAsBytes may silently lose values when KryoSerializer is used This is the root cause behind a userreported wrong answer bug in PySpark caching reported by Ben Leslie on the Spark user mailing list in a thread'
0,'add instructions to BUILDINGtxt describing how to build on Windows Add documentation to BUILDINGtxt describing dependencies and instructions for building on Windows'
0,'Improve documentation on KMS ACLs and delegation tokens ~andrewwang suggested that the current KMS ACL page is not very userfocused and hard to come by without reading the code I read the document and the code and I agree So this jira puts more documentation to explain the current implementation'
0,'ClassCastException occurs when using select query on ORC file Error message is below'
0,'orgapachesparksqlcatalystexpressionsGeneratedClass$SpecificOrdering grows beyond 64 KB I have a wide table 400 columns when I try fitting the traindata on all columns the fatal error occurs'
0,'CodeGenerator failed to compile orgcodehausjaninoJaninoRuntimeException Code of method Error Greetings Im currently in the process of migrating a project Im working on from Spark 162 to 201 The project uses Spark Streaming to convert Thrift structs coming from Kafka into Parquet files stored in S3 This conversion process works fine in 162 but I think there may be a bug in 201 Ill paste the stack trace below Please let me know if you can be of assistance and if theres anything I can do to help'
0,'ML Logistic Regression aggregator serializes unnecessary data LogisticRegressionAggregator  is used to collect gradient updates in ML logistic regression algorithm The  stores a reference to the coefficients array of length equal to the number of features It also stores a reference to an array of standard deviations which is length numFeatures also When a task is completed it serializes the  which also serializes a copy of the two arrays These arrays dont need to be serialized only the gradient updates are being aggregated This causes issues performance issues when the number of features is large and can trigger excess garbage collection when the executor doesnt have much excess memory This results in serializing 2 numFeatures excess data When multiclass logistic regression is implemented the excess will be numFeatures'
0,'Distinct aggregates give incorrect answers on streaming dataframes Unsupported operations checking dont check whether AggregationExpression have isDistincttrue So streamingDfgroupByagg countDistinct key gives incorrect results'
0,'Spark SQL cross join + two joins BUG Error in query cannot resolve T1col given input columns col col line 6 pos 12 Apparently this example is minimal removing the CROSS or one of the JOIN causes no issue'
0,'TestAzureFileSystemInstrumentation testClientErrorMetrics fails intermittently due to assumption that a lease error will be thrown HADOOP12508 changed the behavior of an Azure Storage lease violation during deletes It appears that TestAzureFileSystemInstrumentation testClientErrorMetrics is partly dependent on the old behavior for simulating an error to be tracked by the metrics system I am seeing intermittent failures in this test'
0,'Sending AskPermissionToCommitOutput failed driver enter into task deadloop Executor sends AskPermissionToCommitOutput to driver failed and retry another sending Driver receives 2 AskPermissionToCommitOutput messages and handles them But executor ignores the first response true and receives the second response false The TaskAttemptNumber for this partition in authorizedCommittersByStage is locked forever Driver enters into infinite loop'
0,'Memory leak in Memory store when unable to cache the whole RDD in memory Problem description The following query triggers out of memory error This is not expected we should fallback to use disk instead if there is not enough memory for cache'
0,'Improve the error message when encountering an incompatible DataSourceRegister When encountering an incompatible DataSourceRegister its better to add instructions to remove or upgrade it'
0,'SPARK17387 caused ignorance of conf object passed to SparkContext after patch SPARK17387 was applied Sparkconf object is ignored when launching SparkContext programmatically via python from sparksubmit in case when we are running python SparkContext confxxx from sparksubmit conf is set confjconf is None passed as arg conf object is ignored and used only when we are launching javagateway'
0,'Convert site documentation from apt to markdown stragglers Convert the rest of the documentation to markdown also doing some other shell script rewrite changes along the way'
0,'TestConfigurationFieldsBase fails for fields that are DEFAULT values of skipped properties TestConfigurationFieldsBase fails as it mistakenly treats the two newly added defaultvalues as regular properties'
0,'Full outer join followed by inner join produces wrong results I found strange behaviour using fullouter join in combination with inner join It seems that inner join cant match values correctly after full outer join Here is a reproducible example in spark 20'
0,'hiveexecstagingdir have no effect in spark201 hiveexecstagingdir have no effect in spark201'
0,'Fix testpatch to work with the git repo We want the precommit builds to run against the git repo after the transition'
0,'TestDFVariationstestMount fails intermittently Failure message Error Message Specified path not exist Stacktrace javaioFileNotFoundException'
0,'JWTRedirectAuthenticationHandler breaks java8 javadocs Jenkins on Java8 is failing as JWTRedirectAuthenticationHandler has p tags in it something javadoc on java8 considers illegal
0,'PrefixComparatorsSuites String prefix comparator failed when both input strings are empty strings I could not reproduce it locally But let me add this case in the regressionTests to explicitly test it'
0,'NPE when trying to rename a directory in Windows Azure Storage FileSystem Encountered an NPE when trying to use the HBase utility ExportSnapshot with Azure as the target javalangNullPointerException'
0,'Increase findbugs maxHeap size The release build fails because of an obscure findbugs error Testing reveals that this is related to the findbugs heap size'
0,'startbuildenvsh fails in branch2 startbuildenvsh fails in branch2'
0,'Incomplete Cache Mechanism in CredentialProvider API The AbstractJavaKeyStoreProvider  in the CredentialProvider API has a cache member variable and interrogation of it during access but does not populate it'
0,'TestTextCommand use mkdirs rather than mkdir to create test directory TestTextCommand should use mkdirs rather than mkdir to create the test directory'
0,'Bump master branch version to 220SNAPSHOT'
0,'Create Wiki document about Herriot Test Framework and test development guide'
0,'document the contract of encoder serializer expressions'
0,'nafill mix up original values in long integers Manly the issue is clarified in the following example'
0,'java launched by PySpark as gateway may not be the same java used in the spark environment This could be seen as a py4j issue but from their point of view the fix is easy make sure the java you want is first on your path I cant figure out a way to make that reliably happen through the pyspark executor launch path and it seems like something that would ideally happen automatically If I set JAVAHOME when launching spark I would expect that to be the only java used throughout the stack'
0,'SQL Display the binary encoded values So far we are using commaseparated decimal format to output the encoded contents This way is rare when the data is in binary This could be a common issue when we use Dataset'
0,'Table Existence Checking when Index Table with the Same Name Exists Thus we issue an exception when users try to access Hive Index tables When the internal function tableExists tries to access Hive Index tables it always gets the same error message Hive index table is not supported This message could be confusing to users since their SQL operations could be completely unrelated to Hive Index tables For example when users try to alter a table to a new name and there exists an index table with the same name the expected exception should be a TableAlreadyExistsException'
0,'Fix createrelease script to include docs and necessary txt files The createrelease script doesnt include docs in the binary tarball We should fix that'
0,'Incorrect result when work with data from parquet If I use this code without saving to parquet it works fine If you change type of c column to IntegerType it also works fine'
0,'Pyspark with locality ANY throw javaioStreamCorruptedException In Pyspark 200 any task that accesses cached data nonlocally throws a StreamCorruptedException like the stacktrace below The simplest way I have found to reproduce this is by running the following code in the pyspark shell on a cluster of 2 slaves set to use only one worker core each'
0,'Fix NullPointerException when the returned value of the called method in Invoke is null To reproduce run these commands in the hive shell or beeline Youll see this exception for the byte case The root cause seems to be that Hive creates these tables with a notsocomplete schema When the ParquetReadSupport code tries to consolidate both schemas it just chooses whatever is in the parquet file for primitive types see ParquetReadSupportclipParquetType the vectorized reader uses the catalyst schema which comes from the Hive metastore and says its a byte field so when it tries to read the data the byte data stored in OnHeapColumnVector is null'
0,'Spark web UI The wrong value numCompletedTasks has been assigned to the variable numSkippedTasks The code in UtilswaitForProcess catches the wrong exception when using reflection NoSuchMethodException is thrown but the code catches NoSuchMethodError'
0,'BisectingKMeans Algorithm failing with javautilNoSuchElementException key not found In the following flowchart the batch should get marked fully processed before endpoint C however it is not Currently this does not actually cause an issue'
0,'lag lead using constant input values does not return the default value when the offset row does not exist This case will be triggered even when the function is an unresolved So when the functions like lead are used we may see errors like Window Frame RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW must match the required frame ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING because we wrongly set the the frame specification'
0,'Improve explain message for data source scan node All data sources show up as PhysicalRDD in physical plan explain Itd be better if we can show the name of the data source'
0,'Compilation fails in native link0 function on Windows HDFS6482 introduced a new native code function for creating hard links The Windows implementation of this function does not compile due to an incorrect call to CreateHardLink'
0,'UDFs dont see aliased column names Dunno if Im misinterpreting something here but this seems like a bug in how UDFs work or in how they interface with the optimizer Heres a basic reproduction It looks like from the second execution plan that BatchEvalPython somehow gets the unaliased column names whereas the Project right above it gets the aliased names'
0,'Not all core javadoc are checked by Hudson Since ant javadoc does not generate all core javadocs some javadocs eg HDFS javadocs are not checked by Hudson'
0,'Example code shouldnt use VectorImplicits asML fromML In SPARK14615 we use VectorImplicits and asML in example code to minimize the changes in that PR However this shouldnt appear in the example code We should consider update them during QA
0,'R deprecate unionAll and add union I get a similar error when using complex types in Aggregator Not sure if this is the same issue or something else'
0,'Fix posixspawn error on OS X OS X JDK has issues with localization that can cause utilShellrun to fail This is fixed in JDK9 but JDK7 and JDK8 are still broken'
0,'Add a step in the release process to update the release year in Web UI footer Per the discussion in HDFS9629 this jira is to propose adding a step in the release process to update the release year in Web UI footer when creating RC for a release'
0,'ThreadLocal initialization in several classes is not thread safe Under heavy contention this means during initialization some users will get an NPE'
0,'getNodeNumbered and generateTreeString are not consistent This is a bug introduced by subquery handling generateTreeString numbers trees including innerChildren used to print subqueries but getNodeNumbered ignores that As a result getNodeNumbered is not always correct Note that 3 should be the Project node but getNodeNumbered ignores innerChild and as a result returns the wrong one'
0,'cannot run aggregate function on explode result'
0,'HadoopCore Site add Chukwa as related project in menu On the HadoopCore site page add Chukwa to the list of Related Projects in the menu bar'
0,'Fix hadoopmapreduceclientnativetask unit test which fails because it is not able to open the glibc bug spill file Fix hadoopmapreduceclientnativetask unit test which fails because it is not able to open the glibc bug spill file'
0,'Fix the code injection vulnerability related to Generator functions Similar to SPARK15165 codegen is in danger of arbitrary code injection The root cause is how variable names are created by codegen In the example above a RuntimeException is thrown but attacker can replace it with arbitrary code'
0,'TestDNS fails on Windows after HADOOP12437 HADOOP12437 added several new tests covering functionality of resolving host names based on an alternate network interface These tests are failing on Windows'
0,'sparkfiles & sparkjars should not be passed to driver in yarn mode The following command will fails for spark 20The above command can reproduce the error as following in a multiple node cluster To be noticed this issue only happens in multiple node cluster'
0,'grouping set throws NPE spark sql grouping sets throws NullPointerException This problem can be recreated using the following lines of code'
0,'Collect disks usages on the node In this JIRA we propose to collect disks usages on a node This JIRA is part of a larger effort of monitoring resource usages on the nodes'
0,'SQL page of Sparksql is always blank When I run a sql query in sparksql the Execution page of SQL tab is always blank But the JDBCServer is not blank'
0,'Spark200 unable to infer schema for parquet data written by Spark162 Spark200 seems to have some problems reading a parquet dataset generated by 162 Another strange thing is that the schemas for the first and the last 31 partitions of the subset are identical I have originally posted it to user mailing list but with the last discoveries this clearly seems like a bug'
0,'Spark tasks which cause JVM to exit with a zero exit code may cause app to hang in Standalone mode If you have a Spark standalone cluster which runs a single application and you have a Spark task which repeatedly fails by causing the executor JVM to exit with a zero exit code then this may temporarily freeze hang the Spark application on a cluster will cause all executors to die but those executors wont be replaced unless another Spark application or worker joins or leaves the cluster This is caused by a bug in the standalone Master where schedule is only called on executor exit when the exit code is nonzero'
0,'Update project release notes for 300alpha2 Lets update the website release notes for 300alpha2s changes'
0,'fix partition related behaviors with DataFrameWritersaveAsTable'
0,'Improve Documentation for Sample Methods The documentation for sample is a little unintuitive It was difficult to understand why I wasnt getting exactly the fraction specified of my total DataFrame rows The PR clarifies the documentation for Scala Python and R to explain that that is expected behavior'
0,'StatisticsColumnSuite failures on big endian platforms introduces new tests function that fails on big endian platforms'
0,'Work around exception thrown by HiveResultSetMetaDataisSigned Attempting to use Spark SQLs JDBC data source against the Hive ThriftServer results in a javasqlSQLException Method not supported exception from orgapachehivejdbcHiveResultSetMetaDataisSigned I have filed HIVE14684 to attempt to fix this in Hive by implementing the isSigned method but in the meantime for compatibility with older JDBC drivers I think we should add specialcase error handling to work around this bug'
0,'Remove obsolete reference to Cygwin in BUILDINGtxt The Building on Windows section of BUILDINGtxt has an obsolete reference to Cygwin It should be removed to avoid confusion'
0,'LogLevel main throws exception if no arguments provided I think we can catch the exception in the main method and dump a log error message instead of throw the stack which may frustrate users'
0,'SparkR LDA doesnt set optimizer correctly sparklda pass the optimizer em or online to the backend However LDAWrapper doesnt set optimizer based on the value from R Therefore for optimizer em the isDistributed field is FALSE which should be TRUE'
0,'TimeWindow incorrectly drops slideDuration in constructors Right now the constructors for the TimeWindow expression in Catalyst incorrectly uses the windowDuration in place of the slideDuration This will cause incorrect windowing semantics after time window expressions are analyzed by Catalyst'
0,'YARN files archives broke SPARK18099 broke files and archives options'
0,'Fix KMSClientProvider for nonsecure proxyuser use case The issue was found after HADOOP13988 by HadoopHDFS test TestAclsEndToEnd Sorry both Jenkins and I was not able to catch it HADOOP13988 fixed the issue for KMSClientProvider secure proxy user token use case But the nonsecure proxy user case should not be affected by the new logic The ticket is open to fix it'
0,'JDK8 AuthenticationFilter CertificateUtil SignerSecretProviders KeyAuthorizationKeyProvider Javadoc issues Jenkins on Java8 is failing due to a number of Javadoc violations that are now considered ERRORs'
0,'Serialization of accumulators in heartbeats is not threadsafe Check out the following ConcurrentModificationException Even though accumulators arent threadsafe they can be concurrently read while serializing executor heartbeats and modified while tasks are running leading to ConcurrentModificationException errors or leading to inconsistent data since individual fields of a multifield object might be serialized at different points in time leading to inconsistencies in accumulators like LongAccum This seems like a pretty serious issue but Im not sure whats the best way to fix this An obvious fix would be to properly synchronize all accesses to the fields of our accumulators and to synchronize the writeObject and writeKryo methods but this may have an adverse performance impact'
0,'Update website for recent subproject departures A number of subprojects have left Hadoop yet the websites not been fully updated to reflect that'
0,'Update CHANGEStxt to reflect all the changes in branch27 When committing to branch27 we need to edit CHANGEStxt However there are some recent commits to branch27 without editing CHANGEStxt We need to update the change log'
0,'Create a privacy policy for the Hadoop website It would be great to collect analytics about the visitors to the website and to do so we need to create a privacy policy that tells visitors what we will collect'
0,'NPE in JvmPauseMonitor when calling stop before start It is observed that after YARN4019 some tests are failing in TestRMAdminService with null pointer exceptions in build build failure'
0,'Build failure due to errors of javadoc build in hadoopazure From HADOOP13863 building javadoc of hadoop azure module failed Though these error is not related to the patch of HADOOP13868 directly it can be fixed anyway Building whole project fails with JDK 180112'
0,'Adding a malformed URL to scaddJar and or scaddFile bricks Executors Using a malformed URL in scaddJar or scaddFile bricks the executors forever The executors try to update their dependencies but because the URL is malformed they always throw a malformedURL exception Then your cluster is unusable until you restart it'
0,'Fix bug in the name assignment method in SparkR The names method fails to check for validity of the assignment values This can be fixed by calling colnames within names'
0,'dftake 1 and dflimit 1 collect perform differently in Python In PySpark dftake 1 ends up running a singlestage job which computes only one partition of df while dflimit 1 collect ends up computing all partitions of df and runs a twostage job This difference in performance is confusing so I think that we should generalize the fix from SPARK10731'
0,'Function tojson ignores the userprovided options'
0,'SQLSQLParser fails to resolve nested CASE WHEN statement with parentheses SQLParser fails to resolve nested CASE WHEN statement'
0,'jdbc datasource read fails when quoted columns eg mixed case reserved words in source table are used in the filter I am working on fix for this issue will submit PR soon'
0,'IsotonicRegression takes nonpolynomial time for some inputs To reproduce this I pulled the private method poolAdjacentViolators out of mllibregressionIsotonicRegression and into a benchmarking harness I can also confirm that I run into this issue on a real dataset Im working on when trying to calibrate random forest probability output Some partitions take 12 hours to run This isnt a skew issue since the largest partitions finish in minutes I can only assume that some partitions cause something approaching this worstcase complexity'
0,'KMS Server should log exceptions before throwing In some recent investigation it turns out when KMS throws an exception into tomcat its not logged anywhere and we can only see the exception message from clientside but not the stacktrace Logging the stacktrance would help debugging'
0,'Hadoop Common directory is missing from all download mirrors I checked I checked several download mirrors They all seem to be missing the common folder The only thing I see there is dist hadoop chukwa This is a blocker since I cant download Hadoop at all'
0,'Fix intermittent test failure of TestCopyPreserveFlag Found this issue on HADOOP11149'
0,'Backport HADOOP6578 to branch1 HADOOP6578 should be backported to branch1 so it can also benefit from whitespace trimming in configuration'
0,'hadoop fs getmerge doc is wrong The documentation says that addnl is a valid parameter but as of HADOOP7348 its been replaced with nl The docs should be updated'
0,'Add SQL command for printing out generated code for debugging SPARK14227 adds a programatic way to dump generated code In pure SQL environment this doesnt work It would be great if we can have return the generated code'
0,'Cannot filter by nonexisting column in parquet file'
0,'pysparksqlcolumn exports nonexistent names'
0,'getDropLast is missing in OneHotEncoder We forgot the getter of dropLast in OneHotEncoder'
0,'Fix undefined reference to dlopen error when compiling libhadooppipes When compile hadoop with native support we encounter compile error that undefined reference to dlopen when link libcrypto'
0,'HadoopRDD should not swallow EOFException The code in catches EOFException and mark RecordReader finished However in some cases RecordReader will throw EOFException to indicate the stream is corrupted See the following stack trace as an example Then HadoopRDD doesnt fail the job when files are corrupted eg corrupted gzip files Note NewHadoopRDD doesnt have this issue'
0,'JDBC Source Wrong Results when lowerBound is larger than upperBound in Column Partitioning Several bugs have been found caused by integer overflows in Tungsten This JIRA is for taking a final pass before 20 release to reduce potential bugs and issues Raise exception early instead of later throwing NegativeArraySize Document clearly the largest array size we support in DataFrames'
0,'BytesWritable fails to support 2G chunks due to integer overflow BytesWritablesetSize increases the buffer size by 15 each time 3 2 This is an unsafe operation since it restricts the max size to ~700MB since 700MB 3 2GB I didnt write a test case for this case because in order to trigger this Id need to allocate around 700MB which is pretty expensive to do in a unit test Note that I didnt throw any exception in the case integer overflow as I didnt want to change that behavior callers to this might expect a javalangNegativeArraySizeException'
0,'Avoid NegativeArraySizeException while reserving additional capacity in VectorizedColumnReader Compared with the existing SQL interface it is not user friendly to add such a comment attribute when defining a StructField'
0,'ClassCastException OuterReference cannot be cast to NamedExpression for correlated subquery on the RHS of an IN operator The following test case produces a ClassCastException in the analyzer This bug was discovered while trying to run SQLite bug reports through Spark SQL'
0,'core dumped running Spark SQL on large data volume 100TB Running a query on 100TB parquet database using the Spark master dated 11 04 dump cores on Spark executors The query is TPCDS query 82 though this query is not the only one can produce this core dump just the easiest one to recreate the error Spark output that showed the exception This is not easily reproducible on smaller data volumes eg 1TB or 10TB but easily reproducible on 100TB+so look into data types that may not be big enough to handle hundreds of billion'
0,'IPC Server should allow suppressing exception logging by type not log server too busy messages However every backoff results in a log message Weve seen these log messages slow down the NameNode This log message adds nothing useful The IPC Server should also allow services to skip logging certain exception types altogether'
0,'Fix input metrics for range operator Range operator currently does not output any input metrics and as a result in the SQL UI the number of rows shown is always 0'
0,'Datasetsample with seed result seems to depend on downstream usage Summary to reproduce bug You would expect results 1 and 2 to use the same rows from DF but they appear not to'
0,'Hive context created by HiveContext cant access Hive databases when used in a script launched be sparksubmit The above query failed as expected but an empty table t is created'
0,'releasebuildsh is missing hivethriftserver for scala 211 The same issue as SPARK16453 But for branch 16 we are missing the profile for scala 211 build'
0,'Json path implementation fails to parse key The current json path parser fails to parse expressions like key which are used for named expressions with spaces'
0,'RDDpipe command String in Spark 20 does not work when command is specified with some options Unfortunately I know very little about databases but I figure this is a bug I have a DataFrame with the following schema I am trying to write it to an Oracle database like this The error message is the same as in Could it be that Long is also translated into the wrong data type Thanks'
0,'Broken test showDF in testsparkSQLR Seems showDF in testsparkSQLR is broken'
0,'Submitting a hadoop patch doesnt trigger jenkins test run Im creating this jira to investigate the possible bug in devsupport testpatchsh script'
0,'Patch up Jetty to disable SSLv3 Hadoop uses an older version of Jetty that allows SSLv3 We should fix it up'
0,'Jenkins is failing due to the upgrade of svn client Now Jenkins is failing with the following message'
0,'sparkdaemonsh arguments error lead to throws Unrecognized option'
0,'Enable parallel JUnit tests in precommit HADOOP9287 and related issues implemented the paralleltests Maven profile for running JUnit tests in multiple concurrent processes This issue proposes to activate that profile during precommit to speed up execution'
0,'Scalar subquery with extra group by columns returning incorrect result'
0,'TestWebDelegationToken failing with port in use The TestWebDelegationToken failed on Jenkins with port in use It looks like the code searches for a free port and then starts jetty but maybe theres enough of a race condition between port location and jettystart to cause intermittent failures'
0,'Unable to build compile Spark in IntelliJ due to missing Scala deps in sparktags After merge I am unable to build it in my IntelliJ Got the following compilation error'
0,'Unable to read given csv data Excepion javalangIllegalArgumentException requirement failed Decimal precision 28 exceeds max precision 20 When reading below mentioned csv data even though the maximum decimal precision is 38 following exception is thrown javalangIllegalArgumentException requirement failed Decimal precision 28 exceeds max precision 20'
0,'AnalysisException in first last during aggregation Since Spark 201 the following pyspark snippet fails with AnalysisException The second argument of First should be a boolean literalSimilar code with in Java fails in the same way It worked in Spark 200 so I believe it may be related to the fix for SPARK16648'
0,'NPE in TestSequenceFile NPE thrown which is hiding actual errorHere it should throw numberformatexpection since count exceeds integer range'
0,'Typo in Spark SQL Programming guide that links to examples When creating a data source table without explicit specification of schema or SELECT clause we silently ignore the bucket specification CLUSTERED BY SORTED BY Instead we should issue an error message'
0,'Downgrade the memory leak warning message TaskMemoryManager has a memory leak detector that gets called at task completion callback and checks whether any memory has not been released If they are not released by the time the callback is invoked TaskMemoryManager releases them In practice there are multiple reasons why these can be triggered in the normal code path eg limit or task failures and the fact that these messages are log means the leak is fixed by TaskMemoryManager To not confuse users we should downgrade the message from warning to debug level and avoid using the word leak since it is not actually a leak'
0,'fix slow tests'
0,'improve the error message of using join'
0,'Multiple Bugs in DDL Statements on Temporary Views When the permanent tables views do not exist but the temporary view exists the expected error should be NoSuchTableException for partitionrelated ALTER TABLE commands However it always reports a confusing error message However it reports a strange error'
0,'Fix potential socket file handle leaks identified via static analysis The HP Fortify Opens Source Review team reported a handful of potential resource leaks that were discovered using their static analysis tool We should fix the issues identified by their scan'
0,'Bisecting kmeans in SparkR documentation we need updates to programming guide example and vignettes'
0,'simplify bucket tests and add more comments'
0,'ADLS TestAdlContractRootDirLivetestRmNonEmptyRootDirNonRecursive failed'
0,'Work around Jekyll errorhandling bug which led to silent doc build failures In MultivariateOnlineSummarizer min max method use judgement nnz i weightSum it will cause some numerial problem and make result unstable The bug reason is that because of the floating rounding'
0,'The globber will sometimes erroneously return a permission denied exception when there is a nonterminal wildcard The globber will sometimes erroneously return a permission denied exception when there is a nonterminal wildcard The existing unit tests dont catch this because it doesnt happen for superusers'
0,'Some of the bin hadoop subcommands are not available on Windows conftest distch jnipath and trace are not enabled in hadoopcmd kerbname is enabled but does not appear in the help message'
0,'TestGridmixMemoryEmulation and TestResourceUsageEmulators fail on the environment other than Linux or Windows TestGridmixMemoryEmulationtestTotalHeapUsageEmulatorPlugin fails on Mac The following tests fail on Mac as well'
0,'SparkR sparkglm error on collinear data Spark GeneralizedLinearRegression can handle collinear data since the underlying WeightedLeastSquares can be solved by local lbfgs rather than normal But the SparkR wrapper sparkglm throw errors when fitting on collinear data After depth study of this error I found it was caused the standard error of coefficients t value and p value are not available when the underlying WeightedLeastSquares was solved by local lbfgs So the coefficients matrix was generated failed'
0,'Array of struct with a single field name element cant be decoded from Parquet files written by Spark 16+ While trying to use a custom classpath for metastore jars I ran into the following issue The issue here is that MRVersion is not packaged anywhere with Spark'
0,'Fix git environment check during testpatch Im seeing this error when running testpatch in Apache Hadoop'
0,'Strange Error when Issuing Load Table Against A View Users should not be allowed to issue LOAD DATA against a view Currently when users doing it we got a very strange runtime error'
0,'Style of event timeline is broken Some  was renamed like timeline to vistimeline but that ticket didnt care and now style is broken'
0,'Make whole stage codegen variable names slightly easier to read Use lower case Change long prefixes to something shorter in this case I am changing only one TungstenAggregate agg'
0,'Fail to create a decimal arrays with literals having different inferred precessions and scales In Spark 20 we will parse float literals as decimals However it introduces a sideeffect which is described below'
0,'SparkR cannot parallelize dataframe with NA or NULL in Date columns To reproduce We dont seem to have this problem with POSIXlt'
0,'Create instructions on fully building Spark in Intellij With some of our more complicated modules Im not sure whether Intellij correctly understands all source locations Also we might require specifying some profiles for the build to work directly We should document clearly how to start with vanilla Spark master and get the entire thing building in Intellij'
0,'Floor ceil of decimal returns wrong result if its in compact format'
0,'Add a menu to the documentation of MLlib Right now the table of contents gets generated on a pagebypage basis which makes it hard to navigate between different topics in a project We should make use of the empty space on the left of the documentation to put a navigation menu A picture is worth a thousand words'
0,'Update Hadoop Common Sites Add documentation on our interface classification scheme to thew common site'
0,'Improve error messages for data sources when they are not found We can point them to sparkpackagesorg to find them'
0,'The size of header buffer of HttpServer is too small when HTTPS is enabled Though the issue is fixed for HTTP connections via HADOOP8816 HTTPS connections needs to be fixed as well'
0,'Additional arguments in writedf are not passed to data source writedf passes everything in its arguments to underlying data source in 1x but it is not passing header true in Spark 20 For example the following code snippet produces a header line in older versions of Spark but not in 20'
0,'Memory leak in PySpark StringIndexer StringIndexerModel wont get collected by GC in Java even when deleted in Python It can be reproduced by this code which fails after couple of iterations around 7 if you set driver memory to 600MB Explicit call to Python GC fixes the issue The issue is similar to SPARK6194 and can be probably fixed by calling jvm detach in models destructor This is implemented in pysparkmlibcommonJavaModelWrapper but missing in pysparkmlwrapperJavaWrapper Other models in ml package may also be affected by this memory leak'
0,'Update src site markdown releases to include old versions of Hadoop With the commit of HADOOP11731 we need to include the new format of release information in trunk This JIRA is about including those old versions in the tree'
0,'Corruption and Correctness issues with exploding Python UDFs There are some weird issues with exploding Python UDFs in SparkSQL There are 2 cases where based on the DataType of the exploded column the result can be flat out wrong or corrupt Seems like something bad is happening when telling Tungsten the schema of the rows during or after applying the UDF'
0,'Hadoop Common native compilation broken in windows After HADOOP10962 hadoop common native compilation broken in windows'
0,'Sasl message with MD5 challenge text shouldnt be LOG out even in debug level Some log examples We should get rid of this kind of log in production environment even under debug log level'
0,'ZKDelegationTokenSecretManager fails to renewToken created by a peer When a ZKDTSM tries to renew a token created by a peer It throws an Exception with message bar is trying to renew a token with wrong password'
0,'javautilHashtable limits the throughput of PARSEURL Unfortunately it seems that there is an internal threadsafe cache in there and the instances end up being 90 idle When I view the thread dump for my executors most of the executor threads are BLOCKED in that state'
0,'copy all of testpatch BINDIR prior to reexec During some tests initial mvn install triggered a full test suite run when Jenkins switches from old testpatch to new testpatch This is bad'
0,'NPE in SaslRpcServer Looking at identifier implementations eg AbstractDelegationTokenIdentifier and others I can see that getUser method can return null If debug logging is enabled this NPEs If getUser is not expected to return NULL it should either be checked and erred upon better here or the error should be allowed to happen where it would otherwise happen not in some debug log path'
0,'Formalize the shell API After HADOOP11485 we need to formally document functions and environment variables that 3rd parties can expect to be able to exist use'
0,'The default retry policy does not handle RetriableException correctly The default policy in RetryUtils does not retry RetriableExceptions even when defaultRetryPolicyEnabled is true This was discovered via an HDFS client failing to retry getFileBlockLocations after checkNNStartup failed'
0,'AnalysisException may be thrown when union two DFs whose struct fields have different nullability The following Spark shell snippet reproduces this issue The reason is that we treat two StructType incompatible even if their only differ from each other in field nullability'
0,'HiveClientImpl throws NPE when reading database from a custom metastore When call SparkR sql an error pops up'
0,'The config ignoreCorruptFiles doesnt work for Parquet We have a config which can be used to ignore corrupt files when reading files in SQL Currently the config has two issues and cant work for Parque'
0,'SparkContextaddFile should not fail if called twice with the same file Prior to 20 calling SparkContextaddFile twice with the same path would succeed and would cause future tasks to receive an updated copy of the file This behavior was never explicitly documented but Spark has behaved this way since very early 1x versions some of the relevant lines in ExecutorupdateDependencies have existed since 2012 This problem also affects addJar in a more subtle way the fileServeraddJar call will also fail with an exception but that exception is logged and ignored due to some code which was added in 2014 in order to ignore errors caused by missing Spark examples JARs when running on YARN cluster mode AFAIK'
0,'UDF explosion yielding empty dataframe fails The problem is that in spark 161 the shutdown hook for an executor will be called only for the first submitted job problem not seen when using java 17 problem not seen when using spark 160 looks like this bug is caused by this modification from 160 to 161'
0,'Some modules have dependencies on hadoopclient jar removed by HADOOP11804 There are still dependencies on the nowremoved hadoopclient jar The current code builds only because an obsolete snapshot of the jar is found on the repository server Changing the project version to something new exposes the problem While the build currently dies at hadooptools hadoopsls Im seeing issues with some Hadoop Client modules too Im filing a new bug because I cant reopen HADOOP11804'
0,'Collect network usage on the node In this JIRA we propose to collect the network usage on a node This JIRA is part of a larger effort of monitoring resource usages on the nodes'
0,'JavaDocs for SignerSecretProvider are outofdate in AuthenticationFilter The Javadocs in AuthenticationFilter say However the string implementation is no longer available because HADOOP11748 moved it to be a testonly artifact This also doesnt mention anything about the filebacked secret provider'
0,'Clean up some test methods in TestCodecjava Found two issues when reviewing the patches in HADOOP11627 There is no Test annotation so the test is not executed'
0,'groupby orderby ordinal should throw AnalysisException instead of UnresolvedException'
0,'GraphX Invalid initial capacity when running triangleCount Running GraphX triangle count on largeish file results in the Invalid initial capacity error when running on Spark 20 tested on Spark 20 201 and 202 Running the same code on Spark 16 and the query completes without any problems'
0,'Fix TestKMS testKMSRestart failure Error Message loginUserFromKeyTab must be done first Stacktrace javaioIOException'
0,'Spark generated code causes CompileException when groupByKey reduceGroups and map 2 are used Code logic looks like this 201 error Message'
0,'spark sql use HIVE UDF throw exception when return a Map value I have met a problem like but not with this parameter Map my evaluate function return a Map public Map String String evaluate Text url when run sparksql with this udf getting an exception'
0,'spark 201 enable hive throw AlreadyExistsException message Database default already exists in spark 201 I enable hive support and when init the sqlContext throw a AlreadyExistsException message Database default already exists The result is correctbut a exception also throwBy the code'
0,'Site build is broken'
0,'Reading sequence file consumes 100 cpu with maximum throughput being about 5MB sec per process I did some tests on the throughput of scanning blockcompressed sequence files The sustained throughput was bounded at 5MB sec per process with the cpu of each process maxed at 100 It seems to me that the cpu consumption is too high and the throughput is too low for just scanning files'
0,'Hive build failure on hadoop27 due to HADOOP11356 HADOOP11356 removes orgapachehadoopfspermissionAccessControlException causing build break on Hive when compiling against hadoop27'
0,'Missing Reference in Multi Union Clauses Cause by TypeCoercion Key Points to reproduce issue The reason of issue Appreciate for any comments'
0,'Mini HDFS Cluster fails to start on trunk Its been noticed that Mini HDFS Cluster fails to start on trunk blocking unit tests and Jenkins'
0,'PySpark ML DataFrame example fails on Vector conversion'
0,'releasedocmakerpy doesnt work behind a proxy releasedocmakerpy doesnt work behind a proxy because urlliburlopen doesnt care environment varialibes like $httpproxy or $httpsproxy'
0,'Failed to run SQL show table extended like tablename in Spark200 SQL show table extended like tablename doesnt work in spark 200 that works in spark152 Error orgapachesparksqlcatalystparserParseException missing FUNCTIONS at extended line 1 pos 11'
0,'Add tests to ensure stability of that all Structured Streaming log formats To be able to restart StreamingQueries across Spark version we have already made the logs offset log file source log file sink log use json We should added tests with actual json files in the Spark such that any incompatible changes in reading the logs is immediately caught'
0,'sparkR scsetLogLevel doesnt work Error could not find function scsetLogLevel scsetLogLevel doesnt exist'
0,'SPARKSQL transformation script got failure for python script The problem is that it assumes that databaseproperties is a not null value which is not always the truth As you can see parameters field is set to null'
0,'Backward compatibility creating a Dataframe on a new SQLContext object fails with a Derby error With a local spark instance built with hive support Pyarn Phadoop26 Dhadoopversion260 Phive Phivethriftserver The following script sequence works in Pyspark without any error in 16x but fails in 2x The error produced is The error goes away if sqlContext2 is replaced with sqlContext in the last error line Since the SQLContext  is preserved for backward compatibility the changes in 2x break scripts notebooks that follow the above pattern of calls and used to run fine with 16x'
0,'fromOffsets parameter in Kafkas Direct Streams does not work in python3 KafkaUtilscreateDirectStream does not work in python3 when you set parameter fromOffsets which is starting offsets of the stream on Kafka This is because the long type is removed from python3 and py4j maps numeric variables to javalangInteger or javalangLong depending on number size which causes ClassCastException for small offsets variables'
0,'Azure FileSystem PageBlobInputStream does not return 1 on EOF Azure FileSystem PageBlobInputStream does not return 1 on EOF This is some scenarios causes infinite hands on reading files eg copyToLocal can hang forever'
0,'TRUNCATE TABLE does not work with Datasource tables outside of Hive This notebook demonstrates the bug The obvious issue is that nested UDTs are not supported if the UDT is Pythononly Looking at the exception thrown this seems to be because the encoder on the Java end tries to encode the UDT as a Java  which doesnt exist for the PythonOnlyUDT'
0,'Reused Exchange Aggregations Produce Incorrect Results Was brought to my attention where the following code produces incorrect results Which was different than using a parallelized collection as the DF backing So I tested the same code with a Parquet backed DF and saw the same results Which leads me to believe there is something wrong with the reused exchange'
0,'inputfilename returns empty strings in data sources based on NewHadoopRDD Currently Spark SQL generates wrong results due to wrong partitions'
0,'TestPathData fails intermittently with Mkdirs failed to create d1 A bunch of TestPathData tests failed intermittently Example failure log Failed Error Message Mkdirs failed to create d1 Stacktrace javaioIOException'
0,'Failed to drop database when use the database in Spark 20 Found below case was broken in Spark 20 but it can run successfully in Spark 16 BTW the case can also run successfully in Hive Please see below reproduces for details Error in query Can not drop current database testdb'
0,'Make Kdiag check whether hadooptokenfiles points to existent and valid files Steve proposed that KDiag should fail fast to help debug the case that hadooptokenfiles points to a file not found This JIRA is to affect that'
0,'Fix findbugs in ZK DelegationTokenSecretManagers HADOOP11017 adds ZK implementation for DelegationTokenSecretManager This is a followup JIRA to address review comments there findbugs and order of updates to the currentKey'
0,'TestS3ATemporaryCredentialstestSTS error when using IAM credentials orgapachehadoopfss3aTestS3ATemporaryCredentialstestSTS throws a 403 AccessDenied when run without any AWS credentials access key and secret key in the config It fails because the InstanceProfileCredentialsProvider in the credentials chain on line 91 is used Suggestion on how to fix this test case'
0,'LdapGroupsMapping getPassward shouldnt return null when IOException throws When IOException throws in getPassword getPassword return null String this will cause setConf throws javalangNullPointerException when check isEmpty on null string'
0,'Improve metadata refresh I assume that problem results from the performance problem with reading parquet files Original Issue description Should I expect such a drop in performance I dont know how to prepare sample data to show the problem Any ideas Or public data with many nested columns'
0,'Guarantee a full build of all native code during precommit Some of the native components of the build are considered optional and either will not build at all without passing special flags to Maven or will allow a build to proceed if dependencies are missing from the build machine If these components do not get built then precommit isnt really providing full coverage of the build This issue proposes to update testpatchsh so that it does a full build of all native components'
0,'Spark 20 breaks various Hive cast functions In Spark 1x it is possible to use int string and other functions to perform type cast This functionality is broken in Spark 20 because Spark no longer falls back to Hive for these functions'
0,'Table Comment in the CatalogTable returned from HiveMetastore is Always Empty The comment in CatalogTable returned from Hive is always empty However when we try to retrieve the table metadata from Hive metastore we do not rebuild it The comment is always empty'
0,'NPE in generated SpecificMutableProjection for Aggregator this is a bug in the branch21 but i dont think it was in 210rc1 code contrived but based on real code we run the error seems to be in the code generation for the aggregator result'
0,'Encoderstuple should handle null object correctly When a cluster has PYSPARKPYTHON and PYSPARKDRIVERPYTHON environment variables set needed for using nonsystem Python eg usr bin anaconda bin python then you are unable to override this per submission in YARN cluster mode Expectation is that the conf values above override the environment variables Fix is to change the order of application of conf and env vars in the yarn client'
0,'SparkR sparkrandomForest classification throws exception when training on libsvm data This error is caused by the label column of the R formula already exists we can not force to index label However it must index the label for classification algorithms so we need to rename the RFormulalabelCol to a new value and then we can index the original label This issue also appears at other algorithms sparknaiveBayes sparkglm only for binomial family and sparkgbt only for classification'
0,'Fix tracing documention reflecting the update to htrace4 The sample code in Tracingmd have some problems compilation error by not importing Tracer generic options are not reflected because Tracer is initialized before ToolRunner run it may be confusing to use FsShell in example because it has embedded Tracer now'
0,'Broadcast join produces incorrect results when compressed Oops differs between driver executor Broadcast join produces incorrect columns in join result see below for an example The same join but without using broadcast gives the correct columns'
0,'Log slow name resolutions Logging slow name resolutions would be useful in identifying DNS performance issues in a cluster Most resolutions go through orgapachehadoopsecuritySecurityUtilgetByName see attached call graph Adding additional logging to this method would expose such issues'
0,'Dataset containing a Case  with a List type causes a CompileException converting sequence to list The issue occurs when we run a map over a dataset containing Case  with a List in it It seems to be internally converting the List to a sequence then it cant convert it back'
0,'External distribution stitching scripts do not work correctly on Windows In HADOOP12850 we pulled the distlayoutstitching and disttarstitching scripts out of hadoopdist pomxml and into external files It appears this change is not working correctly on Windows'
0,'TraceAdmin should support Kerberized cluster When I run hadoop trace command for a Kerberized NameNode it failed with the following error It is failing because TraceAdmin does not set up the property'
0,'ML MLLIB ChiSquareSelector based on StatisticschiSqTest RDD is wrong The method to count ChiSqureTestResult in mllib feature ChiSqSelectorscala line 233 is wrong For feature selection method ChiSquareSelector it is based on the ChiSquareTestResultstatistic ChiSqure value to select the features It select the features with the largest ChiSqure value But the Degree of Freedom df of ChiSqure value is different in StatisticschiSqTest RDD and for different df you cannot base on ChiSqure value to select features Because of the wrong method to count ChiSquare value the feature selection results are strange This is strange This result is make sense because the df of each feature in scikit learn is the same'
0,'Some tests in orgapachehadoopfsshellfind occasionally time out An example javalangException test timed out after 1000 milliseconds'
0,'Event time watermark delay threshold specified in months or years gives incorrect results Internally we use CalendarInterval to parse the delay Nondeterminstic intervals like month and year are handled such a way that the generated delay in milliseconds is 0 delayThreshold is in months or years'
0,'bash unit tests are failing Running bats in a docker container on jenkins fails'
0,'Cant set Python via sparksubmit for YARN cluster mode when PYSPARKPYTHON & PYSPARKDRIVERPYTHON are set Both offheap and onheap variants of ColumnVectorreserve can unfortunately overflow while reserving additional capacity during reads Caused by javalangNegativeArraySizeException'
0,'Config archive not properly added to YARN classpath Since they uses SparkContext'
0,'TestFindprocessArguments occasionally fails This failure seems to exist after November 3rd I am still tracing where this can come from Error Message test timed out after 1000 milliseconds Stacktrace javalangException'
0,'sparkrzip is not distributed to executors when run sparkr in RStudio Heres the code to reproduce this issue And this is the exception in executor log'
0,'Visualization and metrics for generated operators'
0,'Fix typing mistake of inline document in hadoopmetrics2properties Fix typing mistake of inline document in hadoopmetrics2properties sinkgangliatagsForPrefixjvmProcesNamecode should be code sinkgangliatagsForPrefixjvmProcessNamecode And also could add examples into the inline document for easier understanding of metrics tag related configuration'
0,'Improve error reporting for FileStressSuite in streaming FileStressSuite doesnt report errors when they occur'
0,'orgapachehadoopfsshellTestCount fails Error Message Argument s are different Wanted If kerberos is enabled while HTTP SPNEGO is not configured some links cannot be accessed Its not correct because enabling Kerberos and HTTP SPNEGO are two steps If Kerberos is enabled while HTTP SPNEGO is not some links cannot be accessed such as logs and it will return error message as below HTTP ERROR 403'
0,'Master may ask a worker to launch an executor before the worker actually got the response of registration I somehow saw a failed test orgapachesparkDistributedSuitecaching in memory serialized replicated Its log shows that Spark master asked the worker to launch an executor before the worker actually got the response of registration So the master knew that the worker had been registered But the worker did not know if it self had been registered Then seems the worker did not launch any executor'
0,'Incorrect Statistics when Queries Containing LIMIT TABLESAMPLE 0 I have df where column1 is struct type and there is 1M rows Is there a change in IS NOT NULL behaviour in Spark 20'
0,'TestKMS testKMSProvider intermittently fails during test rollover draining Ive seen several failures of testKMSProvider all failed in the following snippet with error message'
0,'Cached tables are not used in SubqueryExpression Consider the following In both plans Id expect that both sides of the joins would read from the cached table for both the cross join and anti join but the left anti join produces the following plan which only reads the left side from cache and reads the right side via a regular noncahced scan'
0,'Fix default Locale used in DateFormat NumberFormat to LocaleUS Many parts of the code use DateFormat and NumberFormat instances Although the behavior of these format is mostly determined by things like format strings the exact behavior can vary according to the platforms default locale Although the locale defaults to en it can be set to something else by env variables And if it does it can cause the same code to succeed or fail based just on locale'
0,'RDDpipe returns values for empty partitions Currently we dont check the value returned by called method in Invoke When the returned value is null NullPointerException will be thrown'
0,'Update documentation for hadoops configuration post HADOOP785 With significant changes to hadoops configuration since HADOOP785 the documentation for it needs to be completely overhauled Exhaustive and accurate javadocs including some specific examples Update the wiki Importantly Put up a page describing hadoops configuration on the hadoop website via forrest'
0,'cleanup the dockerfile hackagehaskellorg is pretty unreliable switch to fpcompletes mirror quiet some of the output to make jenkins debugging easier some other random fixes'
0,'File does not exist when i run the spark example raises the following error Is this a bug'
0,'FsShell can suppress the real error if no error message is present The FsShell error handling assumes in displayError that the message argument is not null However in the case where it is this leads to a NPE which results in suppressing the actual error information since a higher level of error handling kicks in and just dumps the stack trace of the NPE instead This is deeply unhelpful because depending on what the underlying error was there may be no stack dumped logged for it as HADOOP7114 provides since FsShell doesnt explicitly dump traces for IllegalArgumentException which appears to be the underlying cause of my issue Line 289 is where displayError is called for IllegalArgumentException handling and that catch clause does not log the error'
0,'ML 20 QA migration guide update Update sparkml and sparkmllib migration guide from 16 to 20'
0,'Fix the java8tests profile and run those tests in Jenkins Spark has some tests for compilation of Java 8 sources using lambdas guarded behind a java8tests maven profile but we currently do not build or run those tests As a result the tests no longer compile We should fix these tests and set up automated CI so that they dont break again'
0,'Applications fail on NM restart on some linux distro because NM container recovery declares AM container as LOST On a debian machine we have seen node manager recovery of containers fail because the signal syntax for process group may not work We see errors in checking if process is alive during container recovery which causes the container to be declared as LOST 154 on a NodeManager restart The application will fail with error The attempts are not retried'
0,'Specifying remote files for Python based Spark jobs in Yarn cluster mode not working When I run a python application and specify a remote path for the extra files to be included in the PYTHONPATH using the pyfiles or sparksubmitpyFiles configuration option in YARN Cluster mode I get the following error So now its broken whether we use pyfiles or sparksubmitpyFiles as the validation gets triggered in both cases irrespective of whether we use Client or Cluster mode with YARN'
0,'Record and recover watermark We should record the watermark into the persistent log and recover it to ensure determinism'
0,'Daemon log documentation is misleading a Execute the command yarn daemonlog setlevel xxxxxxxxx 45020 ResourceManager DEBUG It is not reflecting in process logs even after performing client level operations Log level is not changed'
0,'Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider An Oozie job with a single shell action fails may not be important but if you needs the exact details I can provide them with an error message coming from NodeManager Because of this ClassCastException an uncaught exception is raised we do not see the exact caused by exception message the oozie job fails YARN logs are not reported saved'
0,'Improve test coverage for whole stage codegen'
0,'Add since tag in Roxygen documentation for SparkR This is request adding something in SparkR like versionadded in PySpark and since in Scala Java'
0,'Fix findbugs warnings in hadoopcommon branch2 There are 5 findbugs warnings in branch2'
0,'Windows distro build fails on distcopynativelibs HADOOP12892 pulled the distcopynativelibs script into an external file The call to this script is failing when running a distro build on Windows'
0,'Show batch failures in the Streaming UI landing page'
0,'Fix unrecommended syntax usages in hadoop hdfs yarn script for cygwin in branch2 Were using syntax like if $cygwin then which may be errorounsly evaluated into true if cygwin is unset We need to fix this in branch2'
0,'ZKDelegationTokenSecretManager JaasConfig does not work well with other ZK users in process Theres a race in the globals The nonglobal APIs from ZOOKEEPER2139 are not available yet in a stable ZK version and theres no timeline for availability'
0,'NoSuchMethodException thrown by UtilswaitForProcess These two configs should not be relevant anymore after Spark 20'
0,'Using ordinals in ORDER BY causes an analysis error when the query has a GROUP BY clause using ordinals However the following query does not work'
0,'Fix some warnings by findbugs in hadoopmavenplugin'
0,'Potential Issue of Semantics of BatchCompleted the current implementation of Spark streaming considers a batch is completed no matter the results of the jobs I might have missed something in the checkpoint thread or this handleJobCompletion or it is a potential bug'
0,'orgapachesparksqlexecutiondatasourcesjdbcJdbcUtilssaveTable the case senstivity issue Blindly quoting every field name for inserting is the issue I have issue with the saveTable method in Spark 20 201 I tried to save a dataset to Oracle database but the fields must be uppercase to succeed This is not an expected behavior If only the table names were quoted this utility should concern the case sensitivity The code below throws the exception Caused by javasqlSQLSyntaxErrorException ORA00904 DATETIMEgmt invalid identifier'
0,'select from temptablenocols fails In structured streaming Spark does not report errors when the specified directory does not exist This patch changes the behavior to fail if the directory does not exist when the path is not a glob pattern'
0,'Findbug compilation fails for Kafka Library support Findbug compilation is failing for Kafka Library support'
0,'S3A proxy tests fail after httpclient httpcore upgrade HADOOP12767 upgraded the httpclient and httpcore dependency versions After that I started seeing failures in S3A tests related to proxy handling If I revert that patch locally the tests pass again'
0,'Potential numerical problem in MultivariateOnlineSummarizer min max Chaining cartesian calls in PySpark results in the number of records lower than expected'
0,'PullOutNondeterministic should work for Aggregate operator This test case should pass'
0,'DataFrames pivot doesnt see column created in groupBy Im not completely sure if this is a bug or expected behavior When you groypBy by a column generated inside of it the pivot method apparently doesnt find this column during the analysis Shows the following exception'
0,'Caching data with replication doesnt replicate data If a custom Jekyll template tag throws Rubys equivalent of a file not found exception then Jekyll will stop the doc building process but will exit with a successful status causing our doc publishing jobs to silently fail This is caused by a case of bad errorhandling logic in Jekyll We should work around this in Spark to avoid more silent documentation build breaks See SPARK16553 for an example of a build failure which was ignored due to this Jekyll bug'
0,'regex strings not properly escaped in codegen for aggregations If I use the function regexpextract and then in my regex string use \ ie escape character this fails codegen because the \ character is not properly escaped when codegend'
0,'Add benchmark codes and the performance results for implemented compression schemes for InMemoryRelation This ticket adds benchmark codes for inmemory cache compression to make future developments and discussions more smooth'
0,'hadoop shell commands should print usage if not given a  rootbigtopfedora15 ~ hdfs foobar Exception in thread main javalangNoClassDefFoundError Program will exit Instead of loading any  it would be nice to explain the command is not valid and to call printusage'
0,'SaveAsTable does not work when source DataFrame is built on a Hive Table But if we change it back it will break the semantic of saveAsTable this method uses byname resolution instead of using byposition resolution used by insertInto We should correct the error messages Users can understand how to bypass it before we support it'
0,'Partition schema inference corrupts data Not sure if this is a regression from 20 to 21 I was investigating this for Structured Streaming but it seems it affects batch data as well Heres the issue The partition inference can infer it as IntegerType or I assume LongType or DoubleType basically fixed size types then once UnsafeRows are generated your data will be corrupted I was hoping to fix the issue as part of SPARK18407 but it seems its not only applicable to StructuredStreaming and deserves its own JIRA'
0,'Incorrect UserName at Solaris because it has no whoami command by default Solaris enviroment has no whoami command so the getUnixUserName  at UnixUserGroupInformation  fails because its calling to ShellUSERNAMECOMMAND which is defines as whoami So it launched an Exception and set the default DrWho username ignoring all the FileSystem permissions'
0,'Simple case in spark sql throws ParseException Simple case in sql throws parser exception in spark 20 The following query as well as similar queries fail in spark 20'
0,'SparkConf is Serializable but contains a nonserializable field Added ConfigReader to SparkConf SparkConf is Serializable but ConfigReader is not which results in the following exception'
0,'Error Handling when CTAS Against the Same Data Source Table Using Overwrite Mode When we trying to read a table and then write to the same table using the Overwrite save mode we got a very confusing error message'
0,'BIGINT and INT comparison failure in spark sql But however I get an error when trying to execute from spark sqlContext The following is the error message'
0,'BinaryType fails in Python 3 due to outdated Pyrolite Attempting to create a DataFrame using a BinaryType field fails under Python 3 because the underlying Pyrolite library is out of date Spark appears to be using Pyrolite 49 this issue was fixed in Pyrolite 412 Test case & output attached Im just a Python guy not really sure how to build Spark do classpath magic to test if this works correctly with updated Pyrolite'
0,'Returned Message Null when Hitting an Invocation Exception of Function Lookup When the exception is an invocation exception during function lookup we return a useless confusing error message Below is the error message we got'
0,'persist resolves javalangRuntimeException Invalid PythonUDF lambda requires attributes from more than one child Smells like another optimizer bug thats similar to SPARK17100 and SPARK18254 Im seeing this on 202 and on master at commit I dont have a minimal repro for this yet but the error Im seeing is Note the error at the end when Spark tries to print the physical plan Ive scrubbed some Project fields from the plan to simplify the display but if Ive scrubbed anything you think is important let me know I can get around this problem by adding a right before the operation that fails The failing operation is a filter Any clues on how I can boil this down to a minimal repro Any clues about where the problem is'
0,'joinsLongToUnsafeRowMap crashes with NegativeArraySizeException Here is a crash in Spark SQL joins with a minimal reproducible test case Interestingly it only seems to happen when reading Parquet data I added a crash True variable to show it'
0,'Scala value classes create encoder problems and break at runtime Using Scala value classes as the inner type for Datasets breaks in Spark 20 and 16X This simple Spark 2 application demonstrates that the code will compile but will break at runtime with the error The value  is of course FeatureId as it extends AnyVal'
0,'Enable the tests in HiveCompatibilitySuite for subquery There are a few test cases in HiveCompatibilitySuite for subquery we should enable them to have better coverage'
0,'Fix TestKeyProviderFactory testcases to use default 128 bit length keys TestKeyProviderFactory needs to be fixed to use new key length'
0,'Fix potential NPE in Metrics2 source for DecayRpcScheduler There is a chance of NPE if the updater thread get executed before the counters are fully initialized'
0,'SparkR style guide We should develop a SparkR style guide document based on the some of the guidelines we use and some of the best practices in R We could have a R style guide based on the one from google 1 and adjust some of them with the conversation in Spark'
0,'KMSClientProvider does not work with WebHDFS and Apache Knox w ProxyUser After upgrading to HDP 2530 noticed that all of the KMSClientProvider issues have not been resolved We put a test build together and applied HADOOP13558 and HADOOP13749 these two fixes did still not solve the issue with requests coming from WebHDFS through to Knox to a TDE zone'
0,'Fix performance bug in hash aggregate on long string keys getBytes involves memory copy is thus expensive and causes a performance degradation Fix is to evaluate getBytes before the for loop'
0,'PseudoAuthenticationHandler fails with httpcomponents v44 This shows in the context of WebHCat and Hive but could happen in other places The later returns NULL if there is no query string in the URL In httpcoponents httpclient 425 parse gracefully handles first argument being NULL but in 44 it NPEs'
0,'Deep if expressions cause Generated SpecificUnsafeProjection code to exceed JVM code size limit Problem Description I have an application in which a lot of ifelse decisioning is involved to generate output Im getting following exception Caused by orgcodehausjaninoJaninoRuntimeException Code of method Current splitting of Projection codes doesnt and cant take care of splitting the generated code for individual output column expressions So it can grow to exceed JVM limit Note This issue seems related to SPARK14887 but Im not sure whether the root cause is same'
0,'DatasetdropDuplicates ie distinct should consider the columns with same column name We find and get the first resolved attribute from output with the given column name in DatasetdropDuplicates When we have the more than one columns with the same name Other columns are put into aggregation columns instead of grouping columns We should fix this'
0,'Incorrect results from subquery transformation The correct result of the above query should be an empty set'
0,'Design Document for Credential Provider API Provide detailed overview of the design intent and use of the credential management API'
0,'TestDelegationTokenFetcher testDelegationTokenWithoutRenewer is failing Unfortunately the unit test TestDelegationTokenFetcher testDelegationTokenWithoutRenewer that asserts the message string was not updated accordingly The unit test is failing in both trunk and branch2 branches see example builds This JIRA is to track the effort of fixing this'
0,'ARRAY equality is broken in Spark 20 The following Spark shell reproduces this issue'
0,'Spark Aggregate function LAST returns null on an empty partition Issue description The result from a query that uses the LAST function are incorrect The output obtained for the column that corresponds to the last function is null The result from the query executed for the LAST column call is NULL which I believe is due to the PARTIALLAST on the last partition I believe that this behavior is incorrect The PARTIALLAST call on an empty partition should not return null'
0,'Update release notes for 300alpha1 Per the release instructions we need to update to reflect the right versions new features and big improvements I can put together some notes for HADOOP and HDFS depending on others for YARN and MR'
0,'Potential deadlock in StandaloneSchedulerBackenddead StandaloneSchedulerBackenddead is called in a RPC thread so it should not call SparkContextstop in the same thread SparkContextstop will block until all RPC threads exit if its called inside a RPC thread it will be deadlock'
0,'Mapreduce Job Failed due to failure fetching mapper output on the reduce side The job failed with the reducer failed to fetch the output from mappers see the following stacktrace The problem is that in JIRA MAPREDUCE1784 it added support to handle null compressors to default to noncompressed output In this case when the ionativelibavailable is set to false the compressor will be null However the decompressor has a Java implementation so when the reducer tries to read the mapper output it uses the decompressor but the output does not have the Gzip header'
0,'MiniKdc throws address in use BindException Another address in use error I saw the test testNameRules failed due to porting binding error It appears that the port may be used in between and then throw the exception'
0,'SparkSQL Failed to create table due to catalog string error Trying to build spark 162 but it fails because the maven 333 is gone Is this something that we control I saw the latest 20 builds were updating to 339 and that exists there'
0,'Applications using FileContext fail with the default file system configured to be wasb s3 etc HADOOP11618 fixed a bug with DelegateToFileSystem using the wrong default port As a side effect of this patch file path URLs that previously had no port now insert 0 for the port as per the default implementation of FileSystem getDefaultPort At runtime this can cause an application to erroneously try contacting port 0 for a remote blob store service The connection fails'
0,'Kafka 010 commitQueue needs to be drained Current implementation is just iterating not polling and removing'
0,'Query with Broadcast Hash join fails due to executor OOM in Spark 20 A query which used to work in Spark 16 fails with executor OOM in 20'
0,'Spark 20 history server summary page gets stuck at loading history summary with 10K+ application history The summary page of Spark 20 history server web UI keep displaying Loading history summary all the time and crashes the browser when there are more than 10K application history event logs on HDFS'
0,'hadoop native build fails to detect javalibarch on ppc64le error Bad exit status from var tmp rpmtmpsuXMUs build'
0,'Complex query triggers binding error in HashAggregateExec The following example runs successfully on Spark 200 but fails in the current master This fails with the following exception Note that this error occurs during query execution not during analysis or physical planning'
0,'Potential deadlock in driver handling message So it would cause a deadlock We have found the issue in our deployment it would block the driver to make it handle no messages until the two message all went timeout'
0,'SQLbased three column join loses first column I hope that this is not a known issue I havent had any luck finding anything similar in Jira or the mailing lists but I could be searching with the wrong terms I just started to experiment with Spark SQL and am seeing what appears to be a bug When using Spark SQL to join two tables with a three column inner join the first column join is ignored The example code that I have starts with two tables T1 Is this expected I started to research this a bit and one thing that jumped out at me was the ordering of the HashedRelationBroadcastMode concatenation in the plan this is from the b c d ordering If this concatenated byte array is ever truncated to 64 bits in a comparison the leading column will be lost and could result in this behavior'
0,'Vectorized parquet reader fails to read certain fields from Hive tables this will fail analysis'
0,'Filter rows with null attributes in parquet vectorized reader Its common for many SQL operators to not care about reading null values for correctness Currently this is achieved by performing isNotNull checks for all relevant columns on a perrow basis Pushing these null filters in parquet vectorized reader should bring considerable benefits especially for cases when the underlying data doesnt contain any nulls or contains all nulls'
0,'SELECT distinct does not work if there is a order by clause The plan is wrong because the analyze somehow added struct 21805 to the project list which changes the semantic of the distinct If you use the following query you will get the correct result'
0,'Improve Spark ML user guide improve mlguide replace ML Dataset by DataFrame to simplify the abstraction remove links to Scala API doc in the main guide change ML algorithms to pipeline components'
0,'Output javadoc inside the target directory HADOOP8500 cleaned up the javadoc build but as a result we now have a few javadoc dirs being created outside target folders Thanks to ~aw for finding this issue over on HADOOP8500'
0,'A large Metadata filed in Alias can cause OOM when calling TreeNodetoJSON When using MLLib when calling toJSON on a plan with many level of subqueries it may cause out of memory exception with stack trace'
0,'Inferred partition columns cause assertion error Fails when you run a stream against json data that is stored in partitioned folders if you manually specify the schema and that schema omits the partitioned columns While we are fixing this bug it would be nice to make the assertion better Truncating is not terribly useful as at least in my case it truncated the most interesting part I changed it to this while debugging'
0,'Folder deletion after globbing may fail StructuredStreaming jobs The FileStreamSource used by StructuredStreaming first resolves globs and then creates a ListingFileCatalog which listFiles with the resolved glob patterns If a folder is deleted after glob resolution but before the ListingFileCatalog can list the files we can run into a FileNotFoundException This should not be a fatal exception for a streaming job However we should include a warn message'
0,'NPE thrown by ClientWrapperconf This issue has been fixed in Spark 20 Seems ClientWrapperconf is trying to access the ThreadLocal SessionState which has been set'
0,'Fix flaky test cleanup map and window'
0,'Checkstyle failing Unable to instantiate DoubleCheckedLockingCheck HDFS builds are failing in jenkins'
0,'Spark should not silently drop exceptions in file listing The following works with spark 151 but not anymore with spark 160 I can understand why tables with no columns might not be supported in SQL but in that case I would say that the dfNoColsregisterTempTable call should fail with a more descriptive error'
0,'Fix findbugs warning in VersionInfoMojojava'
0,'Automatically update doc versions The docs version is hard coded It should be updated automatically'
0,'Enumerate Sparks dependencies in a file and diff against it for new pull requests Sometimes when we have dependency changes it can be pretty unclear what transitive set of things are changing If we enumerate all of the dependencies and put them in a source file in the repo we can make it so that it is very explicit what is changing'
0,'App Name is a randomUUID even when sparkappname exists When submitting an application with name The application orgapachesparkexamplesSparkKMeans above did not invoke appName'
0,'DatasetjoinWith broadcasts gigabyte sized table causes OOM Exception The issue is that orgapachesparksqltypesArrayTypedefaultSize is of datatype Int In my dataset there is an Array column whose data size exceeds the limits of an Int and so the data size becomes negative The issue can be repeated by running this code in REPL This causes joinWith to performWith to perform a broadcast join even tho my data is gigabytes in size which of course causes the executors to run out of memory'
0,'Spark returns incorrect result when collect ing a cached Dataset with many columns Although this is reproducible with SparkR it seems more likely that this is an error in the Java Scala Spark sources'
0,'Fix Two Test Failures After Backport In the latest branch 20 we have two test case failure due to backport'
0,'partition calculation mismatch with scbinaryFiles Spark20error occurs when execute the sql statement which includes nvl function while spark16 supports Error orgapachesparksqlAnalysisException cannot resolve nvl bnewuser 0 due to data type mismatch input to function coalesce should all be the same type but its string int line 2 pos 73 statecode0'
0,'Poisson GLM fails due to wrong initialization Poisson GLM fails for many standard data sets The issue is incorrect initialization leading to almost zero probability and weights The following simple example reproduces the error The issue is in the initialization the mean is initialized as the response which could be zero Applying the log link results in very negative numbers protected against Inf which again leads to close to zero probability and weights in the weighted least squares The fix is easy just add a small constant highlighted in red below I already have a fix and test code Will create a PR'
0,'HAServiceProtocols health state is incorrectly transitioned to SERVICENOTRESPONDING When HAServiceProtocolmonitorHealth throws a HealthCheckFailedException the actual exception from protocol buffer RPC is a RemoteException that wraps the real exception Thus the state is incorrectly transitioned to SERVICENOTRESPONDING'
0,'NOT IN subquery with more than one column may return incorrect results When putting more than one column in the NOT IN the query may not return correctly if there is a null data We can demonstrate the problem with the following data set and query'
0,'checknative should display a nicer error message when openssl support is not compiled in checknative should display a nicer error message when openssl support is not compiled in Currently Instead we should display something like this if openssl is not supported by the current build'
0,'Fix precommit builds to execute the right set of tests I have noticed that our precommit builds could end up running the wrong set of unit tests for patches For instance YARN3412 changes only YARN files but the test were run against one of the MR modules I suspect there is a race condition when there are multiple builds executing on the same node or remnants from a previous run are getting picked up'
0,'SparkR hangs when there is download or untar failure When there is any partial download or download error it is not cleaned up and sparkRsession will continue to stuck with no error message'
0,'Hivestaging folders created from Spark hiveContext are not getting cleaned up The issue happens via Sparksubmit as well customer used the following command to reproduce this sparksubmit testhivestagingcleanuppy'
0,'fix some DDL bugs about table management when samename temp view exists'
0,'QuantileDiscretizer throws InvalidArgumentException parameter splits given invalid value on valid data I discovered this bug when working with a build from the master branch which I believe is 210 This used to work fine when running spark 162 But when that gets run it incorrectly throws this error parameter splits given invalid value Infinity 10 10 20 20 30 30 Infinity I dont think that there should be duplicate splits generated should there be'
0,'RetryInvocationHandler logs all remote exceptions RetryInvocationHandler logs a warning for any exception that it does not retry There are many exceptions that the client can automatically handle like FileNotFoundException UnresolvedPathException etc so now every one of these generates a scary looking stack trace as a warning then the program continues normally'
0,'MesosClusterScheduler generate bad command options As above conf option value is not enclosed by for example if we have a multi value config like Without the next alluxio config will be treated as sparksubmit options and cause error'
0,'Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop I was attempting to use the LdapGroupsMapping code and the JavaKeyStoreProvider at the same time and hit a really interesting yet fatal issue The code goes into what ought to have been an infinite loop were it not for it overflowing the stack and Java ending the loop Here is a snippet of the stack my annotations are at the bottom'
0,'Add ML example for SparkR Add ML example for SparkR'
0,'KMS cannot deploy on Windows because  names are too long Windows has a maximum path length of 260 characters KMS includes several long  file names During packaging and creation of the distro these paths get even longer because of prepending the standard war directory structure and our share hadoop etc structure The end result is that the final paths are longer than 260 characters making it impossible to deploy a distro on Windows'
0,'Fix exception message in WASB when connecting with anonymous credential Current implementation returns the correct message when we encounter a Storage exception however for scenarios like querying to check if a container exists does not throw a StorageException but returns false when URI is directly specified Anonymous access the error message returned does not clearly state that credentials for storage account is not provided This JIRA tracks the fix the error message to return what is returned when a storage exception is hit and also correct spelling mistakes in the error message'
0,'Fix documentation for DateDiff The current documentation for DateDiff does not make it clear which one is the start date and which is the end date The example is also wrong about the direction'
0,'FileSystemShell doc should explain relative path'
0,'Fix ClassFormatException in trunk build The mavenprojectinforeportsplugin version 27 depends on mavensharedjar11 which uses bcel 52 This does not work well with the new lamda expression'
0,'Startup scripts will not start instances of Hadoop daemons w different configs w o setting separate PID directories Configuration directories can be specified by either setting HADOOPCONFDIR or using the config command line option However the hadoopdaemonsh script will not start the daemons unless the PID directory is separate for each configuration The issue is that the code for generating PID filenames is not dependent on the configuration directory While the PID directory can be changed in hadoopenvsh it seems a little unnecessary to have this restriction'
0,'SparkR hangs at session start when installed as a package without SPARKHOME set If SparkR is running as a package and it has previously downloaded Spark Jar it should be able to run as before without having to set SPARKHOME Basically with this bug the auto install Spark will only work in the first session'
0,'Spark 150 Testing Plan This is an epic for Spark 150 release QA plans for tracking various components'
0,'distcp on mr1 branch1 fails with NPE using a short relative source path distcp on mr1 branch1 fails with NPE using a short relative source path The failure is at DistCpjava makeRelative return null at the following code'
0,'Synchronization issue in delegation token cancel functionality We are using Hadoop delegation token authentication functionality in Apache Solr As part of the integration testing I found following issue with the delegation token cancelation functionality After investigation I found the root cause for this behavior is due to the race condition between step 4 and the firing of ZK watch on S1 Whenever the watch fires before the step 4 we get HTTP 404 response as expected When that is not the case we get HTTP 200 response along with following ERROR message in the log'
0,'LogicalRelationnewInstance should follow the semantics of MultiInstanceRelation Current LogicalRelationnewInstance causes failure when doing selfjoin'
0,'Fix ASF License warnings in branch27 Please have a look following PreCommit build on branch27'
0,'limit + groupBy leads to javalangNullPointerException Using limit on a DataFrame prior to groupBy will lead to a crash Repartitioning will avoid the crash'
0,'Incorrect result when HAVING clause is added to group by query Random query generation uncovered the following query which returns incorrect results when run on Spark SQL This wasnt the original query uncovered by the generator since I performed a bit of minimization to try to make it more understandable Im not sure how to further shrink this in a straightforward way so Im opening this bug to get help in triaging further'
0,'Executor OOM due to a memory leak in BytesToBytesMap While running a Spark job we see that the job fails because of executor OOM with following stack trace The code is trying to reuse the BytesToBytesMap after spilling by calling the reset function The reset function is releasing all memory pages but its not reseting the pointer array If the pointer array size has grown beyond the fair share the BytesToBytes map is not being allocated any memory page further and hence the OOM'
0,'ForeachSink fails with assertion failed No plan for EventTimeWatermark I have a pretty standard stream I call writeStreamforeachstart and get error'
0,'Stackoverflow for schemeless defaultFS with trailing slash Command hadoop fs fs 172161279 mkdir p usr hduser Results in a Stack Overflow The Problem is the Slash at the End of the IP Address'
0,'Add missing dependency in setting mavenremoteresourceplugin to fix builds After HADOOP12893 we are seeing mvn install DskipTests failing in branch27 branch273 and branch26 This failure is caused by the followings The build failure occurs if the both of the above conditions are satisfied'
0,'Fix wrong AIC calculation in Binomial GLM The AIC calculation in Binomial GLM seems to be wrong when there are weights The result is different from that in R'
0,'RDD zipWithIndex generate wrong result when one partition contains more than 2147483647 records RDD zipWithIndex generate wrong result when one partition contains more than IntMaxValue records When RDD contains a partition with more than 2147483647 records error occurs When we do some operation such as repartition or coalesce it is possible to generate big partition so this bug should be fixed'
0,'Deadlock when SparkContextstop is called in UtilstryOrStopSparkContext When SparkContextstop is called in UtilstryOrStopSparkContext the following three places it will cause deadlock because the stop method needs to wait for the thread running stop to exit'
0,'Fix DelegationTokenAuthenticatedURL to pass the connection Configurator to the authenticator KMS DelegationToken operation keeps throwing unable to find valid certification path to requested target It looks very much like the truststore is not being picked up'
0,'OOM killer may leave SparkContext in broken state causing Connection Refused errors When you run some memoryheavy spark job Spark driver may consume more memory resources than host available to provide In this case OOM killer comes on scene and successfully kills a sparksubmit process The pysparkSparkContext is not able to handle such state of things and becomes completely broken You cannot stop it as on stop it tries to call stop method of bounded java context jsc and fails with Py4JError because such process no longer exists as like as the connection to it You cannot start new SparkContext because you have your broken one as active one and pyspark still is not able to not have SparkContext as sort of singleton The only thing you can do is shutdown your IPython Notebook and start it over Or dive into SparkContext internal attributes and reset them manually to initial None state The OOM killer case is just one of the many any reason of sparksubmit crash in the middle of something leaves SparkContext in a broken state'
0,'HBase classes fail to load with client job classloader enabled Currently if a user uses HBase and enables the client job classloader the job fails to load HBase classes'
0,'TestRPCtestRPCInterruptedSimple fails intermittently Jenkins trunk + java 8 saw a failure of TestRPCtestRPCInterruptedSimple the interrupt wasnt picked up Race in test or a surfacing of a bug in RPC where at some points interrupt exceptions are not picked up'
0,'Improve the explain of wholestage codegen The problem is that the plan looks much different than logical plan make us hard to understand the plan especially when the logical plan is not showed together'
0,'Spark 16 Scala211 repl doesnt honor sparkreplClassServerport Spark 16 Scala211 repl doesnt honor sparkreplClassServerport configuration so user cannot set a fixed port number through sparkreplClassServerport'
0,'scalaMatchError BooleanType when casting a struct I have a Dataframe with a struct and I need to rename some fields to lower case before saving it to cassandra It turns out that its not possible to cast a boolean field of a struct to another boolean field in the renamed struct A workaround is to temporarily cast the field to an Integer and back'
0,'Property iocompressioncodeclzoclass does not work with other value besides default From following code seems iocompressioncodeclzoclass does not work for other codec besides default Hadoop will always treat it as defaultClazz I think it is a bug Please let me know if this is a work as design thing'
0,'Throw an Exception when fspermissionsumaskmode is misconfigured provide better visibility of parsing configuration failure by logging full error message and propagate error message of parsing configuration back to client'
0,'hdfs and nfs builds broken on missing compiletime dependency on netty As discovered in BIGTOP2049 hadoopnfs module compilation is broken Looks like that HADOOP11489 is the rootcause of it'
0,'Comparing Vector in relative tolerance or absolute tolerance in UnitTests error The result of compare two vectors using UnitTests orgapachesparkmllibutilTestingUtils is not right sometime'
0,'Filter join expressions can return incorrect results when comparing strings to longs The physical plan shows both sides of the expression are being cast to Double before evaluation So while comparing numbers to a string number appears to work in many cases when the numbers are sufficiently large and close together there is enough loss of precision to cause incorrect results Expected behavior in this case is probably to choose one side and cast the other compare string to string or long to long instead of using a data type with less precision'
0,'SparkR installspark does not work for RCs snapshots We publish source archives of the SparkR package now in RCs and in nightly snapshot builds One of the problems that still remains is that installspark does not work for these as it looks for the final Spark version to be present in the apache download mirrors'
0,'mapredsystemdir parameter needs documentation We should better document configuring things for fully distributed operation In particular we should probably recommend setting mapredsystemdir in the Fully Distributed Operation section'
0,'IsolatedClientLoader ignores needed Hadoop classes not present in Sparks loader Currently we do not detect whether the expression in LIMIT clause is foldable or not If nonfoldable we might issue a strange error message Then a misleading error message is issued like'
0,'StorageException complaining no lease ID when updating FolderLastModifiedTime in WASB This is a similar issue to HADOOP11523 HADOOP11523 happens when HBase is doing distributed log splitting The fix is the same as HADOOP11523'
0,'Issue Exceptions when Analyze Table on InMemory Cataloged Tables Currently Analyze Table is only for Hiveserde tables We should issue exceptions in all the other cases When the tables are data source tables we issued an exception However when tables are InMemory Cataloged tables we do not issue any exception'
0,'Update Structured Streaming Programming guide and documentation for Update Mode'
0,'Fix compilation failure from missing hadoopkms test jar mvn install fails for me on trunk on a new environment'
0,'Master UI should show the correct core limit when ApplicationInfoexecutorLimit is set The core info of an application in Master UI doesnt consider ApplicationInfoexecutorLimit Its pretty confusing that UI says Unlimited when executorLimit is set'
0,'when setnetgrent returns 0 in linux exception should be thrown In linux setnetgrent returns 0 in linux when something wrong is happen such as out of memory unknown group unavailable service etc So errorMessage should be set and exception should be thrown'
0,'Chained cartesian produces incorrect number of records Spark20 cannot select data from a table stored as an orc file which has been created by hive while hive or spark16 supports'
0,'Implementations of InputStreamread buffer offset bytes to exit 0 if bytes0 HDFS10277 showed that HDFS was return 1 on read buf 0 0 when there was no data left in the stream Java IO says bq If len is zero then no bytes are read and 0 is returned otherwise there is an attempt to read at least one byte Review the implementations of IOStream buffer offset bytes and where necessary and considered safe add a fast exit if the length is 0'
0,'Inappropriate memory management in orgapachesparkstorageMemoryStore may lead to memory leak After updating Spark from 150 to 160 I found that it seems to have a memory leak on my Spark streaming application It shows that scalacollectionmutableDefaultEntry and javalangLong have unexpected big numbers of instances After some further investigation I found that the problem is caused by some inappropriate memory management in releaseUnrollMemoryForThisTask and unrollSafely method of '
0,'Additional fix to LICENSE and NOTICE Fix up LICENSE and NOTICE after HADOOP12893'
0,'Fix SparkR SQL Test to drop test table Currently SparkR tests R runtestssh succeeds only once because testsparkSQLR does not clean up the test table people As a result the test data is accumulated at every run and the test cases fail'
0,'PySpark SQL pythononly UDTs dont support nested types This is observed while debugging We should fix it or disable it by default'
0,'MetricsSinkAdapter hangs when being stopped Weve seen a situation that one RM hangs on stopping the MetricsSinkAdapter looks like the sinkThreadinterrupt in MetricsSinkAdapter stop doesnt really interrupt the thread which cause it to hang at join'
0,'ReplSuite fails with ClassCircularityError in master Maven builds The master Maven build is currently broken because ReplSuite consistently fails with ClassCircularityErrors See for a timeline of the failure Heres the first build where this failed'
0,'TLP site should have a community section I think we should add a community section to the TLP website that deep links to the corresponding pages Ill attach the generated pdf of the site'
0,'DataFrame fill after pivot causing orgapachesparksqlAnalysisException I am trying to run a pivot transformation which I ran on a spark16 cluster after upgrade the environment to spark20 got an error while executing nafill method'
0,'Variable cygwin is undefined in hadoopconfigsh when executed through hadoopdaemonsh HADOOP11464 reinstated support for running the bash scripts through Cygwin The logic involves setting a cygwin flag variable to indicate if the script is executing through Cygwin The flag is set in all of the interactive scripts hadoop hdfs yarn and mapred The flag is not set through hadoopdaemonsh though This can cause an erroneous overwrite of HADOOPHOME and JAVALIBRARYPATH inside hadoopconfigsh'
0,'New Hadoop Common Site New Hadoop Common Site Set up site initial pass May need to add more content May need to update some links'
0,'Remove ThirdParty Hadoop Distributions Doc Page There is a fairly old page in our docs that contains a bunch of assorted information regarding running Spark on Hadoop clusters I think this page should be removed and merged into other parts of the docs because the information is largely redundant and somewhat outdated There are three sections Compile time Hadoop version this information I think can be removed in favor of that on the building spark page These days most advanced users are building without bundling Hadoop so Im not sure giving them a bunch of different Hadoop versions sends the right message Linking against Hadoop this doesnt seem to add much beyond what is in the programming guide Where to run Spark redundant with the hardware provisioning guide Inheriting cluster configurations I think this would be better as a section at the end of the configuration page'
0,'Query planning slows down dramatically for large query plans even when subtrees are cached The following Spark shell snippet creates a series of query plans that grow exponentially We can see that although all plans are cached the query planning time still grows exponentially and quickly becomes unbearable Similar scenarios can be found in iterative ML code and significantly affects usability This issue can be fixed by introducing a checkpoint for Dataset that truncates both the query plan and the lineage of the underlying RDD'
0,'SELECT 1 NULL throws AnalysisException while SELECT 1 NULL works Running SELECT 1 NULL fails with orgapachesparksqlAnalysisException cannot resolve 1 NULL due to data type mismatch differing types in 1 NULL int and null'
0,'query fails if having condition contains grouping column this query fails in 20 but works in 16'
0,'Aggregator fails with Tungsten error when complex types are used for results and partial sum In ListingFileCatalog the implementation of listLeafFiles is shown below When the number of userprovided paths is less than the value of parallelPartitionDiscoveryThreshold we will not use parallel listing which is different from what 16 does'
0,'Timeouts shouldnt be AssertionErrors A timeout should inherit from RuntimeException as its not a fatal error'
0,'Fix deadlinks in Compatibilitymd There are 2 dead links in Compabilitymd The links to MRAppMaster JobHistoryServer REST API are wrong'
0,'DataSet API | RuntimeException Null value appeared in nonnullable field when holding Option Case  I am running into a runtime exception when a DataSet is holding an Empty object instance for an Option type that is holding nonnullable field For instance if we have the following case  Then DataSetOptionDataRow can only hold Some DataRow objects and cannot hold Empty If it does so the following exception is thrown The bug can be reproduce by using the program'
0,'Add link to training from website Add a link to training videos from the getting started section'
0,'Misleading Error Message for Aggregation Without Window GroupBy The following error message points to a random column Im not actually using in my query making it hard to diagnose'
0,'UnsatisifedLinkError with hadoop 24 JARs on hadoop26 due to NativeCRC32 method changes The private native method names and signatures in NativeCrc32 were changed in HDFS6561 as a result hadoopcommon24 JARs get unsatisifed link errors when they try to perform checksums This essentially stops Hadoop 24 applications running on Hadoop 26 unless rebuilt and repackaged with the hadoop 26 JARs'
0,'ShellcheckIsBashSupported swallowed an interrupted exception However its error message is misleading and the logic should be updated If the shell command throws an IOException it does not imply the bash did not run successfully If the shell command process was interrupted its internal logic throws an InterruptedIOException which is a subclass of IOException The test logic in TestRPCWaitForProxytestInterruptedWaitForProxy starts a thread wait it for 1 second and interrupt the thread expecting the thread to terminate However the method ShellcheckIsBashSupported swallowed the interrupt and therefore failed'
0,'Fix findbugs warnings in hadoopsls There are 13 warnings to be fixed'
0,'Fix hadoopcommon to generate jdiff Hadoopcommon failed to generate JDiff We need to fix that'
0,'Performance test for ALS in Spark 20 We made several changes to ALS in 20 It is necessary to run some tests to avoid performance regression We should test synthetic datasets from 1 million ratings to 1 billion ratings'
0,'FsPermission string constructor does not recognize sticky bit FsPermissionss string constructor breaks on valid permission strings like 1777'
0,'LeastSquaresAggregator in Linear Regression serializes unnecessary data This is basically the same issue as SPARK16008 but for linear regression where coefficients and featuresStd are unnecessarily serialized between stages'
0,'PySpark does not work with Python 360 Currently PySpark does not work with Python 360 Running bin pyspark simply throws the error as below'
0,'Flags for posixfadvise are not valid in some architectures In orgapachehadoopionativeioNativeIOjava the posixfadvise flag parameter is hardcoded to the most common values in fcntlh This bug results in calls to posixfadvise failing in zLinux'
0,'Fix HLL++ with small relative error In HyperLogLogPlusPlus if the relative error is so small that p 19 it will cause ArrayIndexOutOfBoundsException in THRESHOLDS p4 We should check p and when p 19 regress to the original HLL result and use the small range correction they use'
0,'JWTRedirectAuthenticationHandler doesnt Retain Original Query String An originally requested URL that contains a query string gets translated into an originalURL query parameter without the original query string This can cause the redirect back to the requested resource to be invalid'
0,'Fix UserGroupInformationjava to support 64bit zLinux Currently the 64 bit check in security UserGroupInformationjava uses osarch and checks for 64 s390x is returned on IBMs z platform s390x is 64 bit Without this change if we try to use HDFS with Spark we get a fatal error unable to login as we cant find a login  This address fixes said issue by identifying s390x as a 64 bit platform and thus allowing Spark to run on zLinux A simple fix with very big implications'
0,'Improve authentication failure WARN message to avoid user confusion Lots of the following messages appeared in NN log The real reason of failure is the second message about StandbyException However the first message is confusing because it talks about DIGESTMD5 IO error acquiring password Filing this jira to modify the first message to have more comprehensive information that can be obtained from getCauseForInvalidToken e'
0,'Set meaningful job descriptions for streaming related jobs Job descriptions will help distinguish jobs of one batch from the other in the Jobs and Stages pages in the Spark UI'
0,'Grep job in Single Cluster document fails In single cluster setup document the grep job fails'
0,'huge log files On our system its not uncommon to get 20 MB of logs with each MapReduce job It would be very helpful if it were possible to configure Hadoop daemons to write logs only when major things happen but the only conf options I could find are for increasing the amount of output The disk is really a bottleneck for us and I believe that short jobs would run much more quickly with less disk usage We also believe that the high disk usage might be triggering a kernel bug on some of our machines causing them to crash If the 20 MB of logs went down to 20 KB we would probably still have all of the information we needed'
0,'List Binary and Source Compatibility Issues with japicompliance checker To identify potential API issues list public API changes which affect binary and source incompatibility by using command Report result attached'
0,'Inconsistent behavior after writing to parquet files Found an inconsistent behavior when using parquet'
0,'Use DirectoryStream in DiskChecker checkDirs to detect errors when listing a directory DiskChecker checkDirs should check null pointer for the return value from File listFiles Based on the document for File listFiles at So it will be good to check null pointer and throw DiskErrorException if it is null'
0,'flush sparksql command line history to history file currently sparksql would not flush command history when exiting'
0,'Add LICENSEtxt entries for bundled javascript dependencies None of our bundled javascript dependencies are mentioned in LICENSEtxt Lets fix that'
0,'A command to print JAVAVERSION used by Hadoop HDFS The java version used by hadoop is controlled by JAVAHOME variable defined in hadoopenvsh We log this information when HDFS starts in the log file However it is quite possible that a user might have many versions of java installed on his her machine Generally users tend to check for the java version via This just means we are printing out the java version in the current shell path This jira proposes adding a new simple command or an extension to existing hadoop version command where the current java version used by hadoop is also printed out This avoids customer confusion when they are looking at if the java stack is properly configured For example checking if JCE is installed correctly This is a very minor change that can be done by modifying hdfscmd or hdfs shell script in the bin directory Thanks to ~sujit for bringing this to my attention'
0,'Update release note in indexmdvm still shows the new features of Hadoop 25 The document should be updated'
0,'Add tests to make sure that mlclassificationLogisticRegression returns meaningful result when labels are the same without intercept When all labels are the same its a dangerous ground for LogisticRegression without intercept to converge GLMNET doesnt support this case and will just exit GLM can train but will have a warning message saying the algorithm doesnt converge'
0,'hadoop key command usage is not documented I found hadoop key command usage is not documented when reviewing HDFS9784 In addition we should document that uppercase is not allowed for key name'
0,'TestSymlinkLocalFSFileContext testSetTimesSymlinkToDir occasionally fail I have observed this test failure a few times in the past When it fails the expected access time of the file link is always 1000 less than the actual access time Error Message Stacktrace javalangAssertionError'
0,'DOC Options are caseinsensitive since Spark 21 After resolving the JIRA the documentation needs an update'
0,'SerializationDebugger run into infinite loop Breaks compilation where the compiler needs a method with single arg This works in 16 and does not compile in 20'
0,'Restore lost leveldbjni LICENSE and NOTICE changes As noted on HADOOP12893 we lost the leveldbjni related NOTICE and LICENSE updates done in YARN1704 when HADOOP10956 was committed Lets restore them'
0,'Python UDF does not work between Sort and Limit Because of this bug Python UDF will not work with ORDER BY and LIMIT'
0,'ZKFailoverController does not log Exception when doRun raises errors In ZKFailoverControllerjava the Exception caught by the run  method does not have a single error log This causes latent problems that are only manifested during failover h5 The problem we encountered An Exception is thrown from the doRun  method during initHM  caused by a configuration error If you want to repeat you can set hahealthmonitorconnectretryintervalms to be any nonsensical value Unfortunately the Exception causing the shutdown of the process is not logged at all This causes latent errors which is only manifested during failover because ZKFC is dead h5 Patch We strongly suggest to add a error log to notify the error caught such as'
0,'Fix SparkR tests on Windows A number of SparkR tests are current failing when run on Windows as discussed'
0,'Fix findbugs warnings in hadooptools module There are 2 warnings in hadoopdatajoin module and 4 warnings in hadoopant module'
0,'Flaky test networksaslSaslIntegrationSuitetestNoSaslClient Saw many failures of this test recently eg Failing for the past 1 build Since Failed 3542 Took 1 ms Error Message'
0,'X86 build of libwinutils is broken Hadoop9922 recently fixed x86 build After YARN2190 compiling x86 results in error'
0,'SparkListenerDriverAccumUpdates event does not deserialize properly in history server The following test fails with a ClassCastException due to oddities in how Jackson object mapping works breaking the SQL tab in the history server'
0,'jdiff is broken in Hadoop 2 Seems like we havent touch the API files from jdiff under devsupport for a while For now were missing the jdiff API files for hadoop 2 Were also missing YARN when generating the jdiff API files'
0,'script transformation does not work on Windows due to fixed bash executable location There are some tests failed on Windows via AppVeyor'
0,'Illegal Inputs In LIMIT or TABLESAMPLE When creating a view a common user error is the number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW Currently Spark SQL reports the following error This error is very confusing'
0,'Add more comment in HiveTypeCoercion for type widening'
0,'regexpextract with optional groups causes NPE When create below table like below schema Spark SQL error out for struct type Error Message'
0,'SparkR 20 This is an umbrella issue addressing all SparkR related issues corresponding to Spark 20 being planned'
0,'alter table tablename drop partition with a empty string will drop the whole table Maybe the partition match have something wrong when the partition value is set to empty string'
0,'joinsLongToUnsafeRowMap crashes with ArrayIndexOutOfBoundsException Ive had a few other errors that I havent managed to reproduce so far as well as what I suspect could be memory leaks I have a query in a loop OOMing after a few iterations despite not caching its results Here is the script to reproduce the ArrayIndexOutOfBoundsException on the current 20 branch'
0,'isin causing SQL syntax error with JDBC When using a JDBC data source the isin function generates invalid SQL syntax when called with an empty array which causes the JDBC driver to throw an exception'
0,'Incorrect results returned following a join of two datasets and a map step where total number of columns 100 We have hit a consistent bug where we have a dataset with more than 100 columns I am raising as a blocker because spark is returning the WRONG results rather than erroring leading to data integrity issues'
0,'Insertion CTAS against Hive Tables Staging Directories and Data Files Not Dropped Until Normal Termination of JVM Below are the files directories generated for three inserts againsts a Hive table The first 18 files are temporary We do not drop it until the end of JVM termination If JVM does not appropriately terminate these temporary files directories will not be dropped'
0,'Alias specified for aggregates in a pivot are not honored When using pivot and multiple aggregations we need to alias to avoid special characters but alias does not help One approach you can fix this issue is to change the  Analyzerscala and change the outputName method'
0,'We should add a link to the PoweredBy page from the left navigation bar Currently the PoweredBy page is hard to find I think we should add link from the left navigation bar'
0,'UnsafeShuffleWriter corrupts encrypted shuffle files when merging The merging algorithm in UnsafeShuffleWriter does not consider encryption and when it tries to merge encrypted files the result data cannot be read since data encrypted with different initial vectors is interleaved in the same partition data This leads to exceptions when trying to read the files during shuffle'
0,'Stabilize branch1win Most of the code changes to make Hadoop branch 1 work natively on Windows are done This jira in intended to track the work needed to achieve 100 test pass for the dev tests'
0,'Spark 20 history server web Ui takes too long for a single application When there are 10K application history in the history server back end it can take a very long time to even get a single application history page After some investigation I found the root cause was the following piece of code Although all application history infos are stored in a LinkedHashMap here to code transforms the map to an iterator and then uses the find api which is O n instead of O 1 from a mapget operation
0,'Concurrent Fetching DataFrameReader JDBC APIs Do Not Work The above two DataFrameReader JDBC APIs ignore the userspecified parameters of parallelism degree'
0,'Remove unimplemented option for hadoop fs ls from document in branch27 The feature is unimplemented in 270 but documented We should fix the document'
0,'The failed stage never resubmitted due to abort stage in another thread there is a race condition when FetchFailed and resubmit failed stage job1 job2 run in different threads if job 1 failed 4 times due to fetchfailed and aborted then job2 can not post ResubmitFailedStages becase the failedStages in DAGScheduler is not empty now'
0,'Dataframe except returns incorrect results when combined with coalesce We were getting incorrect results from the DataFrame except method all rows were being returned instead of the ones that intersected Calling subtract on the underlying RDD returned the correct result We tracked it down to the use of coalesce the following is the simplest example case we created that reproduces the issue We should get the same result from both uses of except but the one using coalesce returns 100 instead of 94'
0,'Specify PositionedReadable add contract tests fix problems Some work on S3a has shown up that there arent tests catching regressions in readFully reviewing the documentation shows that its specification could be improved review the spec review the implementations add tests proposed to the seek contract streams which support seek should support positioned readable fix code where it differs significantly from HDFS or LocalFS'
0,'NPE if hosts not specified in ProxyUsers When using the TokenDelegationAuthenticationFilter I noticed if I dont specify the hosts for a user groups proxy user and then try to authenticate I get an NPE rather than an AuthorizationException'
0,'Test failing hadoopipcTestSaslRPC hadoopipcTestSaslRPC is broken post HADOOP12324 string coming in exception is more detailed than the test expects emergency patch in progress'
0,'Fix computing CPU usage statistics on Windows The CPU usage information on Windows is computed incorrectly The proposed patch fixes the issue and unifies the the interface with Linux'
0,'hadoopcommon native compilation fails on Windows due to missing support for attribute declaration HADOOP11403 made a change to include exceptionh in NativeIOc This header includes use of the nonstandard gcc \\attribute\\ declaration and thus fails compilation on Windows'
0,'better error message for writing bucketing data'
0,'Reading Cataloged Data Sources without Extending SchemaRelationProvider For data sources without extending SchemaRelationProvider we expect users to not specify schemas when they creating tables If the schema is input from users an exception is issued Since Spark 21 for any data source to avoid infer the schema every time we store the schema in the metastore catalog Thus when reading a cataloged data source table the schema could be read from metastore catalog In this case we also got an exception'
0,'Clean up some htrace integration issues Clean up some htrace integration issues'
0,'BZip2CompressionInputStream finds the same compression marker twice in corner case causing duplicate data blocks Unit test TestTextInputFormattestSplitableCodecs failed when the seed is 1313094493'
0,'Website shows incorrect 25 January 2016 as the release date for 273 shows 273 released on 25 January 2016 which should rather be 25 August 2016'
0,'Regular expression replace throws NullPointerException when serialized This query fails with a NullPointerException The problem is that POSEXPLODE is causing the REGEXPREPLACE to be serialized after it is instantiated The null value is a transient StringBuffer that should hold the result The fix is to make the result value lazy'
0,'Update R documentation on ml model summary It has been discovered that there is a fair bit of consistency in the documentation of summary functions For instance what should be listed for the return value Should it be a name or a phrase or should it be a list of items and should there be a longer description on what they mean or reference link to Scala doc We will need to review this for all model summary in mllibR'
0,'Spark20 cannot select data from a table stored as an orc file which has been created by hive while hive or spark16 supports Suppose we have such Spark code It uses a bash script to convert a string to its length So far so good but when I run the code it prints incorrect output I think its a bug Its expected to see only positive integers and avoid zeros'
0,'YarnShuffleService doesnt reinit properly on YARN rolling upgrade When a yarn rolling upgrade happens the Spark YarnShuffleService isnt reinitializing the tokens soon enough which causes running applications to fail with NullPointerExceptions rather then IOExceptions which causes clients to not retry which in turn causes the application to totally fail when it should have just retried and succeeded'
0,'User document for UserGroupInformationdoAs A user document should be added for secure impersonation feature The document should cover a code example about how UserGroupInformationdoAs should be used for secure impersonation'
0,'Correlated subqueries containing nondeterministic operators return incorrect results Correlated subqueries with LIMIT could return incorrect results'
0,'Improves the error message when fails to parse some json file lines in DataFrameReader We silently replace corrupted line with null without any error message'
0,'Need to set version name correctly before decrypting EEK Touchzing a file results in a Null Pointer Exception'
0,'LASTVALUE FALSE OVER throws IndexOutOfBoundsException Calling persist on a data frame with more than 200 columns is removing the data from the data frame This is an issue in Spark 162 Works with out any issues in Spark 161'
0,'GetExternalRowField does not properly escape field names causing generated code not to compile The following endtoend test uncovered a bug in GetExternalRowField Here the problem is that the autogenerated field name contains special characters including backslashes and those arent escaped when being interpolated into the generated code causing the invalid string literal We need to update GetExternalRowField to escape field names and also need to audit other expressions to make sure that were not making the same mistake there'
0,'Remainder expressioneval returns incorrect result Problem Remainder expression returns incorrect result when using expressioneval to calculate the result expressioneval is called in case like constant folding'
0,'testpatch javac warning check reporting total number of warnings instead of incremental javac result should report incremental number of warnings'
0,'Restore Rack Awareness documentation As part of HADOOP8427 large extremely useful sections of the Rack Awareness documentation that was added in HADOOP6616 was wiped out We should restore it as a separate document'
0,'pyspark filter on a udf column after join gives javalangUnsupportedOperationException In pyspark when filtering on a udf derived column after some join types the optimized logical plan results is a javalangUnsupportedOperationException It fails when the join is'
0,'4digit octal umask permissions throws a parse error Providing a 4digit octal number for fs permissions leads to a parse error'
0,'CreateOrReplaceTempView throws Exception when viewName first char is numerical Using a viewName where the the fist char is a numerical value on dataframecreateOrReplaceTempView viewName String causes Exception'
0,'RemoveAliasOnlyProject should not remove alias with metadata When doing a CTAS with a Partition By clause we got a wrong error message For example Operation not allowed Schema may not be specified in a Create Table As Select CTAS'
0,'jira S3a phase II robustness scale and performance HADOOP11571 covered the core s3a bugs surfacing in Hadoop26 & other enhancements to improve S3 performance proxy custom endpoints This JIRA covers post27 issues and enhancements'
0,'Empty blocks fail to serialize due to assert in ChunkedByteBuffer Three broadcast variables created at the beginning of Word2Vecfit are never deleted nor unpersisted'
0,'Infinite recursion loop in orgapachesparksqlcatalysttreesTreeNode when table name collides This make a nice StackOverflowError This does not happen if I change the name of the CTE I guess Catalyst get caught in an infinite recursion loop because the CTE and the source table have the same name'
0,'Fix native compilation on Windows after HADOOP7824 HADOOP7824 introduced a way to set the java static values for POSIX flags this resulted in compilation error in Windows'
0,'StructType doesnt accept Python dicts anymore I found this issue while testing my codebase with 200rc5 StructType in Spark 162 accepts the Python dict type which is very handy 200rc5 does not and throws an error I dont know if this was intended but Id advocate for this behaviour to remain the same MapType is probably wasteful when your key names never change and switching to Python tuples would be cumbersome'
0,'not isnotnull is converted to the always false condition isnotnull && not isnotnull When a logical plan is built containing the following somewhat nonsensical filterDuring optimization the filter is converted into a condition that will always fail This appears to be caused by the following check for NullIntolerant Which recurses through the expression and extracts nested IsNotNull calls converting them to IsNotNull calls on the attribute at the root level This results in the nonsensical condition above'
0,'TestCertificateUtiltestCorruptPEM failing on Jenkins JDK8 Jenkins is failing on TestCertificateUtiltestCorruptPEM'
0,'SparkR Kmeans summary returns error when the cluster size doesnt equal to k When Kmeans using initMode random and some random seed it is possible the actual cluster size doesnt equal to the configured k In this case summary model returns error due to the number of cols of coefficient matrix doesnt equal to k'
0,'Automatic null conversion bug instead of throwing error when creating a Spark Datarame with incompatible types for fields When converting an RDD with a float type field to a spark dataframe with an IntegerType LongType schema field spark 162 and 163 silently convert the field values to nulls instead of throwing an error like LongType can not accept object  in type type float However this seems to be fixed in Spark 202 The following example should make the problem clear It converts all the values in that column to nulls instead of throwing an error that there is a type mismatch'
0,'Jenkins build seems to be broken by changes in testpatchsh A couple jenkins build failure for the same reason It seems to have been broken by HADOOP10926'
0,'spark 20 branchs sparkreleasepublish failed because style check failed ERROR LineLength Line is longer than 100 characters found 110 WARNING checkstyle check violations detected but failOnViolation set to false'
0,'GlobPattern regex library has performance issues with wildcard characters javautilregex classes have performance problems with certain wildcard patterns Namely consecutive characters in a file name not properly escaped as literals will cause commands such as hadoop fs ls file name to consume 100 CPU and probably never return in a reasonable time time scales with number of s'
0,'TestAdlFileContextMainOperationsLivetestGetFileContext1 runtime error'
0,'Aliyun OSS documentation missing from website Im looking at the alpha2 website and cant find a link to the Aliyun OSS documentation Under the Hadoop Compatible File Systems header there are links to S3 Azure blob ADLS and Swift but not Aliyun OSS'
0,'Hadoop commands guide should include examples Currently The Hadoop command guide just lists all the available command line options with a description It should include examples for each command for more clarity'
0,'Fix unit tests to not use uppercase key names After HADOOP11311 uppercase key names arent allowed breaking some unit tests Lets fix them'
0,'Kafka OffsetOutOfRangeException on DStreams union from separate Kafka clusters with identical topic names During migration from Spark 16 to 20 I observed OffsetOutOfRangeException reported by Kafka client Fortunately OffsetOutOfRangeException was thrown because offsets in both Kafka clusters are significantly different'
0,'ObjectHashAggregateSuite is being flaky occasional OOM errors This test suite fails occasionally on Jenkins due to OOM errors Ive already reproduced it locally but havent figured out the root cause We should probably disable it temporarily before getting it fixed so that it doesnt break the PR build too often'
0,'mlR example fails in yarncluster mode due to lacks of e1071 package mlR application fails in spark2 with yarncluster mode'
0,'Job failure due to Executor OOM in offheap mode We have been seeing many job failure due to executor OOM with following stack trace Digging into the code we found out that this is an issue with cooperative memory management for off heap memory allocation When the UnsafeExternalSorter is checking if memory page is being used by upstream the base object in case of off heap memory is always null so the UnsafeExternalSorter does not spill the memory pages'
0,'KMSClientProvider uses equalsIgnoreCase application json KMSClientProvidercall validates the content type thats not going to work in all locales not if upper case is being returned'
0,'builds from git checks fail because of src contrib chukwa opt not being there Git doesnt save empty directories so it would be nice to make a placeholder touch file that would cause git to create the directory'
0,'Insert overwrite statement runs much slower in sparksql than it does in hiveclient I find insert overwrite statement running in sparksql or sparkshell spends much more time than it does in hiveclient i start it in apachehive201bin bin hive where spark costs about ten minutes but hiveclient just costs less than 20 seconds'
0,'Fix intermittent test failure of TestGangliaMetrics Jenkins found this test failure on HADOOP11149'
0,'Managed Partitioned Table in InMemoryCatalog the user specified partition location is not deleted after table dropping The data in the managed table should be deleted after table is dropped However if the partition location is not under the location of the partitioned table it is not deleted as expected'
0,'hive tests should fail if SQL generation failed'
0,'Ignore Structured Streaming 202 logs in history server SPARK18516 changes the event log format of Structured Streaming We should make sure our changes not break the history server'
0,'MESOS Spark application throws exception on exit Here is the scenario Looking into the error log there is this constant error message My observations have led me to believe that the application master does not know about this container being killed and continuously asks the container to remove the executor until eventually failing the attempt or continue trying to remove the executor'
0,'KMS key names are incorrectly encoded when creating key Creating a key that contains special character s in its name will result in failure when creating while that key is in fact created ok on the underlying key provider'
0,'Improve error message for greatest least Greatest least function does not have the most friendly error message for data types Error in SQL statement AnalysisException cannot resolve greatest CAST 10 AS DECIMAL 21 10 due to data type mismatch The expressions should all have the same type got GREATEST ArrayBuffer DecimalType 21 StringType line 1 pos 7'
0,'Clean up FileSystem javadocs logging improve diagnostics on FS load We cant easily debug FS instantiation problems as there isnt much detail in what was going on We can add more logging but cannot simply switch FileSystemLOG to SLF4J the  is used in too many places including tests which cast it Instead add a new private SLF4J Logger LOGGER and switch logging to it While working in the base FileSystem  take the opportunity to clean up javadocs and comments add the list of exceptions including indicating which base classes throw UnsupportedOperationExceptions cut bits in the comments which are not true The outcome of this patch is that IDEs shouldnt highlight most of the file as flawed in some way or another'
0,'Issue Exceptions when ALTER TABLE RENAME PARTITION tries to alter a data source table ALTER TABLE RENAME PARTITION is unable to handle data source tables just like the other ALTER PARTITION commands We should issue an exception instead'
0,'Hadoop javadoc has broken links for AccessControlList ImpersonationProvider DefaultImpersonationProvider and DistCp Same error for DistCp ImpersonationProvider and DefaultImpersonationProvider also Javadoc generated from Trunk has the same problem'
0,'backport trunks smartapplypatchsh to branch2 Now that we have branch switching smartapplypatchsh needs to be updated so that testpatch can apply patches to it'
0,'Full outer join in correlated subquery returns incorrect results Full outer join with a correlated predicate in the left operand in a subquery may return incorrect results'
0,'Can not query hive table starting with number I can do it with spark162'
0,'spark sql length 1 return error these sql will return errors but hive is ok Error in query cannot resolve length 11 due to data type mismatch argument 1 requires string or binary type however 11 is of int type line 1 pos 14 Error in query cannot resolve length 20 due to data type mismatch argument 1 requires string or binary type however 20 is of double type line 1 pos 14'
0,'Do not attach javadoc and sources jars during nondist build Looking at maven output when running with Pdist the source plugin testjar and jar goals are invoked twice This is because its turned on by both the dist profile and on by default Outside of the release context its not that important to have javadoc and source JARs so I think we can turn it off by default'
0,'TreeNodeException when flat mapping RelationalGroupedDataset created from DataFrame containing a column created with lit expr A TreeNodeException is thrown when executing the following minimal example in Spark 20 Crucial is that the column q is generated with lit expr The exception is orgapachesparksqlcatalysterrorspackage$TreeNodeException Binding attribute tree x 5'
0,'Output nonaggregate expressions without GROUP BY in a subquery does not yield an error Found that the following query does not raise a syntax error note the GROUP BY clause is commented out'
0,'S3 filesystem operations stopped working correctly HADOOP10542 was resolved by replacing return null with throwing IOException This causes several S3 filesystem operations to fail possibly more code is expecting that null return value these are just the calls I noticed Resulting stack trace changing the raise IOE to return null fixes all of the above code sites and allows distcp to succeed'
0,'check and add missing documentation for PySpark ML Check and add miss documentation for PySpark ML this issue only check miss docs for oasml not oasmllib'
0,'PySpark CrossValidator reports incorrect avgMetrics The avgMetrics are summed up across all folds instead of being averaged'
0,'Catalysts IN always returns false for infinity This bug was caused by the fix for SPARK18999 This can be reproduced by adding the following test to PredicateSuitescala which will consistently fail This bug is causing to fail approximately 10 of the time it fails anytime the value is Infinity or Infinity and the correct answer is True'
0,'take or isEmpty on dataset leaks s3a connections Im experiensing problems with s3a and working with parquet with dataset api the symptom of problem tasks failing Our way to bypass problem is to use count 0'
0,'Log timezone when query result does not match It is useful to log the timezone when query result does not match especially on build machines that have different timezone from AMPLab Jenkins'
0,'Custom PartitionCoalescer cause serialization exception for example the following code cause exception'
0,'MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter'
0,'JDBC Sources Handling illegal input values for fetchsize and batchsize Throws Error in query No handler for Hive UDF'
0,'Calling outer join and nafill 0 and then inner join will miss rows I reported a similar bug two months ago and its fixed in Spark 201 But I find a new bug when I insert a nafill 0 call between outer join and inner join in the same workflow in SPARK17060 I get wrong result And again if i use persist the result is correct I think the problem is join optimizer similar to this pr'
0,'Unable to query global temp views when hive support is enabled Querying on a global temp view throws Table or view not found exception when Hive support is enabled Testcase to reproduce the problem The test needs to run when hive support is enabled HiveSessionCataloglookupRelation does not check for the global temp views'
0,'KMS does not log detailed stack trace for unexpected errors If the KMS server encounters an unexpected error resulting in an HTTP 500 response it does not log the stack trace This makes it difficult to troubleshoot The client side exception cannot provide further details'
0,'UI Should show blacklisted executors & nodes SPARK8425 will add the ability to blacklist entire executors and nodes to deal w faulty hardware However without displaying it on the UI it can be hard to realize which executor is bad and why tasks arent getting scheduled on certain executors As a first step we should just show nodes and executors that are blacklisted for the entire application no need to show blacklisting for tasks & stages This should also ensure that blacklisting events get into the event logs for the history server'
0,'Fix sprintf warnings in DomainSocketc introduced by HADOOP12344 Fix sprintf warnings in DomainSocketc introduced by HADOOP12344'
0,'Unable to append to a SequenceFile with CompressionNONE If I set CompressionNONE it works when the file is created but when the file already exists Ive a NullPointerException by the way it works if I specify a compression with a codec The following exeception is thrown when the file exists because compression option is checked'
0,'negative numeric literal parsing This introduces problems for the edge cases such as being parsed as decimal instead of bigint'
0,'Improper Popenwait return code handling in dev runtests Issues with current reader behavior'
0,'TestHttpServertestBindAddress bind port range is wider than expected It also appeared previously in HadoopcommontrunkJava8 jenkins on Oct 21 In the following case the first server bound to port 53212 and the second one bound to port 53225 which violated the assertion in the test case Stacktrace javalangAssertionError'
0,'TestRPCtestClientBackOff failing TestRPCtestClientBackOff is failing'
0,'Fix deadlock in DomainSocketWatcher when the notification pipe is full I found some of our DataNodes will run exceeds the limit of concurrent xciever the limit is 4K After check the stack I suspect that orgapachehadoopnetunixDomainSocketwriteArray0 which called by DomainSocketWatcherkick stuck'
0,'sparksql do not support for column datatype of CHAR In sparksql when we create a table using the command as follwing create table tablename col char 5 Hive will support for creating the table but when we desc the table desc tablename spark will report the error orgapachesparksqltypesDataTypeException Unsupported dataType char 5 If you have a struct and a field name of it has any special characters please use backticks to quote that field name eg x+y Please note that backtick itself is not supported in a field name'
0,'HttpServer2 should load jsp DTD from local jars instead of going remote When user want to start NameNode user would got the following exception it is caused by missing orgmortbayjetty jsp21jetty jar 6126 in the pomxml'
0,'Renaming a file into a directory containing the same filename results in a confusing I O error Renaming a file to another existing filename says File exists but colliding with a file in a directory results in the cryptic Input output error'
0,'Add stable version line to the website front page I think it would be worthwhile to add two lines to the top of the welcome website page Stable version 0101 Latest version 0111 With the number linking off to the respective release like so We can promote versions from Latest to Stable when they have proven themselves Thoughts'
0,'ExportSnapshot fails on kerberized cluster using s3a When using HBase ExportSnapshot on a kerberized cluster exporting to s3a using HADOOP10400 we see the following problem Caused by javalangIllegalArgumentException The problem seems to be that the patch in HADOOP10400 does not have getCanonicalServiceName'
0,'Unable to build hadoop 241 FreeBSD ERROR Failed to execute goal on project hadoophdfs Could not resolve dependencies for project'
0,'Add Spark as a related project on the Hadoop documentation page'
0,'Fix build native library on mac osx Some patches for fixing build a hadoop native library on os x 107 108'
0,'LZ4 Compression fails to recognize PowerPC Little Endian Architecture Lz4 Compression fails to identify the PowerPC Little Endian Architecture It recognizes it as Big Endian and several testcases fails due to this'
0,'backport openstack support to branch2 Backport the hadoopopenstack module to branch2 This should require little more than testing'
0,'hadoopuserinfoalloc fails on FreeBSD due to incorrect sysconf use namenode starts up but after first datanode contacts it it throws an exception I did not have such an issue with hadoop121'
0,'Regression s3n read failure recovery broken s3n attempts to read again when it encounters IOException during read Heres a stack trace as an example It seems this is a regression which was introduced by the following optimizations Also test cases should be reviewed so that it covers this scenario'
0,'ProxyUser improvements This is an umbrella jira which addresses few enhancements to proxyUser capability via sub tasks'
0,'Mention LoadBalancingKMSClientProvider in KMS HA documentation Currently there are two ways to achieve KMS HA The first one and the only documented one is running multiple KMS instances behind a load balancer The other way is make use of LoadBalancingKMSClientProvider which is added in HADOOP11620 However the usage is undocumented I think we should update the KMS document to introduce LoadBalancingKMSClientProvider provide examples and also update kmssitexml to explain it'
0,'FileContext does not react on changing umask via configuration After HADOOP13073 RawLocalFileSystem does react on changing umask but FileContext does not react on changing umask via configuration TestDirectoryCollection fails by the inconsistent behavior'
0,'TestCompressorDecompressor failing without stack traces Jenkins failing on TestCompressorDecompressor The exception is being caught and converted to a fail so there is no stack trace of any value'
0,'Mapreduce job failure on submission Try running Enhanced TestDFSIO Job jar is not getting set and not uploaded to staging dir'
0,'Fix a couple javac warnings in NFS This JIRA is to fix 2 javac warnings which are overlooked in HADOOP11195'
0,'oahsecurityTestGroupsCachingtestBackgroundRefreshCounters seems flaky Error Message Stacktrace'
0,'Proxy user verification NPEs if remote host is unresolvable A null is passed to the impersonation providers for the remote address if it is unresolvable DefaultImpersationProvider will NPE ipc will close the connection immediately correct behavior for such unexpected exceptions client fails on EOFException'
0,'TestSymlinkHdfsFileSystem testCreateLinkUsingPartQualPath2 fails after HADOOP13605'
0,'Update the document for hadoop fs stat They should be documented'
0,'RetryPolicies other than FailoverOnNetworkExceptionRetry should put on retry failed reason or the log from RMProxys retry could be very misleading In debugging a NM retry connection to RM nonHA the NM log during RM down time is very misleading We should keep them consistent or it could be very confusing'
0,'Change website to reflect new userhadoopapacheorg mailing list Change website to reflect new userhadoopapacheorg mailing list since weve merged the user lists per discussion on general'
0,'Collect network and disk usage on the node running Windows HADOOP12210 collects the node network usage for Linux this JIRA does it for Windows'
0,'Verify LICENSEtxt and NOTICEtxt We have many bundled dependencies in both the source and the binary artifacts that are not in LICENSEtxt and NOTICEtxt'
0,'JDK8 Fix javadoc error caused by illegal tag After HADOOP11684 mvn package Pdist DskipTests fails on JDK8'
0,'shell scripts ignore JAVAHOME on OS X It is expected to use JDK 7 instead This bug only occurs on trunk but not branch2'
0,'Return value of read ByteBuffer buf in CryptoInputStream is incorrect in some cases In CryptoInputStream for int read ByteBuffer buf if there is unread value in outBuffer then the current return value is incorrect'
0,'MetricsSystemImpl fails to show backtrace when an error occurs While investigating YARN3619 it was frustrating that MetricsSystemImpl was logging a ConcurrentModificationException but without any backtrace Logging a backtrace would be very beneficial to tracking down the cause of the problem'
0,'fix TestHTracedRESTReceiver unit test failures Fix some issues with HTracedRESTReceiver that are resulting in unit test failures So there were two main issues better way to launch htraced fixes to the HTracedRESTReceiver logic'
0,'Index out of range in SysInfoWindows Sometimes the NodeResourceMonitor tries to read the system utilization from winutilsexe and this return empty values This triggers the following exception StringIndexOutOfBoundsException'
0,'TestZKSignerSecretProvider testMultipleInit occasionally fail Error Message expected null but was Stacktrace javalangAssertionError'
0,'Improve documentation for rack awareness'
0,'The number of javadocs warnings is limited to 100 We are generating a lot of javadoc warnings with jdk 18 Right now the number is limited to 100 Enlarge this limitation can probably reveal more problems in one batch for our javadoc generation process'
0,'Create tests for Hadoop metrics It would be good to have a test case for hadoop metrics We could use FileContext or derive something out of NullContext to check the values returned via metrics are correct'
0,'Tarball as local resource type archive fails to localize on Windows When trying Tez out on Windows the tez targz failed to localize on Windows env'
0,'JDK8 Cannot build on Windows error unexpected end tag ul Tried on hadoop260src branch25 and branchtrunkwin All gave this error'
0,'Description of hdfs expunge command is confusing The description is confusing It gives user the impression that this command will empty trash but actually it only removes old checkpoints'
0,'testpatchsh should be documented It might be useful to have all of testpatchshs functionality documented how to use it power user hints etc esp for the bug bash'
0,'TestSymlinkLocalFSFileSystem fails intermittently Error Message Stacktrace javalangAssertionError'
